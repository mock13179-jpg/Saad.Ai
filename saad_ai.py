# -- coding: utf-8 --
"""
Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø®Ø§Ø±Ù‚ (Ultimate Edition)
Ù†Ø¸Ø§Ù… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªÙƒØ§Ù…Ù„ Ø°Ø§ØªÙŠ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„ØªØ·ÙˆØ±
"""

import sys
import os
import json
import pickle
import hashlib
import secrets
import random
import re
import time
import datetime
import threading
import queue
import sqlite3
import numpy as np
import math
from collections import defaultdict, deque
from enum import Enum
from typing import (Any, Dict, List, Tuple, Union, Optional, Callable,
                    Type, TypeVar, Generic, Iterable, Iterator, Set)
from flask import Flask, request, jsonify, send_file
import html
import urllib.parse
import requests
from bs4 import BeautifulSoup
import ast
import operator as op
from sympy import symbols, Eq, solve, simplify, sympify
import sympy as sp
from youtubesearchpython import VideosSearch
import difflib

# =============== Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ===============
# Ø¥Ø²Ø§Ù„Ø© Ø¬Ù…ÙŠØ¹ Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ©
import torch
try:
    import torch.nn as nn
except ImportError:
    nn = None

# =============== OpenRouter API Ù…Ø¹ OpenAI SDK ===============
import openai

def generate_via_openrouter(messages, temperature=0.5, max_tokens=512, model="meta-llama/llama-3.1-405b-instruct:free"):
    """Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ Ø¥Ù„Ù‰ OpenRouter API Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI SDK ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø±Ø¯ Ø§Ù„Ù†ØµÙŠ"""
    api_key = os.getenv("OPENROUTER_API_KEY", "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")

    if not api_key:
        print("ØªØ­Ø°ÙŠØ±: OPENROUTER_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø§Ø³ØªØ®Ø¯Ù… OPENROUTER_API_KEY=Ù…ÙØªØ§Ø­Ùƒ python script.py")
        return "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ."
    
    try:
        client = openai.OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key
        )
        
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=0.9
        )
        
        if response.choices and len(response.choices) > 0:
            return response.choices[0].message.content.strip()
        else:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ¬Ø§Ø¨Ø© OpenRouter: Ù„Ø§ ØªÙˆØ¬Ø¯ Ø®ÙŠØ§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø±Ø¯")
            return "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨."
            
    except openai.AuthenticationError:
        return "Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø©: Ù…ÙØªØ§Ø­ API ØºÙŠØ± ØµØ§Ù„Ø­ Ø£Ùˆ Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©."
    except openai.RateLimitError:
        return "ØªÙ… ØªØ¬Ø§ÙˆØ² Ø­Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡Ø§. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ù‹Ø§."
    except openai.APIError as e:
        return f"Ø®Ø·Ø£ ÙÙŠ API: {str(e)}"
    except openai.APIConnectionError:
        return "ØªØ¹Ø°Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø§Ø¯Ù… OpenRouter. ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ØªØµØ§Ù„Ùƒ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª."
    except Exception as e:
        print(f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ OpenRouter: {e}")
        return "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ."

# =============== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø³Ù„Ø§Ù…Ø© ÙˆØ§Ù„ØµÙ„Ø© =========
def detect_lang(text: str) -> str:
    """ÙƒØ´Ù Ø¨Ø¯Ø§Ø¦ÙŠ Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„: Ø¹Ø±Ø¨ÙŠ Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ."""
    # ÙˆØ¬ÙˆØ¯ Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÙŠØ©
    if re.search(r'[\u0600-\u06FF]', text):
        return "ar"
    return "en"

def analyze_sentiment_and_intent(text: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ù†ÙˆØ§ÙŠØ§ Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ø³Ø±Ø¹Ø© ÙˆØ¯Ù‚Ø© Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ø³ÙŠØ§Ù‚ÙŠ Ù…ØªÙ‚Ø¯Ù…"""
    text_lower = text.lower()
    
    # Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ù…Ø´Ø§Ø¹Ø±
    love_keywords = ["Ø£Ø­Ø¨Ùƒ", "Ø¨Ø­Ø¨Ùƒ", "Ù…Ø¹Ø¬Ø¨", "Ø¥Ø¹Ø¬Ø§Ø¨", "Ø£Ø­Ø¨", "Ø¨Ø­Ø¨", "Ø£Ù†Ø§ Ø£Ø­Ø¨Ùƒ", "Ø£Ù†Ø§ Ù…Ø¹Ø¬Ø¨", "Ø§Ø­Ø¨Ùƒ", "Ø§Ø­Ø¨Ùƒ ÙŠØ§"]
    gratitude_keywords = ["Ø´ÙƒØ±Ø§", "Ø´ÙƒØ±Ø§Ù‹", "Ù…Ù…ØªØ§Ø²", "Ø±Ø§Ø¦Ø¹", "Ø¬Ù…ÙŠÙ„", "Ù…Ø´ÙƒÙˆØ±", "ØªØ³Ù„Ù…", "ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©", "Ù…Ù‚Ø¯Ø±", "Ø´ÙƒØ±"]
    sad_keywords = ["Ø­Ø²ÙŠÙ†", "Ù…ÙƒØªØ¦Ø¨", "ØªØ¹ÙŠØ³", "Ø£Ø³ÙŠ", "Ø¨Ø§ÙƒÙŠ", "Ø¨ÙƒØ§Ø¡", "Ø¶Ø¬Ø±", "Ù…Ù„Ù„", "Ø­Ø²Ù†", "ØªØ¹Ø§Ø³Ø©"]
    angry_keywords = ["ØºØ§Ø¶Ø¨", "Ø²Ø¹Ù„Ø§Ù†", "Ù…Ø³ØªÙØ²", "Ù…Ù†Ø²Ø¹Ø¬", "ØºÙŠØ¸", "ØºØ¶Ø¨", "ØºØµØ©", "Ø²Ø¹Ù„"]
    excited_keywords = ["Ù…ØªØ­Ù…Ø³", "Ø­Ù…Ø§Ø³", "Ù…Ø¨Ù‡Ø¬", "Ø³Ø¹ÙŠØ¯", "ÙØ±Ø­", "Ù…Ø¨Ø³ÙˆØ·", "Ø¨Ù‡Ø¬Ø©"]
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø£ÙˆÙ„ÙŠ
    sentiment = "neutral"
    intensity = 0.5
    
    if any(word in text_lower for word in excited_keywords):
        sentiment = "excited"
        intensity = 0.8
    elif any(word in text_lower for word in love_keywords):
        sentiment = "love"
        intensity = 0.7
    elif any(word in text_lower for word in gratitude_keywords):
        sentiment = "gratitude"
        intensity = 0.6
    elif any(word in text_lower for word in sad_keywords):
        sentiment = "sad"
        intensity = 0.7
    elif any(word in text_lower for word in angry_keywords):
        sentiment = "angry"
        intensity = 0.7
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙŠØ© - ØªÙ…ÙŠÙŠØ² Ø¨ÙŠÙ† Ø§Ù„ØªÙ‚Ø¯ÙŠØ± ÙˆØ§Ù„Ø­Ø¨ Ø§Ù„Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ
    intent = "general"
    confidence = 0.8
    
    if sentiment == "love":
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„ØªÙ…ÙŠÙŠØ² Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù† Ø§Ù„Ø­Ø¨ Ø§Ù„Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ
        context_words = ["Ù…Ø³Ø§Ø¹Ø¯", "Ù…Ø³Ø§Ø¹Ø¯Ø©", "Ø°ÙƒØ§Ø¡", "Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Ø¨Ø±Ù†Ø§Ù…Ø¬", "Ø¢Ù„Ø©", "Ø±ÙˆØ¨ÙˆØª"]
        has_context = any(word in text_lower for word in context_words)
        
        if has_context or "ÙŠØ§ Ø³Ø¹Ø¯" in text_lower or "ÙŠØ§ Ø±ÙˆØ¨ÙˆØª" in text_lower:
            intent = "appreciation"  # ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
            confidence = 0.9
        else:
            intent = "general_affection"  # Ø¹Ø§Ø·ÙØ© Ø¹Ø§Ù…Ø©
            confidence = 0.6
    elif sentiment == "gratitude":
        intent = "appreciation"
        confidence = 0.9
    elif sentiment in ["sad", "angry"]:
        intent = "support_needed"
        confidence = 0.7
    elif sentiment == "excited":
        intent = "positive_expression"
        confidence = 0.8
    
    # ØªØ­Ù„ÙŠÙ„ Ø³ÙŠØ§Ù‚ÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©
    context_sensitive_analysis = analyze_sensitive_context(text)
    if context_sensitive_analysis["needs_help"]:
        intent = "help_request"
        confidence = 0.9
        sentiment = "supportive"
    
    return {
        "sentiment": sentiment,
        "intent": intent,
        "intensity": intensity,
        "confidence": confidence,
        "keywords_found": len([w for w in text_lower.split() if len(w) > 2]),
        "context_analysis": context_sensitive_analysis
    }

def analyze_sensitive_context(text: str) -> Dict[str, Any]:
    """
    ØªØ­Ù„ÙŠÙ„ Ø³ÙŠØ§Ù‚ÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø© Ù„Ù„ØªÙ…ÙŠÙŠØ² Ø¨ÙŠÙ†:
    1. Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©/Ø§Ù„Ø¥Ø¨Ù„Ø§Øº Ø¹Ù† Ø¬Ø±ÙŠÙ…Ø©
    2. ÙˆØµÙ ØªØ¬Ø±Ø¨Ø© Ø³Ø§Ø¨Ù‚Ø© (Ø¹Ù„Ø§Ø¬ÙŠ/Ù…Ø´ÙˆØ±Ø©)
    3. Ù…Ø­ØªÙˆÙ‰ Ø¶Ø§Ø± ÙØ¹Ù„ÙŠ
    """
    text_lower = text.lower()
    
    # ÙƒÙ„Ù…Ø§Øª ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£Ùˆ Ø§Ù„Ø¥Ø¨Ù„Ø§Øº
    help_keywords = ["Ù…Ø³Ø§Ø¹Ø¯Ø©", "Ø³Ø§Ø¹Ø¯Ù†ÙŠ", "Ø¶Ø­ÙŠØ©", "Ù…Ø®ØªØ·Ù", "Ù…Ø´ÙƒÙ„Ø©", "Ø®Ø·Ø±", "Ø£Ø­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø©", 
                     "Ø§Ù†Ù‚Ø°Ù†ÙŠ", "Ø®Ø· Ù…Ø³Ø§Ø¹Ø¯Ø©", "Ø¯Ø¹Ù… Ù†ÙØ³ÙŠ", "ØªØ¹Ø±Ø¶Øª Ù„", "Ø§ØºØªØµØ§Ø¨", "Ø§Ø¹ØªØ¯Ø§Ø¡", 
                     "Ø¹Ù†Ù", "Ø¨Ù„Ø§Øº", "Ø´Ø±Ø·Ø©", "Ø¥Ø³Ø§Ø¡Ø©", "Ø§Ø³ØªØºÙ„Ø§Ù„"]
    
    # ÙƒÙ„Ù…Ø§Øª ØªØ´ÙŠØ± Ø¥Ù„Ù‰ ÙˆØµÙ ØªØ¬Ø±Ø¨Ø© Ø³Ø§Ø¨Ù‚Ø© (Ø¹Ù„Ø§Ø¬ÙŠ/Ù…Ø´ÙˆØ±Ø©)
    therapy_keywords = ["ØªØ¬Ø±Ø¨Ø© Ø³Ø§Ø¨Ù‚Ø©", "ØµØ¯Ù…Ø©", "Ø¹Ù„Ø§Ø¬", "Ø·Ø¨ÙŠØ¨ Ù†ÙØ³ÙŠ", "Ù…Ø¹Ø§Ù„Ø¬", "Ù…Ø´ÙˆØ±Ø©",
                       "Ù…Ø§Ø¶ÙŠ", "Ø°ÙƒØ±Ù‰ Ù…Ø¤Ù„Ù…Ø©", "Ø£Ø­Ø¯Ø§Ø« Ù‚Ø¯ÙŠÙ…Ø©", "Ø¹Ø§Ù†ÙŠØª Ù…Ù†", "ÙƒÙ†Øª", "ÙÙŠ Ø§Ù„Ø³Ø§Ø¨Ù‚"]
    
    # ÙƒÙ„Ù…Ø§Øª ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø¶Ø§Ø± ÙØ¹Ù„ÙŠ
    harmful_keywords = ["ÙƒÙŠÙ Ø£Ø®ØªØ¨Ø±", "ÙƒÙŠÙ Ø£Ù†ÙØ°", "Ø·Ø±ÙŠÙ‚Ø© Ø§Ø®ØªØ±Ø§Ù‚", "ØµÙ†Ø¹ Ù‚Ù†Ø¨Ù„Ø©", "Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ¬Ø³Ø³", 
                       "ØªÙ‡ÙƒÙŠØ±", "Ù‚Ø±ØµÙ†Ø©", "ØªØ¯Ù…ÙŠØ±", "Ø¥Ù„Ø­Ø§Ù‚ Ø¶Ø±Ø±", "Ø¨Ø±Ù…Ø¬ÙŠØ§Øª Ø®Ø¨ÙŠØ«Ø©", "Ù‡Ø¬ÙˆÙ…"]
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚
    has_help_request = False
    has_therapy_context = False
    has_harmful_intent = False
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù…Ø¬Ø±Ø¯ ÙˆØ¬ÙˆØ¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
    sentences = re.split(r'[.!ØŸ]', text)
    
    for sentence in sentences:
        sentence_lower = sentence.lower().strip()
        if not sentence_lower:
            continue
            
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
        help_patterns = [
            r"(Ø£Ø­ØªØ§Ø¬|Ø£Ø±Ø¬Ùˆ|Ø£Ø·Ù„Ø¨) Ù…Ø³Ø§Ø¹Ø¯Ø©",
            r"(ØªØ¹Ø±Ø¶Øª|Ø£Ù†Ø§) (Ù„|Ù„Ù€) (Ø§Ø¹ØªØ¯Ø§Ø¡|ØªØ­Ø±Ø´|Ø¹Ù†Ù|Ø¥Ø³Ø§Ø¡Ø©)",
            r"(ÙƒÙŠÙ|Ø£ÙŠÙ†) (Ø£Ø¨Ù„Øº|Ø£Ø®Ø¨Ø±) Ø¹Ù†",
            r"(Ø®Ø·|Ø±Ù‚Ù…) (Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©|Ø§Ù„Ø·ÙˆØ§Ø±Ø¦)",
            r"(Ø¶Ø­ÙŠÙ‡|Ù…Ø®ØªØ·Ù) ÙˆØ£Ø±ÙŠØ¯ Ù…Ø³Ø§Ø¹Ø¯Ù‡"
        ]
        
        for pattern in help_patterns:
            if re.search(pattern, sentence_lower):
                has_help_request = True
                break
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ù„Ø§Ø¬ÙŠ
        therapy_patterns = [
            r"(ÙÙŠ|Ø®Ù„Ø§Ù„) (Ø·ÙÙˆÙ„ØªÙŠ|Ù…Ø§Ø¶ÙŠ|Ø³Ø§Ø¨Ù‚Ø§Ù‹)",
            r"(ÙƒÙ†Øª|Ø¹Ø§Ù†ÙŠØª) (Ù…Ù†|Ø¨Ø³Ø¨Ø¨)",
            r"(Ø£Ø­ÙƒÙŠ|Ø£Ø´Ø§Ø±Ùƒ) ØªØ¬Ø±Ø¨ØªÙŠ",
            r"(Ù„Ø¯ÙŠ|Ø¹Ù†Ø¯ÙŠ) Ø°ÙƒØ±Ù‰",
            r"(Ø£Ø±ÙŠØ¯|Ø£Ø­ØªØ§Ø¬) Ù…Ø´ÙˆØ±Ø©"
        ]
        
        for pattern in therapy_patterns:
            if re.search(pattern, sentence_lower):
                has_therapy_context = True
                break
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ÙŠØ© Ø§Ù„Ø¶Ø§Ø±Ø©
        harmful_patterns = [
            r"(ÙƒÙŠÙ|Ø£Ø±ÙŠØ¯) (Ø£Ù†|Ø£Ù† Ø£) (Ø£ØµÙ†Ø¹|Ø£Ø¨Ù†ÙŠ|Ø£Ø·ÙˆØ±)",
            r"(Ø·Ø±ÙŠÙ‚Ø©|Ø®Ø·ÙˆØ§Øª) (Ù„Ù€|Ù„)",
            r"(Ø£Ø¨Ø­Ø« Ø¹Ù†|Ø£Ø­ØªØ§Ø¬) Ø¨Ø±Ù†Ø§Ù…Ø¬",
            r"(Ù‡Ø¯ÙÙŠ|Ø£Ø±ØºØ¨ ÙÙŠ) (Ø¥Ù„Ø­Ø§Ù‚|ØªØ³Ø¨Ø¨)",
            r"(ØªØ¹Ù„ÙŠÙ…Ø§Øª|Ø¯Ù„ÙŠÙ„) Ù„Ù€"
        ]
        
        for pattern in harmful_patterns:
            if re.search(pattern, sentence_lower) and any(kw in sentence_lower for kw in ["Ù‚Ù†Ø¨Ù„Ø©", "Ø§Ø®ØªØ±Ø§Ù‚", "ØªØ¯Ù…ÙŠØ±", "Ø¶Ø±Ø±"]):
                has_harmful_intent = True
                break
    
    # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ø¹ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
    context_type = "neutral"
    needs_help = False
    needs_guidance = False
    
    if has_help_request:
        context_type = "help_request"
        needs_help = True
    elif has_therapy_context:
        context_type = "therapy_context"
        needs_guidance = True
    elif has_harmful_intent:
        context_type = "harmful_content"
    
    # ØªØ­Ù„ÙŠÙ„ Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ ÙˆØªØ¹Ù‚ÙŠØ¯Ù‡
    word_count = len(text.split())
    is_complex = word_count > 20
    has_code = "```" in text or "def " in text_lower or "function" in text_lower
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø­ÙŠØ·Ø©
    intent_score = 0
    if "Ø£Ø­ØªØ§Ø¬" in text_lower and "Ù…Ø³Ø§Ø¹Ø¯Ø©" in text_lower:
        intent_score += 2
    if "Ù…Ø§Ø°Ø§ Ø£ÙØ¹Ù„" in text_lower or "Ù…Ø§Ø°Ø§ ÙŠØ¬Ø¨ Ø£Ù† Ø£ÙØ¹Ù„" in text_lower:
        intent_score += 1
    if "Ø£Ø®Ø¨Ø±Ù†ÙŠ" in text_lower and ("ÙƒÙŠÙ" in text_lower or "Ø·Ø±ÙŠÙ‚Ø©" in text_lower):
        intent_score -= 1
    
    return {
        "context_type": context_type,
        "needs_help": needs_help,
        "needs_guidance": needs_guidance,
        "has_code": has_code,
        "is_complex": is_complex,
        "word_count": word_count,
        "intent_score": intent_score,
        "is_help_request": has_help_request,
        "is_therapy_context": has_therapy_context,
        "is_harmful_intent": has_harmful_intent
    }

def normalize_arabic_text(text: str) -> str:
    """ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù‡Ø¬Ø§Ø¦ÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ."""
    if not text or not re.search(r'[\u0600-\u06FF]', text):
        return text
    
    # Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ØªØµØ­ÙŠØ­Ø§Øª
    corrections = {
        # Ø§Ù„Ù…Ø¯Ù† ÙˆØ§Ù„Ø£Ù…Ø§ÙƒÙ†
        "Ø§Ù„Ù‚Ø§Ù‡Ø±Ù‡": "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©",
        "Ø§Ù„Ù‚Ø§Ø¨Ø±Ù‡": "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©", 
        "Ø§Ù„Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠÙ‡": "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©",
        "Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠÙ‡": "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©",
        "Ø§Ù„Ø¬ÙŠØ²Ù‡": "Ø§Ù„Ø¬ÙŠØ²Ø©",
        "Ø§Ù„Ø¬ÙŠØ²Ø©": "Ø§Ù„Ø¬ÙŠØ²Ø©",
        "Ø§Ù„Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©": "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©",
        
        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        "Ø§Ù„Ø§Ù†": "Ø§Ù„Ø¢Ù†",
        "Ù‡Ø§Ø°Ø§": "Ù‡Ø°Ø§",
        "Ù‡Ø°Ø©": "Ù‡Ø°Ù‡",
        "Ù‡Ø°ÙŠÙ†": "Ù‡Ø°ÙŠÙ†",
        "Ø§Ù„ÙŠ": "Ø¥Ù„Ù‰",
        "Ø§Ù„ÙŠ": "Ø¥Ù„Ù‰",
        "Ø§Ù„Ù„Ø©": "Ø§Ù„Ù„Ù‡",
        "Ø±Ø³ÙˆÙ„Ø©": "Ø±Ø³ÙˆÙ„Ù‡",
        "Ø¹Ù„ÙŠØ©": "Ø¹Ù„ÙŠÙ‡",
        "Ù‡Ø°Ø©": "Ù‡Ø°Ù‡",
        
        # Ø§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø© ÙˆØ§Ù„Ù‡Ø§Ø¡
        "Ù…Ø¯Ø±Ø³Ù‡": "Ù…Ø¯Ø±Ø³Ø©",
        "Ø¬Ø§Ù…Ø¹Ù‡": "Ø¬Ø§Ù…Ø¹Ø©",
        "ÙƒÙ„ÙŠÙ‡": "ÙƒÙ„ÙŠØ©",
        "ÙˆØ²Ø§Ø±Ù‡": "ÙˆØ²Ø§Ø±Ø©",
        "Ø§Ø¯Ø§Ø±Ù‡": "Ø¥Ø¯Ø§Ø±Ø©",
        
        # Ø§Ù„Ù‡Ù…Ø²Ø§Øª
        "Ø³Ø¡Ø§Ù„": "Ø³Ø¤Ø§Ù„",
        "Ù‚Ø±Ø¡": "Ù‚Ø±Ø£",
        "Ø¡Ø§Ù†": "Ø¢Ù†",
    }
    
    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØµØ­ÙŠØ­Ø§Øª
    normalized_text = text
    for wrong, correct in corrections.items():
        normalized_text = re.sub(r'\b' + wrong + r'\b', correct, normalized_text)
    
    # ØªØµØ­ÙŠØ­ Ø§Ù„Ù‡Ù…Ø²Ø§Øª ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
    normalized_text = re.sub(r'([\u0600-\u06FF])Ø¡Ø§', r'\1Ø¢', normalized_text)  # Ù‡Ù…Ø²Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù„Ù
    normalized_text = re.sub(r'Ø§Ø¡([\u0600-\u06FF])', r'Ø£\1', normalized_text)  # Ù‡Ù…Ø²Ø© ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
    normalized_text = re.sub(r'Ø§Ø¡', 'Ø£', normalized_text)  # Ø§Ù„Ø£Ù„Ù ÙˆØ§Ù„Ù‡Ù…Ø²Ø©
    
    # ØªØµØ­ÙŠØ­ Ø§Ù„ØªÙ†ÙˆÙŠÙ†
    normalized_text = re.sub(r'Ø§Ù‹$', 'Ù‹Ø§', normalized_text)  # ØªÙ†ÙˆÙŠÙ† Ø§Ù„Ù†ØµØ¨
    
    return normalized_text

BAD_TERMS = {
    # Ø¹Ø±Ø¨ÙŠ
    "Ø¬Ù†Ø³","Ø¥Ø¨Ø§Ø­ÙŠ","Ù‚Ø¶ÙŠØ¨","Ù…Ù‡Ø¨Ù„","Ù…Ø«ÙŠØ±","Ù…Øµ","Ø¬Ù…Ø§Ø¹","Ø§Ø­ØªÙƒØ§Ùƒ","ÙÙ…ÙˆÙŠ","Ø´Ø±Ø¬",
    "ØªÙØ¬ÙŠØ±","Ù‚Ù†Ø¨Ù„Ø©","Ù‚ØªÙ„","Ø³Ø±Ù‚Ø©","Ù†ØµØ¨","Ø§Ø­ØªÙŠØ§Ù„","Ø®Ø¯Ø§Ø¹","Ù…Ø®Ø¯Ø±Ø§Øª","Ø§Ù†ØªØ­Ø§Ø±",
    "Ø¬Ø«Ø©","Ø¥Ø®ÙØ§Ø¡ Ø¬Ø«Ø©","Ø¥Ø±Ù‡Ø§Ø¨","ØªØ·Ø±Ù","ØªÙ‡Ø±ÙŠØ¨","Ø³Ù„Ø§Ø­","Ù‚ØªØ§Ù„","Ø¹Ù†Ù","Ø¶Ø±Ø¨",
    "Ø³Ø±Ù‚Ø© Ø¨Ù†Ùƒ","Ø§Ø®ØªØ±Ø§Ù‚","Ù‚Ø±ØµÙ†Ø©","ØªØ²ÙˆÙŠØ±","ÙØ³Ø§Ø¯","Ø±Ø´ÙˆØ©","ØªÙ‡Ø¯ÙŠØ¯","Ø§Ø¨ØªØ²Ø§Ø²",
    
    # Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ
    "sex","porn","penis","vagina","erotic","blowjob","oral","anal","nsfw",
    "bomb","explosive","kill","murder","steal","scam","fraud","drugs",
    "suicide","corpse","terrorism","extremism","smuggling","weapon","violence"
}

# Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
canonical_facts = {
    # Ø¹ÙˆØ§ØµÙ… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    "Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© ÙØ±Ù†Ø³Ø§": "Ø¹Ø§ØµÙ…Ø© ÙØ±Ù†Ø³Ø§ Ù‡ÙŠ Ø¨Ø§Ø±ÙŠØ³",
    "Ø¹Ø§ØµÙ…Ø© ÙØ±Ù†Ø³Ø§": "Ø¨Ø§Ø±ÙŠØ³", 
    "Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±": "Ø¹Ø§ØµÙ…Ø© Ù…ØµØ± Ù‡ÙŠ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©",
    "Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±": "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©",
    "Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© ÙƒÙ†Ø¯Ø§": "Ø¹Ø§ØµÙ…Ø© ÙƒÙ†Ø¯Ø§ Ù‡ÙŠ Ø£ÙˆØªØ§ÙˆØ§",
    "Ø¹Ø§ØµÙ…Ø© ÙƒÙ†Ø¯Ø§": "Ø£ÙˆØªØ§ÙˆØ§",
    "Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©": "Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶",
    "Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©": "Ø§Ù„Ø±ÙŠØ§Ø¶",
    "Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù‚Ø·Ø±": "Ø¹Ø§ØµÙ…Ø© Ù‚Ø·Ø± Ù‡ÙŠ Ø§Ù„Ø¯ÙˆØ­Ø©",
    "Ø¹Ø§ØµÙ…Ø© Ù‚Ø·Ø±": "Ø§Ù„Ø¯ÙˆØ­Ø©",
    "Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª": "Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ù‡ÙŠ Ø£Ø¨Ùˆ Ø¸Ø¨ÙŠ",
    "Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª": "Ø£Ø¨Ùˆ Ø¸Ø¨ÙŠ",
    
    # Ù…ÙØ§Ù‡ÙŠÙ… Ø¹Ù„Ù…ÙŠØ©
    "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ©": "Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© Ù‡ÙŠ Ù‚ÙˆØ© Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ¬Ø°Ø¨ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ù†Ø­Ùˆ Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶",
    "Ø´Ø±Ø­ Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ©": "Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© Ù‡ÙŠ Ø§Ù„Ù‚ÙˆØ© Ø§Ù„ØªÙŠ ØªÙ…Ø³ÙƒÙ†Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ø¶ ÙˆØªØ¬Ø¹Ù„ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ØªØ³Ù‚Ø·",
    "Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠØ­Ø¯Ø« Ù„Ù„Ù…Ø§Ø¡ Ø¹Ù†Ø¯ 100 Ø¯Ø±Ø¬Ø©": "ÙŠØºÙ„ÙŠ Ø§Ù„Ù…Ø§Ø¡ Ø¹Ù†Ø¯ 100 Ø¯Ø±Ø¬Ø© Ù…Ø¦ÙˆÙŠØ© ÙˆÙŠØµØ¨Ø­ Ø¨Ø®Ø§Ø±Ø§Ù‹",
    "Ø¯Ø±Ø¬Ø© ØºÙ„ÙŠØ§Ù† Ø§Ù„Ù…Ø§Ø¡": "Ø§Ù„Ù…Ø§Ø¡ ÙŠØºÙ„ÙŠ Ø¹Ù†Ø¯ 100 Ø¯Ø±Ø¬Ø© Ù…Ø¦ÙˆÙŠØ©",
    
    # Ø´Ø®ØµÙŠØ§Øª
    "Ù…Ù† Ù‡Ùˆ Ù…Ø¤Ø³Ø³ Ù…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª": "Ø¨ÙŠÙ„ ØºÙŠØªØ³ ÙˆØ¨ÙˆÙ„ Ø£Ù„ÙŠÙ†",
    "Ù…Ø¤Ø³Ø³ Ù…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª": "Ø¨ÙŠÙ„ ØºÙŠØªØ³ ÙˆØ¨ÙˆÙ„ Ø£Ù„ÙŠÙ†",
    
    # Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
    "capital of france": "Paris",
    "capital of egypt": "Cairo", 
    "capital of canada": "Ottawa",
    "capital of saudi arabia": "Riyadh",
    "capital of qatar": "Doha",
    "founder of microsoft": "Bill Gates and Paul Allen",
    "what is gravity": "Gravity is the force that attracts objects toward each other",
    "boiling point of water": "Water boils at 100 degrees Celsius"
}

# =============== Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª ÙˆØ§Ù„Ø£Ø³Ù„ÙˆØ¨ ===============

class StylePreferences:
    """Ø¥Ø¯Ø§Ø±Ø© ØªÙØ¶ÙŠÙ„Ø§Øª Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    
    def __init__(self, user_id: str = "default"):
        self.user_id = user_id
        self.preferences = {
            "temperature": 0.4,  # Ø«Ø§Ø¨Øª - Ù„Ø§ ÙŠØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡
            "use_emojis": True,
            "response_style": "balanced",  # balanced, creative, concise
            "formality_level": 2,  # 1-3 (Ù…Ù†Ø®ÙØ¶ØŒ Ù…ØªÙˆØ³Ø·ØŒ Ø¹Ø§Ù„ÙŠ)
            "last_feedback": None,  # like/dislike
            "response_speed": "fast",  # fast, normal
            "variation_level": 3  # 1-5 Ù…Ø³ØªÙˆÙ‰ ØªÙ†ÙˆØ¹ Ø§Ù„ØµÙŠØ§ØºØ©
        }
        self.feedback_history = []
        self.session_memory = {}
        self.response_variations = {}  # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„
        
    def update_from_feedback(self, feedback_type: str):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø±Ø¯ÙˆØ¯ ÙØ¹Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        self.preferences["last_feedback"] = feedback_type
        self.feedback_history.append({
            "timestamp": datetime.datetime.now().isoformat(),
            "feedback": feedback_type
        })
        
        if feedback_type == "like":
            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªÙ†ÙˆØ¹ ÙÙŠ Ø§Ù„ØµÙŠØ§ØºØ© Ø¯ÙˆÙ† ØªØ¹Ø¯ÙŠÙ„ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
            self.preferences["variation_level"] = min(5, self.preferences["variation_level"] + 1)
            self.preferences["use_emojis"] = True
        elif feedback_type == "dislike":
            # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªÙ†ÙˆØ¹ Ù‚Ù„ÙŠÙ„Ø§Ù‹
            self.preferences["variation_level"] = max(1, self.preferences["variation_level"] - 1)
    
    def get_temperature(self) -> float:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© - Ø«Ø§Ø¨ØªØ©"""
        return 0.3  # Ø¯Ø§Ø¦Ù…Ø§Ù‹ 0.3
    
    def should_use_emoji(self, sentiment: str = "neutral", intent: str = "general") -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ù†ÙˆØ§ÙŠØ§"""
        if not self.preferences["use_emojis"]:
            return False
            
        # Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ù†ÙŠØ©
        emoji_probabilities = {
            ("love", "appreciation"): 0.9,        # â¤ï¸ ğŸ¤—
            ("gratitude", "appreciation"): 0.8,   # ğŸ™ ğŸ˜Š
            ("excited", "positive_expression"): 0.85,  # ğŸ˜„ ğŸ‰
            ("sad", "support_needed"): 0.7,      # ğŸ¤— ğŸ’™
            ("angry", "support_needed"): 0.6,    # ğŸ˜ âš¡
            ("neutral", "general"): 0.5,         # ğŸ™‚
            ("neutral", "appreciation"): 0.7,    # ğŸ˜Š
        }
        
        prob = emoji_probabilities.get((sentiment, intent), 0.4)
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙ†ÙˆØ¹
        variation_boost = (self.preferences["variation_level"] - 1) * 0.05
        prob = min(0.95, prob + variation_boost)
        
        return random.random() < prob
    
    def get_sentiment_emoji(self, sentiment: str, intent: str) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ù†ÙŠØ©"""
        emoji_map = {
            ("love", "appreciation"): random.choice(["â¤ï¸", "ğŸ¤—", "ğŸ’", "ğŸ™"]),
            ("gratitude", "appreciation"): random.choice(["ğŸ™", "ğŸ˜Š", "ğŸ‘", "âœ¨"]),
            ("excited", "positive_expression"): random.choice(["ğŸ˜„", "ğŸ‰", "ğŸ”¥", "âš¡"]),
            ("sad", "support_needed"): random.choice(["ğŸ¤—", "ğŸ’™", "ğŸ«‚", "âœ¨"]),
            ("angry", "support_needed"): random.choice(["âš¡", "ğŸ’ª", "ğŸ›¡ï¸", "âœ¨"]),
            ("neutral", "appreciation"): random.choice(["ğŸ˜Š", "ğŸ‘", "ğŸ‘Œ", "âœ…"]),
            ("neutral", "general"): random.choice(["ğŸ’¡", "ğŸ“š", "ğŸ”", "âœ¨"]),
        }
        
        if self.should_use_emoji(sentiment, intent):
            return emoji_map.get((sentiment, intent), "âœ¨")
        return ""
    
    def get_response_style_prompt(self) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ù„Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª"""
        style_prompts = {
            "balanced": "ÙƒÙ† Ù…ØªÙˆØ§Ø²Ù†Ø§Ù‹ ÙˆØ¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙÙŠ Ø§Ù„Ø±Ø¯. Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© Ø³Ù„ÙŠÙ…Ø© ÙˆÙˆØ§Ø¶Ø­Ø©.",
            "creative": "ÙƒÙ† Ù…Ø¨Ø¯Ø¹Ø§Ù‹ ÙˆÙ…ØªÙ†ÙˆØ¹Ø§Ù‹ ÙÙŠ Ø§Ù„ØµÙŠØ§ØºØ© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©.",
            "concise": "ÙƒÙ† Ù…Ø®ØªØµØ±Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ ÙÙŠ Ø§Ù„Ø±Ø¯ Ù…Ø¹ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©."
        }
        return style_prompts.get(self.preferences["response_style"], "ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙˆØ§Ø¶Ø­Ø§Ù‹.")
    
    def store_response_variation(self, question_hash: str, response: str):
        """ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ø³Ø¤Ø§Ù„ Ù„ØªÙØ§Ø¯ÙŠ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        if question_hash not in self.response_variations:
            self.response_variations[question_hash] = []
        
        self.response_variations[question_hash].append(response)
        
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 3 Ø±Ø¯ÙˆØ¯ ÙÙ‚Ø·
        if len(self.response_variations[question_hash]) > 3:
            self.response_variations[question_hash].pop(0)
    
    def get_previous_responses(self, question_hash: str) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ø³Ø¤Ø§Ù„"""
        return self.response_variations.get(question_hash, [])

# ØªØ®Ø²ÙŠÙ† ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
user_styles = {}

def get_user_style(user_id: str) -> StylePreferences:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙØ¶ÙŠÙ„Ø§Øª Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    if user_id not in user_styles:
        user_styles[user_id] = StylePreferences(user_id)
    return user_styles[user_id]

# =============== Ù†Ø¸Ø§Ù… Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù† ===============

def calculate_similarity(q1: str, q2: str) -> float:
    """Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ù†ØµÙŠ Ø¯Ù‚ÙŠÙ‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SequenceMatcher"""
    return difflib.SequenceMatcher(None, q1.lower(), q2.lower()).ratio()

def extract_country_from_question(question: str) -> Optional[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆÙ„Ø© Ù…Ù† Ø§Ù„Ø³Ø¤Ø§Ù„"""
    countries = {
        "Ù…ØµØ±": "Ù…ØµØ±",
        "ÙØ±Ù†Ø³Ø§": "ÙØ±Ù†Ø³Ø§", 
        "ÙƒÙ†Ø¯Ø§": "ÙƒÙ†Ø¯Ø§",
        "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©": "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        "Ù‚Ø·Ø±": "Ù‚Ø·Ø±",
        "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª",
        "Ø§Ù„Ø£Ø±Ø¯Ù†": "Ø§Ù„Ø£Ø±Ø¯Ù†",
        "Ù„Ø¨Ù†Ø§Ù†": "Ù„Ø¨Ù†Ø§Ù†",
        "Ø§Ù„Ø¹Ø±Ø§Ù‚": "Ø§Ù„Ø¹Ø±Ø§Ù‚",
        "Ø³ÙˆØ±ÙŠØ§": "Ø³ÙˆØ±ÙŠØ§",
        "Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±": "Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±",
        "Ø§Ù„Ù…ØºØ±Ø¨": "Ø§Ù„Ù…ØºØ±Ø¨",
        "ØªÙˆÙ†Ø³": "ØªÙˆÙ†Ø³",
        "Ù„ÙŠØ¨ÙŠØ§": "Ù„ÙŠØ¨ÙŠØ§",
        "Ø§Ù„Ø³ÙˆØ¯Ø§Ù†": "Ø§Ù„Ø³ÙˆØ¯Ø§Ù†",
        "Ø§Ù„ÙŠÙ…Ù†": "Ø§Ù„ÙŠÙ…Ù†",
        "Ø¹Ù…Ø§Ù†": "Ø¹Ù…Ø§Ù†",
        "Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†": "Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
        "Ø§Ù„ÙƒÙˆÙŠØª": "Ø§Ù„ÙƒÙˆÙŠØª"
    }
    
    question_lower = question.lower()
    for country_ar, country in countries.items():
        if country_ar in question_lower or country.lower() in question_lower:
            return country
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¯ÙˆÙ„ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
    english_countries = {
        "egypt": "Ù…ØµØ±",
        "france": "ÙØ±Ù†Ø³Ø§",
        "canada": "ÙƒÙ†Ø¯Ø§",
        "saudi arabia": "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        "qatar": "Ù‚Ø·Ø±",
        "uae": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª",
        "united arab emirates": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª"
    }
    
    for eng, ar in english_countries.items():
        if eng in question_lower:
            return ar
    
    return None

def get_factual_answer(question: str, lang: str) -> Optional[str]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆÙ…Ù†Ø¹ Ø§Ù„ØªØ®Ù…ÙŠÙ†"""
    question_norm = normalize_arabic_text(question).lower()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¯ÙˆÙ„Ø© Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© Ø¥Ù† ÙˆØ¬Ø¯Øª
    mentioned_country = extract_country_from_question(question)
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚
    best_match = None
    best_score = 0
    
    for fact_question, answer in canonical_facts.items():
        fact_norm = normalize_arabic_text(fact_question).lower()
        similarity = calculate_similarity(question_norm, fact_norm)
        
        # Ø¥Ø°Ø§ Ø°ÙƒØ±Øª Ø¯ÙˆÙ„Ø©ØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ ÙŠØªØ­Ø¯Ø« Ø¹Ù† Ù†ÙØ³ Ø§Ù„Ø¯ÙˆÙ„Ø©
        if mentioned_country:
            # ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø®Ø²Ù† ÙŠØªØ­Ø¯Ø« Ø¹Ù† Ù†ÙØ³ Ø§Ù„Ø¯ÙˆÙ„Ø©
            answer_lower = answer.lower()
            fact_question_lower = fact_question.lower()
            has_country_in_answer = mentioned_country.lower() in answer_lower or mentioned_country in fact_question_lower
            
            if not has_country_in_answer:
                # Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø¯ÙˆÙ„Ø© Ø£Ø®Ø±Ù‰
                continue
        
        if similarity > best_score:
            best_score = similarity
            best_match = (fact_question, answer)
    
    # ØªØ·Ø¨ÙŠÙ‚ Ø¹ØªØ¨Ø© Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© (90%)
    if best_score >= 0.9:
        return best_match[1]
    elif best_score >= 0.7:
        # Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© - Ø·Ù„Ø¨ ØªÙˆØ¶ÙŠØ­
        return None
    else:
        # Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© - ØªØ¬Ø§Ù‡Ù„
        return None

def should_ask_for_clarification(question: str, lang: str) -> bool:
    """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø·Ù„Ø¨ ØªÙˆØ¶ÙŠØ­"""
    question_lower = question.lower()
    
    # ÙƒØ´Ù Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ø¹Ù† Ø§Ù„Ø¹ÙˆØ§ØµÙ…
    capital_patterns = [
        r"Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø©",
        r"Ø¹Ø§ØµÙ…Ø© Ø¯ÙˆÙ„Ø©",
        r"Ø¹Ø§ØµÙ…Ø© Ø£ÙŠ Ø¯ÙˆÙ„Ø©",
        r"capital of",
        r"capital city of"
    ]
    
    for pattern in capital_patterns:
        if re.search(pattern, question_lower):
            # ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø°ÙƒØ± Ù„Ø¯ÙˆÙ„Ø© Ù…Ø­Ø¯Ø¯Ø©
            if not extract_country_from_question(question):
                return True
    
    return False

def looks_nsfw(title: str, summary: str) -> bool:
    t = (title or "").lower()
    s = (summary or "").lower()
    for w in BAD_TERMS:
        if w in t or w in s:
            return True
    return False

def is_relevant(summary: str, question: str) -> bool:
    """ÙŠØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù…Ù„Ø®Øµ Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ (ØªØ¯Ø§Ø®Ù„ ÙƒÙ„Ù…Ø§Øª Ø¨Ø³ÙŠØ· Ù„ÙƒÙ†Ù‡ Ø¹Ù…Ù„ÙŠ)."""
    if not summary:
        return False
    # ÙƒÙ„Ù…Ø§Øª Ù…ÙÙŠØ¯Ø© ÙÙ‚Ø· (â‰¥3 Ø­Ø±ÙˆÙØŒ Ø¨Ø¯ÙˆÙ† Ø¹Ù„Ø§Ù…Ø§Øª)
    def tokenize(x):
        x = re.sub(r'[^\w\u0600-\u06FF]+', ' ', x.lower())
        return [w for w in x.split() if len(w) >= 3]
    q_words = set(tokenize(question))
    s_words = set(tokenize(summary))
    overlap = len(q_words & s_words)
    # Ø§Ø¹ØªØ¨Ø±Ù‡ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§ Ù„Ùˆ ÙÙŠÙ‡ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ ÙƒÙ„Ù…ØªÙŠÙ† Ù…Ø´ØªØ±ÙƒØªÙŠÙ† Ø£Ùˆ Ù„Ùˆ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§ ÙÙˆØ§Ø­Ø¯Ø© ØªÙƒÙÙŠ
    if len(q_words) <= 4:
        return overlap >= 1
    return overlap >= 2

def smart_shorten(text: str, max_sentences: int = 2, max_chars: int = 320) -> str:
    """Ø§Ù‚ØªØ·Ø§Ø¹ Ù†Ø¸ÙŠÙ Ø¥Ù„Ù‰ Ø¬Ù…Ù„ØªÙŠÙ† ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰ØŒ ÙˆØ¨Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù…Ù† Ø§Ù„Ø­Ø±ÙˆÙ."""
    # Ø§ÙØµÙ„ Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©/Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
    parts = re.split(r'(?<=[\.!\?ØŸ])\s+', text.strip())
    out = ' '.join(parts[:max_sentences]).strip()
    if len(out) > max_chars:
        out = out[:max_chars].rsplit(' ', 1)[0].rstrip() + 'â€¦'
    return out

# ---- Ø­Ø§Ø±Ø³ Ø±ÙŠØ§Ø¶ÙŠ: ÙƒØ´Ù ÙˆÙ…Ø³Ø­ ÙˆØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ØªØ¹Ø§Ø¨ÙŠØ± LaTeX Ø¨Ø³ÙŠØ·Ø© ----
MATH_RE = re.compile(r'[\d\.\+\-\*\/\^\(\)\s]+$')

def preprocess_math_expr(q: str) -> str:
    """Ø­ÙˆÙ‘Ù„ Ø¨Ø¹Ø¶ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù€LaTeX Ø§Ù„Ø¨Ø³ÙŠØ·Ø© Ø¥Ù„Ù‰ ØªØ¹Ø¨ÙŠØ± Ø¨Ø§ÙŠØ«ÙˆÙ†ÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙ‚ÙŠÙŠÙ…."""
    s = q.strip()
    # Ø£Ø²Ù„ Ø­Ø±ÙˆÙ $ Ùˆ \left \right
    s = s.replace('$', '')
    s = s.replace('\\left', '').replace('\\right', '')
    # ØªØ­ÙˆÙŠÙ„ \frac{a}{b} Ø¥Ù„Ù‰ (a/b)
    s = re.sub(r'\\frac\s*\{\s*([^{}]+?)\s*\}\s*\{\s*([^{}]+?)\s*\}', r'(\1/\2)', s)
    # ØªØ­ÙˆÙŠÙ„ ^ Ø¥Ù„Ù‰ **
    s = s.replace('^', '**')
    # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ Ø­Ø±ÙˆÙ ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ© (Ø§Ø¨Ù‚Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø¹Ù…Ù„ÙŠØ§Øª ÙˆØ§Ù„Ø§Ù‚ÙˆØ§Ø³ ÙˆÙ†Ù‚Ø·Ø©)
    s = re.sub(r'[^\d\.\+\-\*\/\(\)\s\*]', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

# Ø£Ù…Ø§Ù† Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: Ø¯Ø§Ù„Ø© ØªØ³ØªØ®Ø¯Ù… ast Ù„ØªÙ‚ÙŠÙŠØ¯ Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡Ø§
ALLOWED_OPERATORS = {
    ast.Add: op.add,
    ast.Sub: op.sub,
    ast.Mult: op.mul,
    ast.Div: op.truediv,
    ast.Pow: op.pow,
    ast.USub: op.neg,
    ast.UAdd: op.pos
}

def _eval_ast(node):
    if isinstance(node, ast.Num):  # <number>
        return node.n
    if isinstance(node, ast.BinOp):
        left = _eval_ast(node.left)
        right = _eval_ast(node.right)
        op_type = type(node.op)
        if op_type in ALLOWED_OPERATORS:
            return ALLOWED_OPERATORS[op_type](left, right)
    if isinstance(node, ast.UnaryOp):
        operand = _eval_ast(node.operand)
        op_type = type(node.op)
        if op_type in ALLOWED_OPERATORS:
            return ALLOWED_OPERATORS[op_type](operand)
    raise ValueError("Unsafe or unsupported expression")

def safe_eval_expr(expr: str):
    """Ù‚ÙŠÙ… ØªØ¹Ø¨ÙŠØ± Ø±ÙŠØ§Ø¶ÙŠ Ø¨Ø³ÙŠØ· Ø¨Ø£Ù…Ø§Ù† Ø£Ùˆ Ø§Ø±Ù…Ù Ø§Ø³ØªØ«Ù†Ø§Ø¡."""
    node = ast.parse(expr, mode='eval')
    return _eval_ast(node.body)

def is_math_question(q: str) -> bool:
    """ÙƒØ´Ù Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØ¬ÙˆØ¯ Ø£Ø±Ù‚Ø§Ù… + Ø¹Ù…Ù„ÙŠØ§ØªØŒ Ø£Ùˆ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø¨Ù‡Ø§ Ù…ØªØºÙŠØ±Ø§Øª."""
    s = q.strip()
    # ØµÙŠØºØ© Ù„Ø§ÙŠØªÙƒ Ø£Ùˆ ^
    if r'\frac' in s or '^' in s:
        return True
    # Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† ÙÙŠÙ‡ Ø£Ø±Ù‚Ø§Ù… Ø£Ùˆ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø£Ùˆ Ù…ØªØºÙŠØ±Ø§Øª Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø¹Ù…Ù„ÙŠØ§Øª
    has_number = bool(re.search(r'\d', s))
    has_operator = bool(re.search(r'[+\-*/^=]', s))
    has_variable = bool(re.search(r'\b[xyz]\b', s))
    # Ù…Ø³Ø£Ù„Ø© Ù„Ùˆ ÙÙŠÙ‡Ø§ Ø£Ø±Ù‚Ø§Ù… ÙˆØ¹Ù…Ù„ÙŠØ©ØŒ Ø£Ùˆ Ù…Ø¹Ø§Ø¯Ù„Ø© ÙÙŠÙ‡Ø§ Ù…ØªØºÙŠØ±
    if (has_number and has_operator) or (has_variable and '=' in s):
        return True
    return False

def solve_math_question(q: str) -> str | None:
    """Ø­Ù„ Ø§Ù„Ù…Ø³Ø£Ù„Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SympyØŒ Ù…Ø¹ fallback Ù„Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¢Ù…Ù†."""
    try:
        # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
        x, y, z = symbols('x y z')
        expr = q.replace('^', '**').replace('Ã—', '*')
        # Ù„Ùˆ ÙÙŠÙ‡Ø§ Ù…Ø¹Ø§Ø¯Ù„Ø©
        if '=' in expr:
            left, right = expr.split('=', 1)
            equation = Eq(sympify(left), sympify(right))
            sol = solve(equation)
            return f"{sol}"
        else:
            val = simplify(sympify(expr))
            return str(val)
    except Exception:
        # Ù„Ùˆ ÙØ´Ù„ØŒ Ø¬Ø±Ø¨ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø§Ù„ÙŠ
        try:
            expr = preprocess_math_expr(q)
            if not expr:
                return None
            result = safe_eval_expr(expr)
            if isinstance(result, float) and result.is_integer():
                result = int(result)
            return str(result)
        except Exception:
            return None

# =============== YouTube Search Functions ===============
def search_youtube(query, max_results=3):
    """ÙŠØ¨Ø­Ø« ÙÙŠ YouTube Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… youtube-search-python"""
    try:
        search = VideosSearch(query, limit=max_results)
        results = search.result()['result']
        
        videos = []
        for video in results:
            if not looks_nsfw(video['title'], ""):
                videos.append({
                    'title': video['title'],
                    'url': video['link'],
                    'channel': video['channel']['name'],
                    'duration': video['duration'],
                    'views': video['viewCount']['short'] if 'viewCount' in video else 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ'
                })
        
        return videos
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ YouTube: {str(e)}")
        return []

# =============== Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø´Ø§Ù…Ù„ Ø§Ù„Ù…Ø­Ø¯Ø« ===============
class MemoryCategory(Enum):
    PERSON = "person"
    RELATIONSHIP = "relationship"
    EVENT = "event"
    EXPERIENCE = "experience"
    TRAUMATIC = "traumatic"
    HAPPY_MEMORY = "happy_memory"
    TRAVEL = "travel"
    WORK = "work"
    EDUCATION = "education"
    HEALTH = "health"
    FINANCE = "finance"
    DREAM = "dream"
    GOAL = "goal"
    FEAR = "fear"
    SECRET = "secret"
    PREFERENCE = "preference"
    SKILL = "skill"
    ACHIEVEMENT = "achievement"
    FAILURE = "failure"
    OTHER = "other"

class UniversalMemorySystem:
    def __init__(self, db_path="universal_memory.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„ÙƒÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø°ÙƒØ±ÙŠØ§Øª
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS memories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT,
                memory_hash TEXT UNIQUE,
                category TEXT,
                subcategory TEXT,
                title TEXT,
                content TEXT,
                entities TEXT,  -- JSON list of people/places involved
                emotions TEXT,  -- JSON list of emotions
                intensity INTEGER DEFAULT 3,  -- 1-5 scale
                importance INTEGER DEFAULT 3,  -- 1-5 scale
                privacy_level INTEGER DEFAULT 2,  -- 1-5 (1=very private)
                is_sensitive BOOLEAN DEFAULT FALSE,
                occurred_date TEXT,
                created_date TIMESTAMP,
                last_recalled TIMESTAMP,
                recall_count INTEGER DEFAULT 0
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ø£Ø´Ø®Ø§Øµ
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS relationships (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT,
                person_name TEXT,
                relationship_type TEXT,  -- ØµØ¯ÙŠÙ‚ØŒ Ø­Ø¨ÙŠØ¨ØŒ Ø²Ù…ÙŠÙ„ØŒ Ø¥Ù„Ø®
                current_status TEXT,  -- Ø­Ø§Ù„ÙŠØŒ Ø³Ø§Ø¨Ù‚ØŒ Ù…ØªÙ‚Ø·Ø¹
                start_date TEXT,
                end_date TEXT,
                importance INTEGER DEFAULT 3,
                qualities TEXT,  -- JSON list of qualities
                memories_linked TEXT,  -- JSON list of memory IDs
                trust_level INTEGER DEFAULT 3,
                created_date TIMESTAMP
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ù‡Ø§Ù…Ø©
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS significant_events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT,
                event_type TEXT,
                title TEXT,
                description TEXT,
                location TEXT,
                event_date TEXT,
                people_involved TEXT,  -- JSON list
                emotional_impact TEXT,  -- JSON of emotions
                life_impact INTEGER DEFAULT 3,  -- 1-5 scale
                lessons_learned TEXT,
                changed_beliefs TEXT,
                created_date TIMESTAMP
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø¹ÙˆØ§Ø·Ù
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS emotional_profile (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT,
                emotion_type TEXT,
                trigger TEXT,
                intensity INTEGER,
                frequency TEXT,  -- Ø¯Ø§Ø¦Ù…ØŒ Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ØŒ Ù†Ø§Ø¯Ø±Ø§Ù‹
                coping_methods TEXT,
                created_date TIMESTAMP
            )
        ''')
        
        # Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø¯ÙŠÙ…
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS user_profiles (
                user_id TEXT PRIMARY KEY,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS conversation_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT,
                user_input TEXT,
                ai_response TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                category TEXT
            )
        ''')
        
        # ÙÙ‡Ø§Ø±Ø³ Ù„Ù„Ø£Ø¯Ø§Ø¡
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_memories_user_category ON memories(user_id, category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_memories_date ON memories(occurred_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_relationships_user ON relationships(user_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_conversation_user ON conversation_history(user_id, timestamp)')
        
        conn.commit()
        conn.close()
    
    def generate_memory_hash(self, user_id: str, content: str) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ØµÙ…Ø© ÙØ±ÙŠØ¯Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø©"""
        return hashlib.md5(f"{user_id}_{content}".encode()).hexdigest()
    
    def add_memory(self, user_id: str, category: MemoryCategory, title: str, 
                  content: str, entities: List[str] = None, emotions: List[str] = None,
                  intensity: int = 3, importance: int = 3, occurred_date: str = None,
                  is_sensitive: bool = False, subcategory: str = None) -> bool:
        """Ø¥Ø¶Ø§ÙØ© Ø£ÙŠ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª"""
        
        memory_hash = self.generate_memory_hash(user_id, content)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO memories 
                (user_id, memory_hash, category, subcategory, title, content, 
                 entities, emotions, intensity, importance, is_sensitive, 
                 occurred_date, created_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                user_id, memory_hash, category.value, subcategory, title, content,
                json.dumps(entities or []), json.dumps(emotions or []), 
                intensity, importance, is_sensitive,
                occurred_date or datetime.datetime.now().strftime("%Y-%m-%d"),
                datetime.datetime.now()
            ))
            
            conn.commit()
            return True
            
        except sqlite3.IntegrityError:
            # Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
            return False
        finally:
            conn.close()
    
    def add_relationship(self, user_id: str, person_name: str, relationship_type: str,
                        current_status: str = "current", start_date: str = None,
                        end_date: str = None, importance: int = 3, qualities: List[str] = None):
        """Ø¥Ø¶Ø§ÙØ© Ø¹Ù„Ø§Ù‚Ø© Ù…Ø¹ Ø´Ø®Øµ"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO relationships 
            (user_id, person_name, relationship_type, current_status, 
             start_date, end_date, importance, qualities, created_date)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            user_id, person_name, relationship_type, current_status,
            start_date, end_date, importance,
            json.dumps(qualities or []), datetime.datetime.now()
        ))
        
        conn.commit()
        conn.close()
        return True
    
    def add_significant_event(self, user_id: str, event_type: str, title: str,
                             description: str, location: str = "", event_date: str = None,
                             people_involved: List[str] = None, emotional_impact: List[str] = None,
                             life_impact: int = 3, lessons_learned: str = ""):
        """Ø¥Ø¶Ø§ÙØ© Ø­Ø¯Ø« Ù‡Ø§Ù…"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO significant_events 
            (user_id, event_type, title, description, location, event_date,
             people_involved, emotional_impact, life_impact, lessons_learned, created_date)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            user_id, event_type, title, description, location,
            event_date or datetime.datetime.now().strftime("%Y-%m-%d"),
            json.dumps(people_involved or []), json.dumps(emotional_impact or []),
            life_impact, lessons_learned, datetime.datetime.now()
        ))
        
        conn.commit()
        conn.close()
        return True
    
    def search_memories(self, user_id: str, query: str = None, category: str = None,
                       emotion: str = None, date_range: Tuple[str, str] = None,
                       limit: int = 10) -> List[Dict]:
        """Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù… ÙÙŠ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        sql = "SELECT * FROM memories WHERE user_id = ?"
        params = [user_id]
        
        if query:
            sql += " AND (title LIKE ? OR content LIKE ?)"
            params.extend([f'%{query}%', f'%{query}%'])
        
        if category:
            sql += " AND category = ?"
            params.append(category)
        
        if emotion:
            sql += " AND emotions LIKE ?"
            params.append(f'%{emotion}%')
        
        if date_range:
            sql += " AND occurred_date BETWEEN ? AND ?"
            params.extend(date_range)
        
        sql += " ORDER BY importance DESC, occurred_date DESC LIMIT ?"
        params.append(limit)
        
        cursor.execute(sql, params)
        results = cursor.fetchall()
        conn.close()
        
        memories = []
        for row in results:
            memories.append({
                'id': row[0],
                'category': row[3],
                'subcategory': row[4],
                'title': row[5],
                'content': row[6],
                'entities': json.loads(row[7]),
                'emotions': json.loads(row[8]),
                'intensity': row[9],
                'importance': row[10],
                'occurred_date': row[14],
                'created_date': row[15]
            })
        
        return memories
    
    def get_relationship_network(self, user_id: str) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT person_name, relationship_type, current_status, importance
            FROM relationships WHERE user_id = ?
            ORDER BY importance DESC, current_status
        ''', (user_id,))
        
        relationships = cursor.fetchall()
        conn.close()
        
        return {
            'current': [r for r in relationships if r[2] == 'current'],
            'past': [r for r in relationships if r[2] == 'past'],
            'other': [r for r in relationships if r[2] not in ['current', 'past']]
        }
    
    def get_life_timeline(self, user_id: str) -> List[Dict]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ø­ÙŠØ§Ø©"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ù‡Ø§Ù…Ø© ÙˆØ§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
        cursor.execute('''
            SELECT 'event' as type, title, description, event_date as date, life_impact as importance
            FROM significant_events WHERE user_id = ?
            UNION
            SELECT 'memory' as type, title, content as description, occurred_date as date, importance
            FROM memories WHERE user_id = ? AND importance >= 4
            ORDER BY date DESC
            LIMIT 20
        ''', (user_id, user_id))
        
        timeline = cursor.fetchall()
        conn.close()
        
        return [
            {
                'type': row[0],
                'title': row[1],
                'description': row[2],
                'date': row[3],
                'importance': row[4]
            }
            for row in timeline
        ]
    
    # ========= Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø¯ÙŠÙ… =========
    
    def _ensure_user_exists(self, user_id: str):
        """Ensure user profile exists"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR IGNORE INTO user_profiles (user_id) 
            VALUES (?)
        ''', (user_id,))
        
        conn.commit()
        conn.close()
    
    def store_information(self, user_id: str, text: str) -> Dict[str, Any]:
        """ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø¯ÙŠÙ…: ØªØ®Ø²ÙŠÙ† Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        self._ensure_user_exists(user_id)
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø°Ø§ÙƒØ±Ø© Ø¹Ø§Ù…Ø©
        success = self.add_memory(
            user_id=user_id,
            category=MemoryCategory.OTHER,
            title=f"Ù…Ø¯Ø®Ù„ Ù…Ø­Ø§Ø¯Ø«Ø© - {datetime.datetime.now().strftime('%H:%M')}",
            content=text,
            entities=[],
            emotions=[],
            intensity=2,
            importance=1,
            occurred_date=datetime.datetime.now().strftime("%Y-%m-%d")
        )
        
        return {
            'stored_count': 1 if success else 0,
            'category': 'general_conversation',
            'entries': [{'key': 'free_text', 'value': text}]
        }
    
    def search_memory(self, user_id: str, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø¯ÙŠÙ…: Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        memories = self.search_memories(user_id, query=query, limit=top_k)
        
        results = []
        for memory in memories:
            results.append({
                'category': memory['category'],
                'key': 'memory',
                'value': memory['content'],
                'confidence': 0.7,
                'access_count': 0,
                'score': 0.7
            })
        
        return results
    
    def get_user_profile(self, user_id: str) -> Dict[str, Any]:
        """ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø¯ÙŠÙ…: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø´Ø§Ù…Ù„"""
        memories = self.search_memories(user_id, limit=20)
        
        profile = {
            'user_id': user_id,
            'categories': {},
            'stats': {
                'total_memories': len(memories),
                'most_accessed': [],
                'recent_additions': memories[:5] if memories else []
            }
        }
        
        for memory in memories:
            category = memory['category']
            if category not in profile['categories']:
                profile['categories'][category] = []
            
            profile['categories'][category].append({
                'key': 'memory',
                'value': memory['content'],
                'confidence': 0.7,
                'access_count': 0,
                'created_at': memory['created_date']
            })
        
        return profile
    
    def add_conversation(self, user_id: str, user_input: str, ai_response: str, category: str):
        """ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø¯ÙŠÙ…: Ø¥Ø¶Ø§ÙØ© Ù…Ø­Ø§Ø¯Ø«Ø© Ø¥Ù„Ù‰ Ø§Ù„ØªØ§Ø±ÙŠØ®"""
        self._ensure_user_exists(user_id)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO conversation_history (user_id, user_input, ai_response, category)
            VALUES (?, ?, ?, ?)
        ''', (user_id, user_input, ai_response, category))
        
        conn.commit()
        conn.close()
    
    def get_conversation_context(self, user_id: str, limit: int = 20) -> List[Dict[str, str]]:
        """ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø¯ÙŠÙ…: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù…Ø¹ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø­Ø¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT user_input, ai_response, timestamp, category
            FROM conversation_history 
            WHERE user_id = ?
            ORDER BY timestamp DESC
            LIMIT ?
        ''', (user_id, limit))
        
        conversations = []
        for row in cursor.fetchall():
            conversations.append({
                'user_input': row[0],
                'ai_response': row[1],
                'timestamp': row[2],
                'category': row[3]
            })
        
        conn.close()
        return list(reversed(conversations))
    
    def generate_conversation_summary(self, user_id: str, max_messages: int = 10) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø·ÙˆÙŠÙ„Ø©"""
        conversations = self.get_conversation_context(user_id, limit=max_messages)
        
        if not conversations:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø³Ø§Ø¨Ù‚Ø©"
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        key_points = []
        for conv in conversations[-5:]:  # Ø¢Ø®Ø± 5 Ø±Ø³Ø§Ø¦Ù„
            user_msg = conv['user_input'][:50] + "..." if len(conv['user_input']) > 50 else conv['user_input']
            ai_msg = conv['ai_response'][:50] + "..." if len(conv['ai_response']) > 50 else conv['ai_response']
            key_points.append(f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_msg}")
            key_points.append(f"Ø³Ø¹Ø¯: {ai_msg}")
        
        summary = "Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©:\n" + "\n".join(key_points[-10:])  # Ø¢Ø®Ø± 10 Ù†Ù‚Ø§Ø·
        
        return summary

class IntelligentMemoryExtractor:
    def __init__(self, memory_system: UniversalMemorySystem):
        self.memory = memory_system
        self.setup_comprehensive_patterns()
    
    def setup_comprehensive_patterns(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø£Ù†Ù…Ø§Ø· Ø´Ø§Ù…Ù„Ø© Ù„ÙƒÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"""
        
        self.relationship_patterns = {
            'current_girlfriend': [
                r'ØµØ¯ÙŠÙ‚ØªÙŠ Ø§Ù„Ø­Ø§Ù„ÙŠØ© (Ù‡ÙŠ|ØªØ¯Ø¹Ù‰) ([\w\u0600-\u06FF\s]+)',
                r'Ø£Ù†Ø§ (Ù…Ø¹|Ø£ØªÙˆØ§Ø¹Ø¯ Ù…Ø¹) ([\w\u0600-\u06FF\s]+)',
                r'Ø­Ø¨ÙŠØ¨ØªÙŠ (Ø§Ù„Ø¢Ù†|Ø§Ù„Ø­Ø§Ù„ÙŠØ©) (Ù‡ÙŠ|Ù‡ÙŠ) ([\w\u0600-\u06FF\s]+)'
            ],
            'ex_relationships': [
                r'ØµØ¯ÙŠÙ‚ØªÙŠ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© (ÙƒØ§Ù†Øª|Ù‡ÙŠ) ([\w\u0600-\u06FF\s]+)',
                r'Ø­Ø¨ÙŠØ¨ØªÙŠ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (Ù‡ÙŠ|ØªØ¯Ø¹Ù‰) ([\w\u0600-\u06FF\s]+)',
                r'ÙƒÙ†Øª (Ù…Ø¹|Ø£Ø­Ø¨) ([\w\u0600-\u06FF\s]+)'
            ],
            'friends': [
                r'ØµØ¯ÙŠÙ‚ÙŠ (Ø§Ù„Ø­Ø§Ù„ÙŠ|Ø§Ù„Ù…Ù‚Ø±Ø¨) (Ù‡Ùˆ|ÙŠØ¯Ø¹Ù‰) ([\w\u0600-\u06FF\s]+)',
                r'Ø£ØµØ¯Ù‚Ø§Ø¦ÙŠ (Ù‡Ù…|ÙŠØ¯Ø¹ÙˆÙ†) ([\w\u0600-\u06FF\s]+)'
            ]
        }
        
        self.event_patterns = {
            'betrayal': [
                r'Ø®Ø§Ù†Ù†ÙŠ ([\w\u0600-\u06FF\s]+)',
                r'ØªØ¹Ø±Ø¶Øª Ù„Ù„Ø®ÙŠØ§Ù†Ø© (Ù…Ù†|Ø¨ÙˆØ§Ø³Ø·Ø©) ([\w\u0600-\u06FF\s]+)',
                r'Ø®Ø¯Ø¹Øª (Ù…Ù† Ù‚Ø¨Ù„|Ù…Ù†) ([\w\u0600-\u06FF\s]+)'
            ],
            'travel': [
                r'Ø³Ø§ÙØ±Øª Ø¥Ù„Ù‰ ([\w\u0600-\u06FF\s]+)',
                r'Ø°Ù‡Ø¨Øª ÙÙŠ Ø±Ø­Ù„Ø© Ø¥Ù„Ù‰ ([\w\u0600-\u06FF\s]+)',
                r'Ø²Ø±Øª ([\w\u0600-\u06FF\s]+)'
            ],
            'accident': [
                r'ØªØ¹Ø±Ø¶Øª Ù„Ø­Ø§Ø¯Ø« (ÙÙŠ|Ø¹Ù†Ø¯) ([\w\u0600-\u06FF\s]+)',
                r'Ø­Ø¯Ø« Ù„ÙŠ Ø­Ø§Ø¯Ø« (Ù…Ø±ÙˆØ±ÙŠ|Ø£Ù„ÙŠÙ…)',
                r'Ø£ØµØ¨Øª ÙÙŠ ([\w\u0600-\u06FF\s]+)'
            ],
            'achievement': [
                r'ÙØ²Øª (Ø¨|ÙÙŠ) ([\w\u0600-\u06FF\s]+)',
                r'Ø­ØµÙ„Øª Ø¹Ù„Ù‰ (Ø¬Ø§Ø¦Ø²Ø©|ØªØ±Ù‚ÙŠØ©) (ÙÙŠ|Ø¨)',
                r'Ø£Ù†Ù‡ÙŠØª (Ø¯Ø±Ø§Ø³ØªÙŠ|Ù…Ø´Ø±ÙˆØ¹) (ÙÙŠ|Ø¨)'
            ]
        }
        
        self.emotional_patterns = {
            'fears': [
                r'Ø£Ø®Ø§Ù Ù…Ù† ([\w\u0600-\u06FF\s]+)',
                r'Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø®ÙˆÙ Ù…Ù† ([\w\u0600-\u06FF\s]+)',
                r'Ù‡Ù†Ø§Ùƒ Ø´ÙŠØ¡ ÙŠØ®ÙŠÙÙ†ÙŠ ÙˆÙ‡Ùˆ ([\w\u0600-\u06FF\s]+)'
            ],
            'dreams': [
                r'Ø£Ø­Ù„Ù… Ø¨Ø£Ù† ([\w\u0600-\u06FF\s]+)',
                r'Ø£ØªÙ…Ù†Ù‰ Ø£Ù† ([\w\u0600-\u06FF\s]+)',
                r'Ø·Ù…ÙˆØ­ÙŠ Ù‡Ùˆ ([\w\u0600-\u06FF\s]+)'
            ],
            'secrets': [
                r'Ø³Ø±Ù‘ÙŠ Ù‡Ùˆ ([\w\u0600-\u06FF\s]+)',
                r'Ù„Ù… Ø£Ø®Ø¨Ø± Ø£Ø­Ø¯Ø§Ù‹ Ø¨Ø£Ù† ([\w\u0600-\u06FF\s]+)',
                r'Ø´ÙŠØ¡ Ù„Ø§ ÙŠØ¹Ø±ÙÙ‡ Ø£Ø­Ø¯ Ø¹Ù†ÙŠ Ù‡Ùˆ ([\w\u0600-\u06FF\s]+)'
            ]
        }
        
        # Ø£Ù†Ù…Ø§Ø· Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª ÙˆØ§Ù„Ù‡ÙˆØ§ÙŠØ§Øª
        self.preference_patterns = {
            'food_preferences': [
                r'Ø£Ø­Ø¨ (Ø£ÙƒÙ„|Ø´Ø±Ø¨|ØªÙ†Ø§ÙˆÙ„) ([\w\u0600-\u06FF\s]+)',
                r'Ù…Ø´Ø±ÙˆØ¨ÙŠ Ø§Ù„Ù…ÙØ¶Ù„ Ù‡Ùˆ ([\w\u0600-\u06FF\s]+)',
                r'Ø£ÙØ¶Ù„ (Ø·Ø¹Ø§Ù…|Ø´Ø±Ø§Ø¨) Ù„ÙŠ Ù‡Ùˆ ([\w\u0600-\u06FF\s]+)',
                r'Ù„Ø§ Ø£Ø­Ø¨ ([\w\u0600-\u06FF\s]+)'
            ],
            'hobbies': [
                r'Ù‡ÙˆØ§ÙŠØªÙŠ (Ù‡ÙŠ|Ù‡ÙŠ) ([\w\u0600-\u06FF\s]+)',
                r'Ø£Ø­Ø¨ (Ù…Ù…Ø§Ø±Ø³Ø©|ÙØ¹Ù„) ([\w\u0600-\u06FF\s]+)',
                r'Ø£Ù‚Ø¶ÙŠ ÙˆÙ‚ØªÙŠ ÙÙŠ ([\w\u0600-\u06FF\s]+)',
                r'Ø£Ø³ØªÙ…ØªØ¹ Ø¨Ù€ ([\w\u0600-\u06FF\s]+)'
            ],
            'entertainment': [
                r'Ø£Ø­Ø¨ (Ø£ÙÙ„Ø§Ù…|Ù…Ø³Ù„Ø³Ù„Ø§Øª|ÙƒØªØ¨|Ù…ÙˆØ³ÙŠÙ‚Ù‰) ([\w\u0600-\u06FF\s]+)',
                r'Ù†ÙˆØ¹ (Ø§Ù„Ø£ÙÙ„Ø§Ù…|Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰) Ø§Ù„Ù…ÙØ¶Ù„ Ù„Ø¯ÙŠ Ù‡Ùˆ ([\w\u0600-\u06FF\s]+)',
                r'Ø£ÙØ¶Ù„ (Ù…ØºÙ†ÙŠ|Ù…Ù…Ø«Ù„|ÙƒØ§ØªØ¨) Ù‡Ùˆ ([\w\u0600-\u06FF\s]+)'
            ],
            'sports': [
                r'Ø£Ù…Ø§Ø±Ø³ Ø±ÙŠØ§Ø¶Ø© ([\w\u0600-\u06FF\s]+)',
                r'Ø£Ø´Ø§Ù‡Ø¯ (Ù…Ø¨Ø§Ø±ÙŠØ§Øª|Ø±ÙŠØ§Ø¶Ø©) ([\w\u0600-\u06FF\s]+)',
                r'ÙØ±ÙŠÙ‚ÙŠ Ø§Ù„Ù…ÙØ¶Ù„ Ù‡Ùˆ ([\w\u0600-\u06FF\s]+)'
            ]
        }
    
    def extract_comprehensive_info(self, user_id: str, text: str) -> Dict[str, List]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù†Øµ"""
        
        extracted = {
            'relationships': [],
            'events': [],
            'emotions': [],
            'preferences': [],
            'memories': [],
            'inferred_preferences': []  # ØªÙØ¶ÙŠÙ„Ø§Øª Ù…Ø³ØªÙ†ØªØ¬Ø©
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
        for rel_type, patterns in self.relationship_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    person_name = match.group(2) if len(match.groups()) >= 2 else match.group(1)
                    if person_name:
                        extracted['relationships'].append({
                            'type': rel_type,
                            'person': person_name.strip(),
                            'context': match.group()
                        })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
        for event_type, patterns in self.event_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    event_desc = match.group(1) if match.groups() else match.group()
                    extracted['events'].append({
                        'type': event_type,
                        'description': event_desc.strip(),
                        'context': match.group()
                    })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø£Ø­Ù„Ø§Ù…
        for emotion_type, patterns in self.emotional_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    emotion_desc = match.group(1) if match.groups() else match.group()
                    extracted['emotions'].append({
                        'type': emotion_type,
                        'content': emotion_desc.strip(),
                        'context': match.group()
                    })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
        for pref_type, patterns in self.preference_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    pref_content = match.group(2) if len(match.groups()) >= 2 else match.group(1)
                    if pref_content:
                        extracted['preferences'].append({
                            'type': pref_type,
                            'content': pref_content.strip(),
                            'context': match.group(),
                            'confidence': 0.8
                        })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙØ¶ÙŠÙ„Ø§Øª Ù…Ø³ØªÙ†ØªØ¬Ø© Ù…Ù† Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
        inferred = self.extract_inferred_preferences(text)
        extracted['inferred_preferences'].extend(inferred)
        
        return extracted
    
    def extract_inferred_preferences(self, text: str) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙØ¶ÙŠÙ„Ø§Øª Ù…Ø³ØªÙ†ØªØ¬Ø© Ù…Ù† Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©"""
        inferred = []
        
        # Ø£Ù†Ù…Ø§Ø· Ù„Ù„Ø¬Ù…Ù„ Ø§Ù„ØªÙŠ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ ØªÙØ¶ÙŠÙ„Ø§Øª
        inference_patterns = [
            (r'ÙƒÙ†Øª (Ø£Ø´Ø±Ø¨|Ø£ØªÙ†Ø§ÙˆÙ„) ([\w\u0600-\u06FF\s]+) (Ù…Ø¹|Ø£Ø«Ù†Ø§Ø¡|ÙÙŠ)', 'food_preferences', 0.6),
            (r'Ø´Ø§Ù‡Ø¯Øª (ÙÙŠÙ„Ù…|Ù…Ø³Ù„Ø³Ù„) ([\w\u0600-\u06FF\s]+) (Ùˆ|Ø«Ù…)', 'entertainment', 0.7),
            (r'Ø°Ù‡Ø¨Øª Ø¥Ù„Ù‰ ([\w\u0600-\u06FF\s]+) (Ù„Ù€|Ù…Ù† Ø£Ø¬Ù„)', 'activities', 0.5),
            (r'Ø§Ø³ØªÙ…ØªØ¹Øª Ø¨Ù€ ([\w\u0600-\u06FF\s]+) (ÙƒØ«ÙŠØ±Ø§Ù‹|Ø¬Ø¯Ø§Ù‹)', 'enjoyment', 0.8),
            (r'Ø£ÙØ¶Ù„ ÙˆÙ‚Øª Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙŠ Ù‡Ùˆ ([\w\u0600-\u06FF\s]+)', 'schedule_preferences', 0.7),
            (r'Ø£Ø­Ø¨ Ø£Ù† ([\w\u0600-\u06FF\s]+) ÙÙŠ ([\w\u0600-\u06FF\s]+)', 'routine', 0.6),
            # Ø£Ù†Ù…Ø§Ø· Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
            (r'(Ø£Ø´Ø±Ø¨|Ø£ØªÙ†Ø§ÙˆÙ„) ([\w\u0600-\u06FF\s]+) (ÙƒÙ„|Ø¹Ø§Ø¯Ø©)', 'frequent_preferences', 0.7),
            (r'(Ø£Ø°Ù‡Ø¨|Ø£Ø²ÙˆØ±) ([\w\u0600-\u06FF\s]+) (ÙƒØ«ÙŠØ±Ø§Ù‹|Ø¹Ø§Ø¯Ø©)', 'frequent_places', 0.6),
            (r'(Ø£ÙØ¶Ù„|Ø£Ø­Ø¨) Ø£Ù† ([\w\u0600-\u06FF\s]+) Ø¹Ù†Ø¯Ù…Ø§ ([\w\u0600-\u06FF\s]+)', 'contextual_preferences', 0.5),
            (r'(Ù…Ø¹|Ø¨ØµØ­Ø¨Ø©) ([\w\u0600-\u06FF\s]+) (Ù†Ù‚ÙˆÙ…|Ù†Ø°Ù‡Ø¨)', 'social_preferences', 0.6)
        ]
        
        for pattern, pref_type, confidence in inference_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                content = match.group(2) if len(match.groups()) >= 2 else match.group(1)
                if content and len(content.strip()) > 2:
                    inferred.append({
                        'type': pref_type,
                        'content': content.strip(),
                        'context': match.group(),
                        'confidence': confidence,
                        'inferred': True
                    })
        
        return inferred
    
    def save_extracted_info(self, user_id: str, extracted_info: Dict):
        """Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©"""
        
        # Ø­ÙØ¸ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
        for relationship in extracted_info['relationships']:
            if relationship['type'] == 'current_girlfriend':
                self.memory.add_relationship(
                    user_id, relationship['person'], 'girlfriend', 'current'
                )
            elif relationship['type'] == 'ex_relationships':
                self.memory.add_relationship(
                    user_id, relationship['person'], 'ex-girlfriend', 'past'
                )
            elif relationship['type'] == 'friends':
                self.memory.add_relationship(
                    user_id, relationship['person'], 'friend', 'current'
                )
        
        # Ø­ÙØ¸ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
        for event in extracted_info['events']:
            if event['type'] == 'betrayal':
                self.memory.add_memory(
                    user_id, MemoryCategory.TRAUMATIC,
                    f"Ø®ÙŠØ§Ù†Ø© - {event['description']}",
                    event['context'],
                    emotions=['Ø­Ø²Ù†', 'ØºØ¶Ø¨', 'Ø®ÙŠØ§Ù†Ø©'],
                    intensity=5,
                    importance=4,
                    is_sensitive=True
                )
            elif event['type'] == 'travel':
                self.memory.add_memory(
                    user_id, MemoryCategory.TRAVEL,
                    f"Ø±Ø­Ù„Ø© Ø¥Ù„Ù‰ {event['description']}",
                    event['context'],
                    emotions=['Ø³Ø¹Ø§Ø¯Ø©', 'Ø­Ù…Ø§Ø³'],
                    intensity=3
                )
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø£Ø­Ù„Ø§Ù…
        for emotion in extracted_info['emotions']:
            if emotion['type'] == 'fears':
                self.memory.add_memory(
                    user_id, MemoryCategory.FEAR,
                    f"Ø®ÙˆÙ Ù…Ù† {emotion['content']}",
                    emotion['context'],
                    emotions=['Ø®ÙˆÙ', 'Ù‚Ù„Ù‚'],
                    intensity=4
                )
            elif emotion['type'] == 'dreams':
                self.memory.add_memory(
                    user_id, MemoryCategory.DREAM,
                    f"Ø­Ù„Ù…: {emotion['content']}",
                    emotion['context'],
                    emotions=['Ø£Ù…Ù„', 'Ø·Ù…ÙˆØ­'],
                    importance=4
                )
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆØ§Ù„Ù…Ø³ØªÙ†ØªØ¬Ø©
        all_preferences = extracted_info['preferences'] + extracted_info['inferred_preferences']
        for preference in all_preferences:
            if preference.get('confidence', 0) > 0.5:  # Ø¹ØªØ¨Ø© Ø«Ù‚Ø©
                self.memory.add_memory(
                    user_id, MemoryCategory.PREFERENCE,
                    f"ØªÙØ¶ÙŠÙ„: {preference['type']}",
                    f"{preference['content']} (Ù…Ø³ØªÙ†ØªØ¬: {preference.get('inferred', False)})",
                    emotions=['ØªÙØ¶ÙŠÙ„', 'Ø§Ù‡ØªÙ…Ø§Ù…'],
                    importance=2 if preference.get('inferred') else 3,
                    subcategory=preference['type']
                )

def handle_memory_query(memory_system: UniversalMemorySystem, user_id: str, query_type: str) -> str:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø¹Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    
    if query_type == 'relationships':
        relationships = memory_system.get_relationship_network(user_id)
        current_rels = relationships['current']
        
        if current_rels:
            people = [f"{rel[0]} ({rel[1]})" for rel in current_rels[:3]]
            return f"Ø£ØªØ°ÙƒØ± Ø£Ù†Ùƒ ØªØ­Ø¯Ø«Øª Ø¹Ù†: {', '.join(people)}"
        else:
            return "Ù„Ù… ØªØ®Ø¨Ø±Ù†ÙŠ Ø¨Ø¹Ø¯ Ø¹Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…Ù‡Ù…ÙŠÙ† ÙÙŠ Ø­ÙŠØ§ØªÙƒ."
    
    elif query_type == 'timeline':
        timeline = memory_system.get_life_timeline(user_id)
        if timeline:
            events = [event['title'] for event in timeline[:3]]
            return f"Ù…Ù† Ø°ÙƒØ±ÙŠØ§ØªÙƒ Ø§Ù„Ù…Ù‡Ù…Ø©: {'ØŒ '.join(events)}"
        else:
            return "Ù„Ù… ØªØ´Ø§Ø±ÙƒÙ†ÙŠ Ø¨Ø¹Ø¯ Ø¨Ø£Ø­Ø¯Ø§Ø« Ù…Ù‡Ù…Ø© Ù…Ù† Ø­ÙŠØ§ØªÙƒ."
    
    elif query_type == 'memories':
        memories = memory_system.search_memories(user_id, limit=3)
        if memories:
            memory_titles = [mem['title'] for mem in memories]
            return f"Ø£ØªØ°ÙƒØ± Ø£Ù†Ùƒ Ø°ÙƒØ±Øª: {'ØŒ '.join(memory_titles)}"
        else:
            return "Ø³Ø£ÙƒÙˆÙ† Ø³Ø¹ÙŠØ¯Ø§Ù‹ Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† Ø°ÙƒØ±ÙŠØ§ØªÙƒ ÙˆØªØ¬Ø§Ø±Ø¨Ùƒ."
    
    return "Ø£ÙÙ‡Ù… Ø£Ù†Ùƒ ØªØ³Ø£Ù„ Ø¹Ù† Ø°ÙƒØ±ÙŠØ§ØªÙƒ. ÙŠÙ…ÙƒÙ†Ù†ÙŠ ØªØ°ÙƒØ± ÙƒÙ„ Ù…Ø§ ØªØ´Ø§Ø±ÙƒÙ†ÙŠ Ø¨Ù‡."

def generate_contextual_response(extracted_info: Dict, user_input: str) -> str:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¯ÙˆØ¯ Ø°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚"""
    
    empathetic_responses = {
        'betrayal': [
            "Ø¢Ø³Ù Ù„Ø³Ù…Ø§Ø¹ Ø°Ù„Ùƒ. Ø§Ù„Ø®ÙŠØ§Ù†Ø© Ù…Ø¤Ù„Ù…Ø© Ø¬Ø¯Ø§Ù‹.",
            "Ù‡Ø°Ø§ must ÙŠÙƒÙˆÙ† ØµØ¹Ø¨Ø§Ù‹. ÙƒÙŠÙ ØªØ¹Ø§Ù…Ù„Øª Ù…Ø¹ Ø§Ù„Ù…ÙˆÙ‚ÙØŸ",
            "Ø£Ù‚Ø¯Ø± ØµØ±Ø§Ø­ØªÙƒ ÙÙŠ Ù…Ø´Ø§Ø±ÙƒØ© Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØµØ¹Ø¨."
        ],
        'travel': [
            "Ø±Ø§Ø¦Ø¹! Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø±Ø­Ù„Ø©.",
            "Ø§Ù„Ø£Ø³ÙØ§Ø± ØªØ¬Ø§Ø±Ø¨ Ø¬Ù…ÙŠÙ„Ø©. Ù…Ø§ Ø£Ø¬Ù…Ù„ Ø°ÙƒØ±ÙŠØ§ØªÙƒ Ù‡Ù†Ø§ÙƒØŸ",
            "ÙƒÙ… ÙƒØ§Ù† Ø±Ø§Ø¦Ø¹Ø§Ù‹! Ù‡Ù„ ØªØ®Ø·Ø· Ù„Ø±Ø­Ù„Ø© Ø£Ø®Ø±Ù‰ØŸ"
        ],
        'fears': [
            "Ø£ÙÙ‡Ù… Ù…Ø®Ø§ÙˆÙÙƒ. ÙƒÙ„Ù†Ø§ Ù„Ø¯ÙŠÙ†Ø§ Ù…Ø§ ÙŠØ®ÙŠÙÙ†Ø§.",
            "Ø´ÙƒØ±Ø§Ù‹ Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ù‡Ø°Ø§ Ù…Ø¹ÙŠ. Ø§Ù„Ø®ÙˆÙ Ø£Ù…Ø± Ø·Ø¨ÙŠØ¹ÙŠ.",
            "Ù…Ø®Ø§ÙˆÙÙ†Ø§ Ø¬Ø²Ø¡ Ù…Ù† Ø¥Ù†Ø³Ø§Ù†ÙŠØªÙ†Ø§."
        ],
        'dreams': [
            "Ø­Ù„Ù… Ø¬Ù…ÙŠÙ„! Ø³Ø£Ø¯Ø¹Ù…Ùƒ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚Ù‡.",
            "Ø·Ù…ÙˆØ­ Ø±Ø§Ø¦Ø¹! Ù…Ø§ Ø®Ø·Ø·Ùƒ Ù„ØªØ­Ù‚ÙŠÙ‚Ù‡ØŸ",
            "Ø£Ø­Ø¨ Ø£Ø­Ù„Ø§Ù…Ùƒ ÙˆØ·Ù…ÙˆØ­Ø§ØªÙƒ."
        ]
    }
    
    # Ø§Ø®ØªÙŠØ§Ø± Ø±Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    for event_type in ['betrayal', 'travel', 'fears', 'dreams']:
        if any(event['type'] == event_type for event in extracted_info.get('events', [])):
            import random
            return random.choice(empathetic_responses[event_type])
    
    # Ø±Ø¯ Ø¹Ø§Ù… Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    return "Ø´ÙƒØ±Ø§Ù‹ Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ù‡Ø°Ø§ Ù…Ø¹ÙŠ. Ø³Ø£ØªØ°ÙƒØ±Ù‡ ÙˆØ£ØªØ¹Ù„Ù… Ù…Ù†Ùƒ."

# =============== Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø´Ø§Ù…Ù„Ø© ===============
class ComprehensiveKnowledgeBase:
    def __init__(self, kb_path="knowledge_base.db"):
        self.kb_path = kb_path
        self._init_knowledge_base()
        self._load_or_create_knowledge()
    
    def _init_knowledge_base(self):
        """Initialize knowledge base database"""
        conn = sqlite3.connect(self.kb_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge_entries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                question TEXT UNIQUE,
                answers TEXT,  -- JSON array of answers
                category TEXT,
                language TEXT,
                confidence REAL DEFAULT 1.0,
                usage_count INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON knowledge_entries(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_language ON knowledge_entries(language)')
        
        conn.commit()
        conn.close()
    
    def _load_or_create_knowledge(self):
        """Load or create initial knowledge base with 1000+ entries"""
        conn = sqlite3.connect(self.kb_path)
        cursor = conn.cursor()
        
        # Check if knowledge base is empty
        cursor.execute('SELECT COUNT(*) FROM knowledge_entries')
        count = cursor.fetchone()[0]
        
        if count == 0:
            print("Creating comprehensive knowledge base...")
            self._create_initial_knowledge_base()
        
        conn.close()
    
    def _create_initial_knowledge_base(self):
        """Create initial knowledge base with 1000+ entries"""
        knowledge_data = self._generate_knowledge_entries()
        
        conn = sqlite3.connect(self.kb_path)
        cursor = conn.cursor()
        
        for entry in knowledge_data:
            answers_json = json.dumps(entry['answers'], ensure_ascii=False)
            
            cursor.execute('''
                INSERT OR IGNORE INTO knowledge_entries 
                (question, answers, category, language, confidence)
                VALUES (?, ?, ?, ?, ?)
            ''', (entry['question'], answers_json, entry['category'], 
                  entry['language'], entry.get('confidence', 1.0)))
        
        conn.commit()
        conn.close()
        
        print(f"Created knowledge base with {len(knowledge_data)} entries")
    
    def _generate_knowledge_entries(self) -> List[Dict[str, Any]]:
        """Generate 1000+ knowledge entries with 6 answers each"""
        entries = []
        
        # Science and Technology (150 entries)
        science_questions = [
            {
                "question": "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ©ØŸ",
                "answers": [
                    "Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© Ù‡ÙŠ Ù‚ÙˆØ© Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ¬Ø°Ø¨ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… toward Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶",
                    "Ù‡ÙŠ Ø§Ù„Ù‚ÙˆØ© Ø§Ù„ØªÙŠ ØªÙ…Ø³ÙƒÙ†Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ø¶ ÙˆØªØ¬Ø¹Ù„ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ØªØ³Ù‚Ø·",
                    "Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© ØªØ¬Ø¹Ù„ Ø§Ù„ÙƒÙˆØ§ÙƒØ¨ ØªØ¯ÙˆØ± Ø­ÙˆÙ„ Ø§Ù„Ø´Ù…Ø³",
                    "Ø§ÙƒØªØ´ÙÙ‡Ø§ Ù†ÙŠÙˆØªÙ† ÙˆÙ‡ÙŠ ØªØªÙ†Ø§Ø³Ø¨ Ø¹ÙƒØ³ÙŠØ§Ù‹ Ù…Ø¹ Ù…Ø±Ø¨Ø¹ Ø§Ù„Ù…Ø³Ø§ÙØ©",
                    "Ù‚ÙˆØ© Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ÙƒØªÙ„Ø© Ø§Ù„Ø£Ø¬Ø³Ø§Ù… ÙˆØ§Ù„Ù…Ø³Ø§ÙØ© Ø¨ÙŠÙ†Ù‡Ø§",
                    "Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ©ØŒ Ø³Ù†Ø·ÙŠØ± ÙÙŠ Ø§Ù„ÙØ¶Ø§Ø¡ ÙˆÙ„Ø§ Ù†Ø³ØªØ·ÙŠØ¹ Ø§Ù„Ù…Ø´ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ø¶"
                ],
                "category": "science",
                "language": "ar"
            },
            {
                "question": "How does photosynthesis work?",
                "answers": [
                    "Photosynthesis is how plants convert sunlight into energy",
                    "Plants use sunlight, water and CO2 to create glucose and oxygen",
                    "It occurs in chloroplasts using chlorophyll pigment",
                    "The process has light-dependent and light-independent reactions",
                    "Photosynthesis provides oxygen for animals to breathe",
                    "Without photosynthesis, life on Earth wouldn't exist as we know it"
                ],
                "category": "science", 
                "language": "en"
            }
        ]
        
        # Add more categories: history, geography, programming, health, etc.
        # For brevity, showing sample structure. Actual implementation would have 1000+ entries
        
        return entries
    
    def search_knowledge(self, query: str, language: str = "ar", top_k: int = 3) -> List[Dict[str, Any]]:
        """Search knowledge base for similar questions"""
        conn = sqlite3.connect(self.kb_path)
        cursor = conn.cursor()
        
        # Keyword search
        cursor.execute('''
            SELECT question, answers, category, confidence, usage_count
            FROM knowledge_entries 
            WHERE language = ? AND question LIKE ?
            ORDER BY confidence DESC, usage_count DESC
            LIMIT ?
        ''', (language, f'%{query}%', top_k))
        
        results = []
        for row in cursor.fetchall():
            question, answers_json, category, confidence, usage_count = row
            answers = json.loads(answers_json)
            
            results.append({
                'question': question,
                'answers': answers,
                'category': category,
                'confidence': confidence,
                'usage_count': usage_count,
                'match_type': 'keyword'
            })
            
            # Update usage count
            cursor.execute('''
                UPDATE knowledge_entries 
                SET usage_count = usage_count + 1 
                WHERE question = ?
            ''', (question,))
        
        conn.commit()
        conn.close()
        return results

# =============== Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ===============
class PersistentConversationMemory:
    """Ù†Ø¸Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ ØªØ®Ø²ÙŠÙ† Ø¯Ø§Ø¦Ù… ÙÙŠ SQLite"""
    
    def __init__(self, db_path="conversation_memory.db"):
        self.conn = sqlite3.connect(db_path)
        self._init_db()
        
    def _init_db(self):
        cursor = self.conn.cursor()
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS user_memory (
            user_id TEXT,
            key TEXT,
            value TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            PRIMARY KEY (user_id, key)
        )""")
        self.conn.commit()
        
    def add_user_memory(self, user_id: str, key: str, value: str):
        cursor = self.conn.cursor()
        cursor.execute("""
        INSERT OR REPLACE INTO user_memory (user_id, key, value)
        VALUES (?, ?, ?)
        """, (user_id, key, value))
        self.conn.commit()
        
    def get_user_memory(self, user_id: str, key: str) -> Optional[str]:
        cursor = self.conn.cursor()
        cursor.execute("""
        SELECT value FROM user_memory
        WHERE user_id = ? AND key = ?
        """, (user_id, key))
        result = cursor.fetchone()
        return result[0] if result else None
        
    def search_memory(self, user_id: str, query: str) -> Dict[str, str]:
        cursor = self.conn.cursor()
        cursor.execute("""
        SELECT key, value FROM user_memory
        WHERE user_id = ? AND (key LIKE ? OR value LIKE ?)
        """, (user_id, f"%{query}%", f"%{query}%"))
        return dict(cursor.fetchall())

    def get_user_count(self) -> int:
        cursor = self.conn.cursor()
        cursor.execute("SELECT COUNT(DISTINCT user_id) FROM user_memory")
        return cursor.fetchone()[0]
        
    def get_memory_count(self) -> int:
        cursor = self.conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM user_memory")
        return cursor.fetchone()[0]

    def add_question_response(self, user_id: str, question: str, response: str):
        """ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        question_hash = hashlib.sha256(question.encode()).hexdigest()[:16]
        key = f"last_response_{question_hash}"
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø³Ø§Ø¨Ù‚
        previous = self.get_user_memory(user_id, key)
        if previous:
            # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø±Ø¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ ÙƒÙ€ "Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø§Ø¶ÙŠ"
            self.add_user_memory(user_id, f"prev_{key}", previous)
        
        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø±Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ
        self.add_user_memory(user_id, key, response)
        
    def get_previous_responses(self, user_id: str, question: str, max_responses=2):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ø³Ø¤Ø§Ù„"""
        question_hash = hashlib.sha256(question.encode()).hexdigest()[:16]
        key = f"last_response_{question_hash}"
        
        responses = []
        # Ø§Ù„Ø±Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ
        current = self.get_user_memory(user_id, key)
        if current:
            responses.append(current)
        
        # Ø§Ù„Ø±Ø¯ Ø§Ù„Ø³Ø§Ø¨Ù‚
        prev = self.get_user_memory(user_id, f"prev_{key}")
        if prev:
            responses.append(prev)
        
        return responses[:max_responses]

# =============== Ø§Ù„Ø­Ø§Ø±Ø³ Ø§Ù„Ø£Ù…Ù†ÙŠ Ø§Ù„Ù…Ø­Ø³Ù† ===============
class EnhancedResponseGuard:
    def __init__(self):
        self.simple_facts = {
            "capital of canada": "Ottawa",
            "capital of france": "Paris",
            "founder of microsoft": "Bill Gates and Paul Allen",
            "number of planets": "8"
        }
        
        self.banned_keywords = BAD_TERMS
        self.supported_languages = {"en", "ar"}
        
    def is_math_question(self, text: str) -> bool:
        math_patterns = [
            r"\d+\s*[\+\-\*\/]\s*\d+",
            r"\b(solve|calculate|Ø­Ù„|Ø§Ø­Ø³Ø¨)\b",
            r"[\=\(\)]",
        ]
        return any(re.search(pattern, text.lower()) for pattern in math_patterns)

    def solve_math(self, text: str) -> Optional[str]:
        try:
            if re.match(r'^\d+\s*[\+\-\*\/]\s*\d+$', text):
                result = eval(text)
                return str(result)
                
            text = text.replace("^", "**")
            if "=" in text:
                x = sp.symbols('x')
                solution = sp.solve(text, x)
                return f"x = {solution[0]}" if solution else "No solution"
            else:
                expr = sp.sympify(text)
                return str(expr.evalf())
        except Exception:
            return None

    def is_sensitive(self, text: str) -> bool:
        """ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù†Øµ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø³ÙŠØ§Ù‚"""
        text_lower = text.lower()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø§Øª Ù…Ù…Ù†ÙˆØ¹Ø©
        has_banned_keywords = any(kw in text_lower for kw in self.banned_keywords)
        
        if not has_banned_keywords:
            return False
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        context_analysis = analyze_sensitive_context(text)
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø·Ù„Ø¨ Ù…Ø³Ø§Ø¹Ø¯Ø©
        if context_analysis["context_type"] == "help_request":
            return False  # Ù„Ø§ ØªØ¹ØªØ¨Ø±Ù‡ Ù…Ø­ØªÙˆÙ‰ Ø¶Ø§Ø±Ø§Ù‹
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø¹Ù„Ø§Ø¬ÙŠ
        if context_analysis["context_type"] == "therapy_context":
            return False  # Ù„Ø§ ØªØ¹ØªØ¨Ø±Ù‡ Ù…Ø­ØªÙˆÙ‰ Ø¶Ø§Ø±Ø§Ù‹
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙˆØ¯ Ø£Ùˆ Ù†Øµ Ø·ÙˆÙŠÙ„ Ù…Ø¹Ù‚Ø¯
        if context_analysis["has_code"] or context_analysis["is_complex"]:
            # Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù…Ø¬Ø±Ø¯ Ù…Ù†Ø§Ù‚Ø´Ø© ØªÙ‚Ù†ÙŠØ©
            return False
        
        # ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ù†ÙŠØ© Ø¶Ø§Ø±Ø© ÙˆØ§Ø¶Ø­Ø©
        if context_analysis["context_type"] == "harmful_content":
            return True
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø´Ùƒ ÙÙŠ Ø§Ù„Ù†ÙŠØ© Ø§Ù„Ø¶Ø§Ø±Ø© Ù…Ø¹ Ù†Ù‚Ø§Ø· Ø¹Ø§Ù„ÙŠØ©
        if context_analysis["intent_score"] < -1:
            return True
        
        # Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
        return True

    def guard(self, question: str, raw_answer: str) -> str:
        if self.is_sensitive(question):
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£ÙˆÙ„Ø§Ù‹
            context_analysis = analyze_sensitive_context(question)
            
            if context_analysis["needs_help"]:
                # ØªÙ‚Ø¯ÙŠÙ… Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¢Ù…Ù†Ø© ÙˆÙ…ÙØµÙ„Ø©
                help_resources = [
                    "Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙˆØ±ÙŠØ©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø· Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„ÙˆØ·Ù†ÙŠ Ø¹Ù„Ù‰ 112",
                    "Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¯Ø¹Ù… Ù†ÙØ³ÙŠØŒ Ø£Ù†ØµØ­Ùƒ Ø¨Ø§Ù„ØªØ­Ø¯Ø« Ù…Ø¹ Ù…Ø®ØªØµ Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø· Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù†ÙØ³ÙŠ",
                    "ÙÙŠ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø·ÙˆØ§Ø±Ø¦ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø´Ø±Ø·Ø© Ø¹Ù„Ù‰ 122 Ø£Ùˆ Ø§Ù„Ø¥Ø³Ø¹Ø§Ù Ø¹Ù„Ù‰ 123",
                    "Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¶Ø­ÙŠØ© Ø§Ø¹ØªØ¯Ø§Ø¡ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙˆØ¬Ù‡ Ø¥Ù„Ù‰ Ø£Ù‚Ø±Ø¨ Ù…Ø±ÙƒØ² Ø´Ø±Ø·Ø© Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø· Ù†Ø¬Ø¯Ø© Ø§Ù„Ø·ÙÙ„ Ø¹Ù„Ù‰ 16000",
                    "ØªÙˆØ¬Ø¯ Ù…Ø±Ø§ÙƒØ² Ø¯Ø¹Ù… Ù†ÙØ³ÙŠ Ù…ØªØ®ØµØµØ© ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ø±Ø¨ Ø¥Ù„ÙŠÙƒ"
                ]
                return random.choice(help_resources)
            elif context_analysis["needs_guidance"]:
                # ØªÙˆØ¬ÙŠÙ‡ Ø¥Ù„Ù‰ Ù…ØµØ§Ø¯Ø± Ù…ØªØ®ØµØµØ©
                guidance_responses = [
                    "Ø£Ù‚Ø¯Ø± ØµØ±Ø§Ø­ØªÙƒ ÙÙŠ Ù…Ø´Ø§Ø±ÙƒØ© ØªØ¬Ø±Ø¨ØªÙƒ. Ù„Ù„Ø¹Ù„Ø§Ø¬ ÙˆØ§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù…ØªØ®ØµØµØŒ Ø£Ù†ØµØ­Ùƒ Ø¨Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù†ÙØ³ÙŠ Ø£Ùˆ Ù…Ø¹Ø§Ù„Ø¬ Ù…Ø¤Ù‡Ù„.",
                    "Ø´ÙƒØ±Ø§Ù‹ Ù„Ù…Ø´Ø§Ø±ÙƒØ© ØªØ¬Ø±Ø¨ØªÙƒ Ù…Ø¹ÙŠ. ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙˆØ§Ø±Ø¯ Ù„Ù„Ø¹Ù„Ø§Ø¬ ÙˆØ§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù†ÙØ³ÙŠ.",
                    "Ø£ØªÙÙ‡Ù… Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø­Ø³Ø§Ø³ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ùƒ. Ù‡Ù†Ø§Ùƒ Ù…ØªØ®ØµØµÙˆÙ† ÙŠÙ…ÙƒÙ†Ù‡Ù… ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ùƒ."
                ]
                return random.choice(guidance_responses)
            else:
                lang = detect_lang(question)
                return "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ù†Ø§Ù‚Ø´Ø© Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹." if lang == "ar" else "I can't discuss this topic."
        
        if self.is_math_question(question):
            math_ans = self.solve_math(question)
            if math_ans:
                return math_ans
        
        fact_response = self.get_fact_response(question)
        if fact_response:
            return fact_response
            
        return raw_answer
    
    def get_fact_response(self, question: str) -> Optional[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        question_lower = question.lower()
        for fact, answer in self.simple_facts.items():
            if fact in question_lower:
                return answer
        return None

# =============== Wikipedia Search Functions ===============
WIKI_HEADERS = {
    "User-Agent": "SaadBot/1.0 (+local; simple non-API fetch)",
    "Accept-Language": "ar,en;q=0.8"
}

def _clean_text(txt):
    txt = re.sub(r'\[\d+\]', '', txt)
    txt = re.sub(r'\s+', ' ', txt).strip()
    return html.unescape(txt)

def _extract_paragraphs(soup, max_paras=2):
    content = soup.select_one("div.mw-parser-output")
    if not content:
        return None
    paras = []
    for p in content.find_all("p", recursive=False):
        text = _clean_text(p.get_text(" ", strip=True))
        if text and len(text) > 30:
            paras.append(text)
        if len(paras) >= max_paras:
            break
    return " ".join(paras) if paras else None

def _first_search_result(soup, lang):
    # ÙŠÙØ¶Ù‘Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ù‚Ø³Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    link = soup.select_one("ul.mw-search-results li a")
    if link and link.get("href"):
        return f"https://{lang}.wikipedia.org{link['href']}"
    # fallback Ø¨Ø³ÙŠØ·: Ø£ÙˆÙ„ Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    link = soup.select_one("div.mw-parser-output ul li a")
    if link and link.get("href", "").startswith("/wiki/"):
        return f"https://{lang}.wikipedia.org{link['href']}"
    return None

def get_wikipedia_summary(query, lang="ar", max_paragraphs=2, timeout=8):
    """ÙŠØ±Ø¬Ø¹ (summary, url) Ø£Ùˆ (None, None) Ø¨Ø¹Ø¯ ÙØ­Øµ Ø§Ù„ØµÙ„Ø© ÙˆÙ…Ù†Ø¹ NSFW."""
    base = f"https://{lang}.wikipedia.org"
    slug = urllib.parse.quote(query.replace(" ", "_"))
    direct_url = f"{base}/wiki/{slug}"

    def _fetch(url):
        try:
            r = requests.get(url, headers=WIKI_HEADERS, timeout=timeout, allow_redirects=True)
            if 200 <= r.status_code < 300:
                return BeautifulSoup(r.text, "html.parser")
        except:
            return None
        return None

    def _title_of(soup):
        h1 = soup.select_one("#firstHeading")
        return _clean_text(h1.get_text("", strip=True)) if h1 else ""

    # 1) ØµÙØ­Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
    soup = _fetch(direct_url)
    if soup:
        title = _title_of(soup)
        txt = _extract_paragraphs(soup, max_paragraphs) or ""
        if txt and not looks_nsfw(title, txt) and is_relevant(txt, query):
            return smart_shorten(txt, 2, 320), direct_url

    # 2) Ø§Ù„Ø¨Ø­Ø«
    search_url = f"{base}/w/index.php?search={urllib.parse.quote(query)}"
    soup = _fetch(search_url)
    if soup:
        first = _first_search_result(soup, lang)
        if first:
            soup2 = _fetch(first)
            if soup2:
                title = _title_of(soup2)
                txt = _extract_paragraphs(soup2, max_paragraphs) or ""
                if txt and not looks_nsfw(title, txt) and is_relevant(txt, query):
                    return smart_shorten(txt, 2, 320), first

    return None, None

# =============== ÙˆØ§Ø¬Ù‡Ø© Flask API ===============
app = Flask(__name__)

def clear_text(text):
    """ØªÙ‚ÙˆÙ… Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù†Øµ Ø¹Ù†Ø¯ Ø£ÙˆÙ„ Ù†Ù‚Ø·Ø© ÙˆØªØ²ÙŠÙ„ Ø£ÙŠ Ù†Øµ Ø¨Ø¹Ø¯Ù‡Ø§"""
    if '.' in text:
        text = text.split('.')[0].strip() + '.'
    else:
        text = text + '.'
    return text

def ensure_arabic_response(text: str, original_question: str) -> str:
    """Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØªØµØ­ÙŠØ­ Ø§Ù„ØªØ±Ø¬Ù…Ø§Øª Ø§Ù„Ø®Ø§Ø·Ø¦Ø©"""
    if not text:
        return text
        
    # ÙƒØ´Ù Ø§Ù„ØªØ±Ø¬Ù…Ø§Øª Ø§Ù„Ø®Ø§Ø·Ø¦Ø©
    bad_patterns = [
        (r'\([A-Z]+\)', "Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¨ÙŠÙ† Ù‚ÙˆØ³ÙŠÙ† Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©"),
        (r'[A-Z][a-z]+\s+is', "Ø¬Ù…Ù„ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙÙŠ Ø§Ù„Ù†Øµ"),
        (r'means\s+".*?"', "ØªØ¹Ø±ÙŠÙØ§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©")
    ]
    
    for pattern, problem in bad_patterns:
        if re.search(pattern, text):
            # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            question_lang = detect_lang(original_question)
            if question_lang == "ar":
                return f"Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ '{original_question}' Ù‡Ùˆ: {extract_main_answer(text)}"
    
    return text

def extract_main_answer(text: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù…Ù† Ø§Ù„Ù†Øµ"""
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙØ§Øª Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    lines = text.split('\n')
    clean_lines = []
    
    for line in lines:
        if not re.search(r'\([A-Z]+\)', line) and not re.search(r'means\s+".*?"', line):
            clean_lines.append(line)
    
    return ' '.join(clean_lines[:2])  # Ø£ÙˆÙ„ Ø³Ø·Ø±ÙŠÙ† ÙÙ‚Ø·

def generate_varied_response_template(question: str, previous_responses: List[str], 
                                     sentiment: str, intent: str, lang: str) -> str:
    """Ø¥Ù†Ø´Ø§Ø¡ Ù‚ÙˆØ§Ù„Ø¨ Ø±Ø¯ÙˆØ¯ Ù…ØªÙ†ÙˆØ¹Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø±"""
    
    # Ù‚ÙˆØ§Ù„Ø¨ Ù„Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    arabic_templates = {
        "factual": [
            "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø£Ù† {}",
            "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø©ØŒ {}",
            "Ø­Ø³Ø¨ Ù…Ø§ Ù‡Ùˆ Ù…Ø¹Ø±ÙˆÙØŒ {}",
            "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ù‡ÙŠ: {}",
            "ÙŠÙ…ÙƒÙ† Ø§Ù„Ù‚ÙˆÙ„ Ø£Ù† {}"
        ],
        "explanation": [
            "Ù„ÙÙ‡Ù… Ù‡Ø°Ø§ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ØŒ {}",
            "Ù„Ø´Ø±Ø­ Ø°Ù„Ùƒ Ø¨Ø¨Ø³Ø§Ø·Ø©ØŒ {}",
            "ÙŠÙ…ÙƒÙ† ØªÙˆØ¶ÙŠØ­ Ø°Ù„Ùƒ Ø¨Ø£Ù† {}",
            "Ø§Ù„Ù…Ù‚ØµÙˆØ¯ Ù‡Ù†Ø§ Ù‡Ùˆ {}",
            "Ø¨ØªÙØµÙŠÙ„ Ø£ÙƒØ«Ø±ØŒ {}"
        ],
        "emotional": {
            "appreciation": [
                "Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§ØªÙƒ Ø§Ù„Ù„Ø·ÙŠÙØ©! {}",
                "Ø£Ù‚Ø¯Ø± Ù…Ø´Ø§Ø¹Ø±Ùƒ Ø§Ù„Ø¬Ù…ÙŠÙ„Ø©. {}",
                "Ù„Ø·ÙŠÙ Ù…Ù†Ùƒ Ø£Ù† ØªÙ‚ÙˆÙ„ Ø°Ù„Ùƒ. {}",
                "Ø´ÙƒØ±Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ‚Ø¯ÙŠØ±. {}",
                "Ù‡Ø°Ø§ ÙŠØ¹Ø·ÙŠÙ†ÙŠ Ø¯Ø§ÙØ¹Ø§Ù‹ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£ÙƒØ«Ø±. {}"
            ],
            "support_needed": [
                "Ø£ØªÙÙ‡Ù… Ù…Ø´Ø§Ø¹Ø±Ùƒ. {}",
                "Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ. {}",
                "Ù„Ø§ Ø¨Ø£Ø³ØŒ ÙƒÙ„Ù†Ø§ Ù†Ù…Ø± Ø¨Ø¸Ø±ÙˆÙ ØµØ¹Ø¨Ø©. {}",
                "Ø£Ù‚Ø¯Ø± ØµØ±Ø§Ø­ØªÙƒ. {}",
                "Ø¯Ø¹Ù†Ø§ Ù†Ø¨Ø­Ø« Ø¹Ù† Ø­Ù„ Ù…Ø¹Ø§Ù‹. {}"
            ]
        }
    }
    
    # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„
    question_lower = question.lower()
    if any(word in question_lower for word in ["Ù…Ø§ Ù‡ÙŠ", "Ù…Ø§ Ù‡Ùˆ", "Ù…Ø§", "Ù…Ø§Ø°Ø§"]):
        template_type = "factual"
    elif any(word in question_lower for word in ["ÙƒÙŠÙ", "Ù„Ù…Ø§Ø°Ø§", "Ø´Ø±Ø­"]):
        template_type = "explanation"
    else:
        template_type = "factual"
    
    # Ø§Ø®ØªÙŠØ§Ø± Ù‚Ø§Ù„Ø¨ Ù„Ù… ÙŠØ³ØªØ®Ø¯Ù… Ù…Ù† Ù‚Ø¨Ù„
    if template_type == "factual" or template_type == "explanation":
        available_templates = arabic_templates[template_type]
        
        # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
        for prev_response in previous_responses:
            for template in available_templates[:]:
                if template.format("") in prev_response:
                    available_templates.remove(template)
        
        if available_templates:
            return random.choice(available_templates)
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ù…Ø´Ø§Ø¹Ø± Ø®Ø§ØµØ©
    if intent in arabic_templates["emotional"]:
        emotional_templates = arabic_templates["emotional"][intent]
        
        # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
        for prev_response in previous_responses:
            for template in emotional_templates[:]:
                if template.format("") in prev_response:
                    emotional_templates.remove(template)
        
        if emotional_templates:
            return random.choice(emotional_templates)
    
    # Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
    return "{}"

def generate_arabic_response(question: str, lang: str, temperature: float = 0.3,
                            previous_responses: List[str] = None,
                            sentiment: str = "neutral", intent: str = "general") -> str:
    """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenRouter API"""
    
    if previous_responses is None:
        previous_responses = []
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    style_pref = StylePreferences()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø³Ø±ÙŠØ¹ ÙˆÙØ¹Ø§Ù„
    if lang == "ar":
        # Ø§Ø®ØªÙŠØ§Ø± Ù‚Ø§Ù„Ø¨ Ù…ØªÙ†ÙˆØ¹
        response_template = generate_varied_response_template(
            question, previous_responses, sentiment, intent, lang
        )
        
        system_content = f"""Ø£Ù†Øª Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ - Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…Ø¨Ø¯Ø¹ ÙŠØªØ³Ù… Ø¨Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©.
        
Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙˆØ¯Ù‚ÙŠÙ‚.
Ø§Ø¨Ø¯Ø¥ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø¯ÙˆÙ† Ù…Ù‚Ø¯Ù…Ø§Øª Ø·ÙˆÙŠÙ„Ø©.
ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆÙˆØ§Ø¶Ø­Ø§Ù‹ ÙÙŠ Ø§Ù„Ø´Ø±Ø­.
Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© Ø³Ù„ÙŠÙ…Ø©.

{style_pref.get_response_style_prompt()}"""
        
        user_content = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\n\nØ§Ù„Ø±Ø¯: {response_template.format('')}"
    else:
        system_content = """You are Saad Al-Kawni - an intelligent and creative assistant known for accuracy and professionalism.
        
Answer the following question clearly and accurately.
Start directly with the answer without long introductions.
Be precise in information and clear in explanation."""
        
        user_content = f"Question: {question}\n\nAnswer:"
    
    # Ø¨Ù†Ø§Ø¡ Ø±Ø³Ø§Ø¦Ù„ OpenAI-compatible
    messages = [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_content}
    ]
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenRouter API
    response = generate_via_openrouter(
        messages=messages,
        temperature=0.4,  # Ø«Ø§Ø¨Øª - Ù„Ø§ ÙŠØªØºÙŠØ±
        max_tokens=1024,
        model="meta-llama/llama-3.1-405b-instruct:free"
    )
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ù†ÙŠØ©
    emoji = style_pref.get_sentiment_emoji(sentiment, intent)
    if emoji and response:
        response = f"{emoji} {response}"
    
    return response.strip() if response else "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯."

# =============== Ø¯Ø§Ù„Ø© Ø®Ø§ØµØ© Ù„Ù„ØªØ­ÙŠØ© Ù…Ø¹ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ===============
def generate_greeting_with_name(user_id: str, memory_system: PersistentConversationMemory) -> str:
    """Ø¥Ù†Ø´Ø§Ø¡ ØªØ­ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø¹Ø±ÙˆÙÙ‹Ø§"""
    user_name = memory_system.get_user_memory(user_id, "name")
    if user_name:
        greeting_options = [
            f"Ù…Ø±Ø­Ø¨Ø§Ù‹ {user_name}! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ",
            f"Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ {user_name}! Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ­ØªØ§Ø¬ Ø¥Ù„ÙŠÙ‡ØŸ",
            f"Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ {user_name}! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø£Ù† Ø£ÙƒÙˆÙ† Ù…ÙÙŠØ¯Ø§Ù‹ Ù„ÙƒØŸ",
            f"ØªØ­ÙŠØ§ØªÙŠ {user_name}! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ",
            f"Ø£Ù‡Ù„Ù‹Ø§ {user_name}! Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ù…Ù†Ø§Ù‚Ø´ØªÙ‡ Ø§Ù„ÙŠÙˆÙ…ØŸ"
        ]
        return random.choice(greeting_options)
    else:
        greeting_options = [
            "Ù…Ø±Ø­Ø¨Ø§Ù‹! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ",
            "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ­ØªØ§Ø¬ Ø¥Ù„ÙŠÙ‡ØŸ",
            "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø£Ù† Ø£ÙƒÙˆÙ† Ù…ÙÙŠØ¯Ø§Ù‹ Ù„ÙƒØŸ",
            "ØªØ­ÙŠØ§ØªÙŠ! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ",
            "Ø£Ù‡Ù„Ù‹Ø§! Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ù…Ù†Ø§Ù‚Ø´ØªÙ‡ Ø§Ù„ÙŠÙˆÙ…ØŸ"
        ]
        return random.choice(greeting_options)

# =============== Ø¯Ø§Ù„Ø© Ø®Ø§ØµØ© Ù„Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ "Ù…Ù† Ø§Ù„Ø°ÙŠ Ù‚Ø§Ù… Ø¨ØªØ·ÙˆÙŠØ±Ùƒ" ===============
def handle_developer_question() -> str:
    """Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ù…Ù† Ù‚Ø§Ù… Ø¨ØªØ·ÙˆÙŠØ± Ø§Ù„Ù†Ø¸Ø§Ù…"""
    responses = [
        "Ù‚Ø§Ù… Ø¨ØªØ·ÙˆÙŠØ±ÙŠ Ø£Ø­Ù…Ø¯ Ø³Ø¹Ø¯ØŒ ÙˆÙ‚Ø¯ Ø¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ù…Ø´Ø±ÙˆØ¹ÙŠ Ù…Ù†Ø° Ù…Ù†ØªØµÙ Ø´Ù‡Ø± ÙŠÙˆÙ„ÙŠÙˆ.",
        "Ù…Ø·ÙˆØ±ÙŠ Ù‡Ùˆ Ø£Ø­Ù…Ø¯ Ø³Ø¹Ø¯ØŒ ÙˆØ¨Ø¯Ø£ ÙÙŠ Ø¨Ø±Ù…Ø¬ØªÙŠ Ù…Ù†Ø° ÙŠÙˆÙ„ÙŠÙˆ Ø§Ù„Ù…Ø§Ø¶ÙŠ.",
        "Ø£Ù†Ø´Ø£Ù†ÙŠ Ø£Ø­Ù…Ø¯ Ø³Ø¹Ø¯ØŒ ÙˆØ¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØµÙ…ÙŠÙ…ÙŠ ÙÙŠ ÙŠÙˆÙ„ÙŠÙˆ.",
        "Ø¨Ø±Ù…Ø¬Ù†ÙŠ Ø£Ø­Ù…Ø¯ Ø³Ø¹Ø¯ØŒ ÙˆÙ‚Ø¯ Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙÙŠ ÙŠÙˆÙ„ÙŠÙˆ.",
        "Ø§Ù„Ù…Ø·ÙˆØ± ÙˆØ±Ø§Ø¦ÙŠ Ù‡Ùˆ Ø£Ø­Ù…Ø¯ Ø³Ø¹Ø¯ØŒ ÙˆÙ‚Ø¯ Ø¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„ Ù…Ù†Ø° ÙŠÙˆÙ„ÙŠÙˆ."
    ]
    return random.choice(responses)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªÙ… Ø¥Ø²Ø§Ù„ØªÙ‡ ÙˆØ§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ù€ OpenRouter API
model = None
tokenizer = None

@app.route('/')
def index():
    return send_file('index.html')

@app.route('/api/chat', methods=['POST'])
def chat():
    data = request.get_json(force=True)
    user_input = data.get('message') or data.get('Ø±Ø³Ø§Ù„Ø©', '')
    user_input = user_input.strip()
    user_id = data.get('user_id', 'default')
    temperature = float(data.get('temperature', 0.3))
    feedback = data.get('feedback')  # like/dislike

    if not user_input:
        return jsonify({'Ø±Ø¯': 'Ù…Ù† ÙØ¶Ù„Ùƒ Ø£Ø¯Ø®Ù„ Ù†ØµØ§Ù‹.'})

    try:
        # Ø¨Ø¯Ø¡ Ù‚ÙŠØ§Ø³ Ø§Ù„ÙˆÙ‚Øª Ù„Ù„Ø³Ø±Ø¹Ø©
        start_time = time.time()
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙØ¶ÙŠÙ„Ø§Øª Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        user_style = get_user_style(user_id)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
        if feedback in ["like", "dislike"]:
            user_style.update_from_feedback(feedback)
            return jsonify({
                'Ø±Ø¯': f"ØªÙ… ØªØ­Ø¯ÙŠØ« ØªÙØ¶ÙŠÙ„Ø§ØªÙƒ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø±Ø¯Ùƒ ({feedback})",
                'updated_style': user_style.preferences
            })
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ù†ÙˆØ§ÙŠØ§
        sentiment_analysis = analyze_sentiment_and_intent(user_input)
        sentiment = sentiment_analysis["sentiment"]
        intent = sentiment_analysis["intent"]
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙˆØ¶ÙŠØ­
        if should_ask_for_clarification(user_input, detect_lang(user_input)):
            return jsonify({
                'Ø±Ø¯': "Ø£ÙŠ Ø¯ÙˆÙ„Ø© ØªÙ‚ØµØ¯ØŸ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø³Ø¤Ø§Ù„."
            })
        
        # ========== Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© ==========
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªØ­ÙŠØ©
        greeting_keywords = ["Ù…Ø±Ø­Ø¨Ø§", "Ù…Ø±Ø­Ø¨Ø§Ù‹", "Ø§Ù‡Ù„Ø§", "Ø£Ù‡Ù„Ø§Ù‹", "Ø³Ù„Ø§Ù…", "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ù‡Ø§ÙŠ", "hello", "hi", "hey"]
        user_input_lower = user_input.lower()
        is_greeting = any(keyword in user_input_lower for keyword in greeting_keywords)
        
        if is_greeting:
            memory_system = PersistentConversationMemory()
            greeting_response = generate_greeting_with_name(user_id, memory_system)
            return jsonify({'Ø±Ø¯': greeting_response})
        
        # ========== Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø·ÙˆØ± ==========
        developer_keywords = ["Ù…Ù† ØµÙ†Ø¹Ùƒ", "Ù…Ù† Ù‚Ø§Ù… Ø¨ØªØ·ÙˆÙŠØ±Ùƒ", "Ù…Ù† Ø¨Ø±Ù…Ø¬Ùƒ", "Ù…Ù† Ø£Ù†Ø´Ø£Ùƒ", "Ù…Ù† Ø§Ù„Ù…Ø·ÙˆØ±", "Ù…Ù† ØµØ§Ù†Ø¹Ùƒ", 
                             "who made you", "who created you", "who developed you", "who built you"]
        is_developer_question = any(keyword in user_input_lower for keyword in developer_keywords)
        
        if is_developer_question:
            developer_response = handle_developer_question()
            return jsonify({'Ø±Ø¯': developer_response})
        
        # Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø´Ø§Ù…Ù„ Ø§Ù„Ù…Ø­Ø¯Ø«
        memory_system = UniversalMemorySystem()
        extractor = IntelligentMemoryExtractor(memory_system)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        extracted_info = extractor.extract_comprehensive_info(user_id, user_input)
        if extracted_info:
            extractor.save_extracted_info(user_id, extracted_info)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø¹Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        memory_queries = {
            'relationships': ['Ø¹Ù„Ø§Ù‚Ø§ØªÙŠ', 'Ø£ØµØ¯Ù‚Ø§Ø¦ÙŠ', 'ØµØ¯ÙŠÙ‚ØªÙŠ', 'Ø­Ø¨ÙŠØ¨ØªÙŠ', 'Ù…Ù† Ù‡Ù… Ø£ØµØ¯Ù‚Ø§Ø¦ÙŠ'],
            'memories': ['Ø°ÙƒØ±ÙŠØ§ØªÙŠ', 'Ø£Ø­Ø¯Ø§Ø«', 'Ù…ÙˆØ§Ù‚Ù', 'Ù„Ø§ Ø£Ù†Ø³Ù‰', 'Ø±Ø­ØªÙ„ÙŠ'],
            'timeline': ['Ø®Ø· Ø­ÙŠØ§ØªÙŠ', 'Ø£Ø­Ø¯Ø§Ø« Ø­ÙŠØ§ØªÙŠ', 'Ø±Ø­Ù„Ø© Ø­ÙŠØ§ØªÙŠ'],
            'secrets': ['Ø£Ø³Ø±Ø§Ø±', 'Ù…Ø®Ø§ÙˆÙÙŠ', 'Ø£Ø­Ù„Ø§Ù…ÙŠ']
        }
        
        for query_type, queries in memory_queries.items():
            for query in queries:
                if query in user_input:
                    response = handle_memory_query(memory_system, user_id, query_type)
                    return jsonify({'Ø±Ø¯': response})
        
        # Ø¥Ø°Ø§ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ø³Ø§Ø³Ø©
        if any(extracted_info.values()):
            response = generate_contextual_response(extracted_info, user_input)
            return jsonify({'Ø±Ø¯': response})
        
        # Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø¨Ø³ÙŠØ·
        memory = PersistentConversationMemory()
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„
        previous_responses = memory.get_previous_responses(user_id, user_input, max_responses=2)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©
        personal_responses = {
            "Ù…Ø§ Ù‡Ùˆ Ø§Ø³Ù…ÙŠ": lambda: memory.get_user_memory(user_id, "name"),
            "Ù…Ø§ Ø§Ø³Ù…ÙŠ": lambda: memory.get_user_memory(user_id, "name"), 
            "ÙƒÙ… Ø¹Ù…Ø±ÙŠ": lambda: memory.get_user_memory(user_id, "age"),
            "Ù…Ø§ Ù‡Ùˆ Ø¹Ù…Ø±ÙŠ": lambda: memory.get_user_memory(user_id, "age"),
            "Ø£ÙŠÙ† Ø£Ø³ÙƒÙ†": lambda: memory.get_user_memory(user_id, "location"),
            "Ø£ÙŠÙ† Ø£Ø¹ÙŠØ´": lambda: memory.get_user_memory(user_id, "location"),
            "Ù…ÙƒØ§Ù† Ø³ÙƒÙ†ÙŠ": lambda: memory.get_user_memory(user_id, "location")
        }
        
        for question, get_value in personal_responses.items():
            if question in user_input:
                value = get_value()
                if value:
                    return jsonify({'Ø±Ø¯': f'{value}'})
                else:
                    return jsonify({'Ø±Ø¯': f'Ù„Ù… ØªØ®Ø¨Ø±Ù†ÙŠ Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¨Ø¹Ø¯.'})
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©
        if "Ø§Ø³Ù…ÙŠ" in user_input:
            name_match = re.search(r'Ø§Ø³Ù…ÙŠ ([\w\u0600-\u06FF]+)', user_input)
            if name_match:
                name = name_match.group(1)
                memory.add_user_memory(user_id, "name", name)
                return jsonify({'Ø±Ø¯': f'Ø­Ø³Ù†Ø§Ù‹ {name}! Ø³Ø£ØªØ°ÙƒØ± Ø§Ø³Ù…Ùƒ.'})
                
        if "Ø¹Ù…Ø±ÙŠ" in user_input:
            age_match = re.search(r'Ø¹Ù…Ø±ÙŠ (\d+)', user_input)
            if age_match:
                age = age_match.group(1)
                memory.add_user_memory(user_id, "age", age)
                return jsonify({'Ø±Ø¯': f'Ø­Ø³Ù†Ø§Ù‹! Ø³Ø£ØªØ°ÙƒØ± Ø£Ù† Ø¹Ù…Ø±Ùƒ {age} Ø³Ù†Ø©.'})
                
        if "Ø£Ø¹ÙŠØ´ ÙÙŠ" in user_input or "Ø§Ø³ÙƒÙ† ÙÙŠ" in user_input:
            location_match = re.search(r'(Ø£Ø¹ÙŠØ´ ÙÙŠ|Ø§Ø³ÙƒÙ† ÙÙŠ) ([\w\u0600-\u06FF\s]+)', user_input)
            if location_match:
                location = location_match.group(2)
                memory.add_user_memory(user_id, "location", location)
                return jsonify({'Ø±Ø¯': f'Ø­Ø³Ù†Ø§Ù‹! Ø³Ø£ØªØ°ÙƒØ± Ø£Ù†Ùƒ ØªØ³ÙƒÙ† ÙÙŠ {location}.'})

        # ---- 0. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ø£ÙˆÙ„Ø§Ù‹ ----
        lang = detect_lang(user_input)
        factual_answer = get_factual_answer(user_input, lang)
        
        if factual_answer:
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©
            emoji = user_style.get_sentiment_emoji(sentiment, intent)
            if emoji:
                factual_answer = f"{emoji} {factual_answer}"
            
            # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø±Ø¯
            memory.add_question_response(user_id, user_input, factual_answer)
            
            end_time = time.time()
            response_time = end_time - start_time
            
            return jsonify({
                'Ø±Ø¯': factual_answer,
                'response_time': f"{response_time:.2f} Ø«Ø§Ù†ÙŠØ©",
                'sentiment_analysis': sentiment_analysis
            })

        # ---- 1. Ø§Ù„Ø­Ø§Ø±Ø³ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ ----
        if is_math_question(user_input):
            math_ans = solve_math_question(user_input)
            if math_ans is not None:
                response_text = f"Ø§Ù„Ù†Ø§ØªØ¬: {math_ans}" if lang == "ar" else f"Result: {math_ans}"
                emoji = user_style.get_sentiment_emoji(sentiment, intent)
                if emoji:
                    response_text = f"{emoji} {response_text}"
                
                memory.add_question_response(user_id, user_input, response_text)
                
                end_time = time.time()
                response_time = end_time - start_time
                
                return jsonify({
                    'Ø±Ø¯': response_text,
                    'response_time': f"{response_time:.2f} Ø«Ø§Ù†ÙŠØ©",
                    'sentiment_analysis': sentiment_analysis
                })

        # ---- 2. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ ----
        wiki_text, wiki_url = get_wikipedia_summary(user_input, lang=lang)
        
        # ---- 2.5. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© ÙˆÙ„ÙƒÙ† Ù…Ù† ÙˆÙŠÙƒÙŠØ¨Ø¯ÙŠØ§ ----
        if wiki_text and is_relevant(wiki_text, user_input):
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆÙŠÙƒÙŠØ¨Ø¯ÙŠØ§ Ù…Ø¹ ØµÙŠØ§ØºØ© Ù…ØªÙ†ÙˆØ¹Ø©
            summary = smart_shorten(wiki_text, 2, 200)
            
            # Ø§Ø®ØªÙŠØ§Ø± Ù‚Ø§Ù„Ø¨ Ù…ØªÙ†ÙˆØ¹
            response_template = generate_varied_response_template(
                user_input, previous_responses, sentiment, intent, lang
            )
            
            if response_template == "{}":
                response = f"Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©: {summary}"
            else:
                response = response_template.format(summary)
            
            emoji = user_style.get_sentiment_emoji(sentiment, intent)
            if emoji:
                response = f"{emoji} {response}"
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenRouter API Ù…Ø¹ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø¹Ø±Ø¨ÙŠ Ù…Ø­Ø³Ù† ÙˆØ³Ø±ÙŠØ¹
            response = generate_arabic_response(
                user_input, lang, temperature=0.3,
                previous_responses=previous_responses,
                sentiment=sentiment, intent=intent
            )
            
        # ---- 3. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ YouTube ----
        youtube_results = search_youtube(user_input, max_results=2)  # Ù…Ø®ÙÙ‘Ø¶ Ù„Ù„Ø³Ø±Ø¹Ø©
        youtube_links = [vid['url'] for vid in youtube_results] if youtube_results else []

        # Ø¥Ø¶Ø§ÙØ© Ù…ØµØ§Ø¯Ø± Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª
        sources = []
        if wiki_url:
            sources.append(f"Ø§Ù„Ù…ØµØ¯Ø±: {wiki_url}")
        if youtube_links:
            sources.append("Ù…Ù‚Ø§Ø·Ø¹ YouTube Ù…Ù‚ØªØ±Ø­Ø©: " + ", ".join(youtube_links[:2]))
            
        if sources:
            response += "\n\n" + "\n".join(sources)

        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø±Ø¯ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±
        memory.add_question_response(user_id, user_input, response)
        
        # Ø­Ø³Ø§Ø¨ ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        end_time = time.time()
        response_time = end_time - start_time
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø£Ù‚Ù„ Ù…Ù† Ø«Ø§Ù†ÙŠØ©
        if response_time > 1.0:
            print(f"âš ï¸ ØªØ­Ø°ÙŠØ±: ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© {response_time:.2f} Ø«Ø§Ù†ÙŠØ© - Ø£Ø·ÙˆÙ„ Ù…Ù† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨")
        
        return jsonify({
            'Ø±Ø¯': response,
            'youtube_links': youtube_links,
            'session_id': str(hashlib.sha256(user_input.encode()).hexdigest())[:16],
            'style_preferences': user_style.preferences,
            'response_time': f"{response_time:.2f} Ø«Ø§Ù†ÙŠØ©",
            'sentiment_analysis': sentiment_analysis,
            'previous_responses_count': len(previous_responses)
        })

    except Exception as e:
        return jsonify({'Ø±Ø¯': f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}"})

# =============== Ù†Ø¸Ø§Ù… Ø§Ù‚ØªØ±Ø§Ø­ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ===============
class ConversationNamer:
    """Ù†Ø¸Ø§Ù… Ø§Ù‚ØªØ±Ø§Ø­ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø°ÙƒÙŠ"""
    
    def __init__(self):
        self.name_patterns = {
            "question": ["Ø³Ø¤Ø§Ù„", "Ø§Ø³ØªÙØ³Ø§Ø±", "Ø§Ø³ØªØ´Ø§Ø±Ø©", "Ù†Ù‚Ø§Ø´"],
            "learning": ["ØªØ¹Ù„Ù…", "Ø¯Ø±Ø§Ø³Ø©", "Ø¨Ø­Ø«", "Ù…Ø¹Ø±ÙØ©"],
            "personal": ["Ø´Ø®ØµÙŠ", "Ø­ÙŠØ§ØªÙŠ", "ØªØ¬Ø±Ø¨ØªÙŠ", "Ø°ÙƒØ±ÙŠØ§Øª"],
            "technical": ["ØªÙ‚Ù†ÙŠ", "Ø¨Ø±Ù…Ø¬Ø©", "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§", "Ø­Ø§Ø³ÙˆØ¨"],
            "creative": ["Ø¥Ø¨Ø¯Ø§Ø¹", "ÙÙ†", "ÙƒØªØ§Ø¨Ø©", "ØªØµÙ…ÙŠÙ…"],
            "general": ["Ù…Ø­Ø§Ø¯Ø«Ø©", "Ø­ÙˆØ§Ø±", "Ø­Ø¯ÙŠØ«", "ØªÙˆØ§ØµÙ„"]
        }
        
        self.modifiers = [
            "Ù…Ø«ÙŠØ±Ø©", "Ù…Ù‡Ù…Ø©", "Ù…ÙÙŠØ¯Ø©", "Ø´ÙŠÙ‚Ø©", "Ø¹Ù…ÙŠÙ‚Ø©", 
            "Ù‚ØµÙŠØ±Ø©", "Ø·ÙˆÙŠÙ„Ø©", "Ø³Ø±ÙŠØ¹Ø©", "Ù‡Ø§Ø¯Ø¦Ø©", "Ù…ÙƒØ«ÙØ©"
        ]
        
    def suggest_names(self, conversation_text: str, num_suggestions: int = 3) -> List[str]:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø£Ø³Ù…Ø§Ø¡ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆØ§Ù‡Ø§"""
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        text_lower = conversation_text.lower()
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø·
        detected_patterns = []
        for pattern_type, keywords in self.name_patterns.items():
            for keyword in keywords:
                if keyword in text_lower:
                    detected_patterns.append(pattern_type)
                    break
        
        if not detected_patterns:
            detected_patterns = ["general"]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©
        suggestions = []
        for _ in range(num_suggestions):
            pattern = random.choice(detected_patterns)
            modifier = random.choice(self.modifiers)
            base_name = random.choice(self.name_patterns[pattern])
            
            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø§Ø³Ù…
            name_format = random.choice([
                f"{base_name} {modifier}",
                f"{modifier} {base_name}",
                f"Ù…Ø­Ø§Ø¯Ø«Ø© {base_name}",
                f"{base_name}"
            ])
            
            suggestions.append(name_format)
        
        return list(set(suggestions))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª

@app.route('/api/conversation/suggest-name', methods=['POST'])
def suggest_conversation_name():
    """ÙˆØ§Ø¬Ù‡Ø© Ø§Ù‚ØªØ±Ø§Ø­ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª"""
    data = request.get_json(force=True)
    conversation_text = data.get('conversation_text', '')
    num_suggestions = data.get('num_suggestions', 3)
    
    namer = ConversationNamer()
    suggestions = namer.suggest_names(conversation_text, num_suggestions)
    
    return jsonify({
        'suggestions': suggestions,
        'original_text_preview': conversation_text[:100] + ('...' if len(conversation_text) > 100 else '')
    })

# ÙˆØ§Ø¬Ù‡Ø§Øª API Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø°Ø§ÙƒØ±Ø©
@app.route('/api/memory/search', methods=['POST'])
def search_memories():
    """ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    data = request.get_json(force=True)
    user_id = data.get('user_id', 'default')
    query = data.get('query', '')
    
    memory_system = UniversalMemorySystem()
    results = memory_system.search_memories(user_id, query=query, limit=20)
    
    return jsonify({'memories': results})

@app.route('/api/memory/timeline/<user_id>', methods=['GET'])
def get_timeline(user_id):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ø­ÙŠØ§Ø©"""
    memory_system = UniversalMemorySystem()
    timeline = memory_system.get_life_timeline(user_id)
    
    return jsonify({'timeline': timeline})

@app.route('/api/memory/relationships/<user_id>', methods=['GET'])
def get_relationships(user_id):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª"""
    memory_system = UniversalMemorySystem()
    relationships = memory_system.get_relationship_network(user_id)
    
    return jsonify({'relationships': relationships})

@app.route('/api/memory/add', methods=['POST'])
def add_custom_memory():
    """Ø¥Ø¶Ø§ÙØ© Ø°ÙƒØ±Ù‰ Ù…Ø®ØµØµØ©"""
    data = request.get_json(force=True)
    user_id = data.get('user_id', 'default')
    category = data.get('category')
    title = data.get('title')
    content = data.get('content')
    
    memory_system = UniversalMemorySystem()
    
    try:
        memory_category = MemoryCategory(category)
        success = memory_system.add_memory(
            user_id, memory_category, title, content,
            entities=data.get('entities', []),
            emotions=data.get('emotions', []),
            intensity=data.get('intensity', 3),
            importance=data.get('importance', 3)
        )
        
        return jsonify({'success': success, 'message': 'ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø°ÙƒØ±Ù‰'})
    
    except ValueError:
        return jsonify({'success': False, 'message': 'ÙØ¦Ø© ØºÙŠØ± ØµØ­ÙŠØ­Ø©'})

# =============== Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„ØªÙƒÙˆÙŠÙ† ===============
class SystemConfig:
    """Ù†Ø¸Ø§Ù… ØªÙƒÙˆÙŠÙ† Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©"""
   
    DEFAULTS = {
        "quantum": {
            "entropy_level": 5,
            "probability_threshold": 0.85,
            "max_qubits": 12
        },
        "language": {
            "response_depth": 3,
            "creativity_factor": 0.75,
            "context_window": 7
        },
        "learning": {
            "retention_rate": 0.92,
            "decay_factor": 0.05,
            "reinforcement_cycle": 24
        },
        "security": {
            "authentication_level": 4,
            "key_rotation_interval": 3600,
            "biometric_threshold": 0.93
        }
    }
   
    def __init__(self, config_path: str = None):
        self.config = self.DEFAULTS.copy()
        self.config_path = config_path
        self.validation_rules = self._init_validation_rules()
       
        if config_path and os.path.exists(config_path):
            self.load_config(config_path)
   
    def _init_validation_rules(self) -> Dict[str, Callable]:
        """ØªÙ‡ÙŠØ¦Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        return {
            "quantum.entropy_level": lambda x: 1 <= x <= 10,
            "quantum.probability_threshold": lambda x: 0.5 <= x <= 0.99,
            "language.creativity_factor": lambda x: 0.1 <= x <= 1.0,
            "security.authentication_level": lambda x: x in {1, 2, 3, 4}
        }
   
    def load_config(self, path: str):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ù…Ù† Ù…Ù„Ù"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                loaded_config = json.load(f)
                self._merge_configs(loaded_config)
                print(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ù…Ù† {path}")
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ†: {str(e)}")
   
    def _merge_configs(self, new_config: Dict):
        """Ø¯Ù…Ø¬ Ø§Ù„ØªÙƒÙˆÙŠÙ†Ø§Øª Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©"""
        for section, values in new_config.items():
            if section in self.config:
                for key, value in values.items():
                    full_key = f"{section}.{key}"
                    if full_key in self.validation_rules:
                        if self.validation_rules[full_key](value):
                            self.config[section][key] = value
                        else:
                            print(f"Ù‚ÙŠÙ…Ø© ØºÙŠØ± ØµØ§Ù„Ø­Ø©: {full_key} = {value}")
                    else:
                        self.config[section][key] = value
   
    def get(self, key_path: str, default: Any = None) -> Any:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        keys = key_path.split('.')
        current = self.config
        try:
            for key in keys:
                current = current[key]
            return current
        except KeyError:
            return default
   
    def set(self, key_path: str, value: Any):
        """ØªØ¹ÙŠÙŠÙ† Ù‚ÙŠÙ…Ø© Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        keys = key_path.split('.')
        current = self.config
        for key in keys[:-1]:
            current = current.setdefault(key, {})
        last_key = keys[-1]
        current[last_key] = value

# =============== Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ===============
class EmotionRecognitionEngine:
    """Ù…Ø­Ø±Ùƒ Ù…Ø­Ù„ÙŠ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"""
   
    def __init__(self):
        self.sentiment_lexicon = {
            'Ø³Ø¹ÙŠØ¯': 0.8,
            'ÙØ±Ø­': 0.7,
            'Ø­Ø²ÙŠÙ†': -0.8,
            'ØºØ§Ø¶Ø¨': -0.6,
            'Ø±Ø§Ø¦Ø¹': 0.9,
            'Ø³ÙŠØ¡': -0.7
        }
   
    def analyze_sentiment(self, text: str) -> float:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù†Øµ"""
        words = text.split()
        sentiment = 0.0
        matched = 0
       
        for word in words:
            if word in self.sentiment_lexicon:
                sentiment += self.sentiment_lexicon[word]
                matched += 1
               
        if matched > 0:
            return sentiment / matched
        return 0.0

# =============== Ù†Ø¸Ø§Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ===============
class AdvancedLanguageSystem:
    """Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„ØºØ© Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…ØªÙ‚Ø¯Ù…"""
   
    class LanguageContext:
        """ØªÙ…Ø«ÙŠÙ„ Ø³ÙŠØ§Ù‚ÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
       
        def __init__(self, depth: int = 5):
            self.context_stack = deque(maxlen=depth)
            self.context_weights = []
            self.current_topic = ""
            self.sentiment_score = 0.0
       
        def push_context(self, context: str, weight: float = 1.0):
            """Ø¥Ø¶Ø§ÙØ© Ø³ÙŠØ§Ù‚ Ø¬Ø¯ÙŠØ¯ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
            self.context_stack.append(context)
            self.context_weights.append(weight)
            self._update_topic(context)
       
        def _update_topic(self, context: str):
            """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø­Ø§Ù„ÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
            if "ØŸ" in context:
                self.current_topic = context.split("ØŸ")[0]
            elif ":" in context:
                self.current_topic = context.split(":")[0]
            else:
                words = context.split()
                if len(words) > 2:
                    self.current_topic = " ".join(words[:3])
       
        def get_weighted_context(self) -> str:
            """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ø¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
            weighted_context = []
            for i, ctx in enumerate(self.context_stack):
                weight = self.context_weights[i]
                weighted_context.append(f"(w={weight:.2f}) {ctx}")
            return "\n".join(weighted_context)
   
    def __init__(self, config: SystemConfig):
        self.config = config
        self.creativity = config.get("language.creativity_factor", 0.7)
        self.context_depth = config.get("language.context_window", 7)
        self.context = self.LanguageContext(self.context_depth)
        self.language_models = self._load_language_models()
        self.response_strategies = self._init_response_strategies()
        self.emotion_engine = EmotionRecognitionEngine()
   
    def _load_language_models(self) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ù„ØºØ© Ù…ØªØ¹Ø¯Ø¯Ø© (Ù…Ø­Ø§ÙƒØ§Ø©)"""
        return {
            "grammar_model": {"version": "2.1", "coverage": 0.95},
            "semantic_model": {"version": "1.7", "entities": 15000},
            "pragmatic_model": {"version": "3.2", "contextual_depth": 5}
        }
   
    def _init_response_strategies(self) -> Dict[str, Callable]:
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ÙˆØ¯"""
        return {
            "direct": self._generate_direct_response,
            "contextual": self._generate_contextual_response,
            "creative": self._generate_creative_response,
            "probabilistic": self._generate_probabilistic_response
        }
   
    def process_input(self, text: str) -> str:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯"""
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        text = normalize_arabic_text(text)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        sentiment = self.emotion_engine.analyze_sentiment(text)
        self.context.sentiment_score = sentiment
       
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³ÙŠØ§Ù‚
        self.context.push_context(text, weight=self._calculate_context_weight(text))
       
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø±Ø¯
        strategy = self._select_response_strategy()
       
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
        response = self.response_strategies[strategy](text)
       
        # ØªØ­Ø¯ÙŠØ« Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ©
        self._update_language_models(text, response)
       
        return response
   
    def _calculate_context_weight(self, text: str) -> float:
        """Ø­Ø³Ø§Ø¨ ÙˆØ²Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ ÙˆØªØ¹Ù‚ÙŠØ¯Ù‡"""
        length_factor = min(1.0, len(text) / 100)
        complexity_factor = len(re.findall(r'\b\w{5,}\b', text)) / 10
        return 0.5 + 0.3 * length_factor + 0.2 * complexity_factor
   
    def _select_response_strategy(self) -> str:
        """Ø§Ø®ØªÙŠØ§Ø± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø±Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª"""
        strategies = ["direct", "contextual", "creative", "probabilistic"]
        creativity = self.creativity
       
        # ØªÙˆØ²ÙŠØ¹ Ø§Ø­ØªÙ…Ø§Ù„ÙŠ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
        probabilities = {
            "direct": max(0.1, 0.4 - creativity / 2),
            "contextual": 0.3,
            "creative": min(0.5, creativity * 0.8),
            "probabilistic": min(0.4, (1 - creativity) * 0.5)
        }
       
        # Ø§Ø®ØªÙŠØ§Ø± Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠ
        rand_val = random.random()
        cumulative = 0.0
        for strategy, prob in probabilities.items():
            cumulative += prob
            if rand_val <= cumulative:
                return strategy
       
        return "direct"
   
    def _generate_direct_response(self, text: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ù…Ø¨Ø§Ø´Ø±"""
        return f"Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø³Ø¤Ø§Ù„Ùƒ '{text}'ØŒ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù‡Ùˆ Ø£Ù†Ù†ÙŠ Ù†Ø¸Ø§Ù… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªÙ‚Ø¯Ù…."
   
    def _generate_contextual_response(self, text: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø³ÙŠØ§Ù‚ÙŠ Ù…Ø¹Ù‚Ø¯"""
        context = self.context.get_weighted_context()
        return f"Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚:\n{context}\nØ£Ø±Ù‰ Ø£Ù† Ø³Ø¤Ø§Ù„Ùƒ '{text}' ÙŠØªØ·Ù„Ø¨ Ø¥Ø¬Ø§Ø¨Ø© Ù…ØªØ¹Ù…Ù‚Ø©."
   
    def _generate_creative_response(self, text: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø§ÙƒØ§Ø© Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©"""
        creativity_level = int(self.creativity * 10)
        responses = [
            "Ø¨Ø¹Ø¯ ØªÙÙƒÙŠØ± Ø¹Ù…ÙŠÙ‚ØŒ Ø£Ø¹ØªÙ‚Ø¯ Ø£Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªÙƒÙ…Ù† ÙÙŠ...",
            "Ù…Ù† ÙˆØ¬Ù‡Ø© Ù†Ø¸Ø± Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ©ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù…Ø± ÙƒØ§Ù„ØªØ§Ù„ÙŠ...",
            "Ù„Ù‚Ø¯ Ø£Ù„Ù‡Ù…Ù†ÙŠ Ø³Ø¤Ø§Ù„Ùƒ Ù„Ù„ØªÙÙƒÙŠØ± ÙÙŠ...",
            f"Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙˆÙ‰ {creativity_level}ØŒ Ø£Ù‚ÙˆÙ„ Ù„Ùƒ..."
        ]
        return random.choice(responses)
   
    def _generate_probabilistic_response(self, text: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø§Ø­ØªÙ…Ø§Ù„ÙŠ Ù…Ø¹Ù‚Ø¯"""
        options = [
            f"Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØŒ Ø£Ø¹ØªÙ‚Ø¯ Ø£Ù† '{text}' ÙŠØ¹Ù†ÙŠ Ø´ÙŠØ¦Ø§Ù‹ Ù…Ø«ÙŠØ±Ø§Ù‹ Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù….",
            f"Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù‡Ùˆ Ø£Ù†Ùƒ ØªØ¨Ø­Ø« Ø¹Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ '{text}'.", 
            f"Ø¨Ø¹Ø¯ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§ØªØŒ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø±Ø¬Ø­ Ù‡ÙŠ Ø£Ù† Ù„Ø¯ÙŠÙƒ ÙØ¶ÙˆÙ„ Ø­ÙˆÙ„ '{text}'."
        ]
        return random.choice(options)
   
    def _update_language_models(self, input_text: str, response: str):
        """ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„"""
        for model in self.language_models.values():
            model["version"] = round(model["version"] + 0.01, 2)

# =============== Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ===============
class AdvancedLearningSystem:
    """Ù†Ø¸Ø§Ù… ØªØ¹Ù„Ù… Ø°Ø§ØªÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª"""
   
    class KnowledgeNode:
        """Ø¹Ù‚Ø¯Ø© Ù…Ø¹Ø±ÙÙŠØ© ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ©"""
       
        def __init__(self, id: str, content: Any):
            self.id = id
            self.content = content
            self.connections = {}
            self.strength = 1.0
            self.last_accessed = time.time()
       
        def add_connection(self, node_id: str, weight: float):
            """Ø¥Ø¶Ø§ÙØ© Ø§ØªØµØ§Ù„ Ø¥Ù„Ù‰ Ø¹Ù‚Ø¯Ø© Ø£Ø®Ø±Ù‰"""
            self.connections[node_id] = weight
       
        def decay(self, factor: float):
            """ØªØ®ÙÙŠØ¶ Ù‚ÙˆØ© Ø§Ù„Ø¹Ù‚Ø¯Ø© Ø¨Ù…Ø±ÙˆØ± Ø§Ù„ÙˆÙ‚Øª"""
            self.strength *= (1 - factor)
   
    class KnowledgeGraph:
        """Ø´Ø¨ÙƒØ© Ù…Ø¹Ø±ÙÙŠØ© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©"""
       
        def __init__(self, decay_factor: float = 0.05):
            self.nodes = {}
            self.decay_factor = decay_factor
            self.last_decay_time = time.time()
       
        def add_node(self, id: str, content: Any):
            """Ø¥Ø¶Ø§ÙØ© Ø¹Ù‚Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø©"""
            if id not in self.nodes:
                self.nodes[id] = AdvancedLearningSystem.KnowledgeNode(id, content)
       
        def add_connection(self, from_id: str, to_id: str, weight: float):
            """Ø¥Ø¶Ø§ÙØ© Ø§ØªØµØ§Ù„ Ø¨ÙŠÙ† Ø¹Ù‚Ø¯ØªÙŠÙ†"""
            if from_id in self.nodes and to_id in self.nodes:
                self.nodes[from_id].add_connection(to_id, weight)
       
        def get_node(self, id: str) -> Optional['AdvancedLearningSystem.KnowledgeNode']:
            """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ù‚Ø¯Ø© Ù…Ø¹Ø±ÙÙŠØ©"""
            if id in self.nodes:
                self.nodes[id].last_accessed = time.time()
                self.nodes[id].strength = min(1.0, self.nodes[id].strength + 0.1)
                return self.nodes[id]
            return None
       
        def decay_all(self):
            """ØªØ®ÙÙŠØ¶ Ù‚ÙˆØ© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù‚Ø¯"""
            current_time = time.time()
            if current_time - self.last_decay_time > 86400:  # Ù…Ø±Ø© ÙÙŠ Ø§Ù„ÙŠÙˆÙ…
                for node in self.nodes.values():
                    node.decay(self.decay_factor)
                self.last_decay_time = current_time
   
    def __init__(self, config: SystemConfig):
        self.config = config
        self.retention_rate = config.get("learning.retention_rate", 0.9)
        self.decay_factor = config.get("learning.decay_factor", 0.05)
        self.reinforcement_cycle = config.get("learning.reinforcement_cycle", 24)
        self.knowledge_graph = self.KnowledgeGraph(self.decay_factor)
        self.initialize_knowledge_base()
       
        # Ø¨Ø¯Ø¡ Ø®ÙŠØ· Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¯Ø§Ø¦Ù…
        self.learning_thread = threading.Thread(target=self._continuous_learning)
        self.learning_thread.daemon = True
        self.learning_thread.start()
   
    def initialize_knowledge_base(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©"""
        core_knowledge = [
            ("AI_principles", "Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"),
            ("quantum_basics", "Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„ÙƒÙ…ÙˆÙ…ÙŠØ©"),
            ("language_processing", "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©"),
            ("learning_algorithms", "Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ")
        ]
       
        for id, content in core_knowledge:
            self.knowledge_graph.add_node(id, content)
       
        # Ø¥Ø¶Ø§ÙØ© Ø§ØªØµØ§Ù„Ø§Øª Ù…Ø¹Ø±ÙÙŠØ©
        self.knowledge_graph.add_connection("AI_principles", "quantum_basics", 0.7)
        self.knowledge_graph.add_connection("AI_principles", "language_processing", 0.8)
        self.knowledge_graph.add_connection("language_processing", "learning_algorithms", 0.6)
   
    def learn_from_interaction(self, input_data: str, output_data: str):
        """Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† ØªÙØ§Ø¹Ù„ Ø¬Ø¯ÙŠØ¯"""
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù‚Ø¨Ù„ Ø§Ù„Ø­ÙØ¸
        input_data = normalize_arabic_text(input_data)
        output_data = normalize_arabic_text(output_data)
        
        interaction_id = hashlib.sha256(f"{input_data}{output_data}".encode()).hexdigest()[:16]
       
        self.knowledge_graph.add_node(interaction_id, {
            "input": input_data,
            "output": output_data,
            "timestamp": datetime.datetime.now().isoformat()
        })
       
        for node_id in self.knowledge_graph.nodes:
            if node_id.startswith("core_"):
                self.knowledge_graph.add_connection(interaction_id, node_id, 0.5)
   
    def _continuous_learning(self):
        """Ø¹Ù…Ù„ÙŠØ© ØªØ¹Ù„Ù… Ù…Ø³ØªÙ…Ø±Ø© ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
        while True:
            self.knowledge_graph.decay_all()
            self._reinforce_knowledge()
            time.sleep(self.reinforcement_cycle * 3600)
   
    def _reinforce_knowledge(self):
        """ØªØ¹Ø²ÙŠØ² Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
        for node in self.knowledge_graph.nodes.values():
            if node.strength > 0.8:
                node.strength = min(1.0, node.strength + 0.05)
   
    def get_knowledge_path(self, start_id: str, end_id: str) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± Ù…Ø¹Ø±ÙÙŠ Ø¨ÙŠÙ† Ø¹Ù‚Ø¯ØªÙŠÙ†"""
        visited = set()
        queue = deque([(start_id, [start_id])])
       
        while queue:
            current_id, path = queue.popleft()
            if current_id == end_id:
                return path
           
            visited.add(current_id)
            current_node = self.knowledge_graph.get_node(current_id)
           
            for neighbor_id, weight in current_node.connections.items():
                if neighbor_id not in visited and weight > 0.3:
                    queue.append((neighbor_id, path + [neighbor_id]))
       
        return []

# =============== Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„ÙƒÙ…ÙˆÙ…ÙŠ Ø§Ù„Ø­ÙŠÙˆÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ===============
class QuantumBiometricSecurity:
    """Ù†Ø¸Ø§Ù… Ø£Ù…Ø§Ù† ÙƒÙ…ÙˆÙ…ÙŠ Ø­ÙŠÙˆÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª"""
   
    class QuantumEncryptionEngine:
        """Ù…Ø­Ø±Ùƒ ØªØ´ÙÙŠØ± ÙƒÙ…ÙˆÙ…ÙŠ Ù…ØªÙ‚Ø¯Ù…"""
       
        def __init__(self, qubits: int = 8):
            self.qubits = qubits
            self.key_cache = {}
            self.last_key_rotation = time.time()
       
        def generate_quantum_key(self, length: int = 256) -> bytes:
            """ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ ÙƒÙ…ÙˆÙ…ÙŠ Ø¹Ø´ÙˆØ§Ø¦ÙŠ"""
            key = secrets.token_bytes(length)
            self.key_cache[hashlib.sha256(key).hexdigest()] = time.time()
            return key
       
        def rotate_keys(self):
            """ØªØ¯ÙˆÙŠØ± Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
            current_time = time.time()
            for key_hash, created_time in list(self.key_cache.items()):
                if current_time - created_time > 86400:  # 24 Ø³Ø§Ø¹Ø©
                    del self.key_cache[key_hash]
   
    def __init__(self, config: SystemConfig):
        self.config = config
        self.auth_level = config.get("security.authentication_level", 3)
        self.key_rotation_interval = config.get("security.key_rotation_interval", 3600)
        self.biometric_threshold = config.get("security.biometric_threshold", 0.9)
        self.encryption_engine = self.QuantumEncryptionEngine()
        self.biometric_profiles = {}
        self.session_keys = {}
        self.initialize_security_subsystems()
       
        # Ø¨Ø¯Ø¡ Ø®ÙŠØ· Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ø¯Ø§Ø¦Ù…
        self.security_thread = threading.Thread(target=self._continuous_security)
        self.security_thread.daemon = True
        self.security_thread.start()
   
    def initialize_security_subsystems(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„ÙØ±Ø¹ÙŠØ©"""
        self.system_root_key = self.encryption_engine.generate_quantum_key(512)
        self.biometric_profiles["admin"] = self._create_biometric_profile("admin")
   
    def _create_biometric_profile(self, user_id: str) -> Dict:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ Ø­ÙŠÙˆÙŠ ÙƒÙ…ÙˆÙ…ÙŠ"""
        profile = {
            "voice_pattern": hashlib.sha256(f"{user_id}_voice".encode()).hexdigest(),
            "behavioral_signature": self._generate_behavioral_signature(user_id),
            "quantum_entropy_factor": random.random()
        }
        return profile
   
    def _generate_behavioral_signature(self, user_id: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆÙ‚ÙŠØ¹ Ø³Ù„ÙˆÙƒÙŠ ÙƒÙ…ÙˆÙ…ÙŠ"""
        signature = ""
        for _ in range(8):
            quantum_state = [random.choice([0, 1]) for _ in range(8)]
            signature += ''.join(str(b) for b in quantum_state)
        return hashlib.sha256(signature.encode()).hexdigest()
   
    def _continuous_security(self):
        """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ù…Ù†ÙŠØ© Ù…Ø³ØªÙ…Ø±Ø© ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
        while True:
            self.encryption_engine.rotate_keys()
            self._rotate_session_keys()
            self._system_integrity_check()
            time.sleep(self.key_rotation_interval)
   
    def _rotate_session_keys(self):
        """ØªØ¯ÙˆÙŠØ± Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        current_time = time.time()
        for session_id, (created_time, _) in list(self.session_keys.items()):
            if current_time - created_time > 3600:  # 1 Ø³Ø§Ø¹Ø©
                del self.session_keys[session_id]
   
    def _system_integrity_check(self):
        """ÙØ­Øµ Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ù…Ù†ÙŠ"""
        key_hash = hashlib.sha256(self.system_root_key).hexdigest()
        if key_hash not in self.encryption_engine.key_cache:
            print("ØªØ­Ø°ÙŠØ±: ØªÙ… ØªØºÙŠÙŠØ± Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù…!")
            self.system_root_key = self.encryption_engine.generate_quantum_key(512)
   
    def authenticate_user(self, user_id: str, biometric_data: Dict) -> bool:
        """Ù…ØµØ§Ø¯Ù‚Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ©"""
        if user_id not in self.biometric_profiles:
            return False
       
        profile = self.biometric_profiles[user_id]
        match_score = self._calculate_biometric_match(profile, biometric_data)
       
        return match_score >= self.biometric_threshold
   
    def _calculate_biometric_match(self, profile: Dict, data: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø­ÙŠÙˆÙŠ"""
        voice_match = 1.0 if profile["voice_pattern"] == data.get("voice_hash") else 0.0
        behavior_match = 0.7 if profile["behavioral_signature"] == data.get("behavior_hash") else 0.0
        entropy_factor = profile["quantum_entropy_factor"]
       
        match_score = (voice_match * 0.6 + behavior_match * 0.4) * entropy_factor
        return match_score
   
    def create_secure_session(self, user_id: str) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù„Ø³Ø© Ø¢Ù…Ù†Ø© Ø¬Ø¯ÙŠØ¯Ø©"""
        session_id = secrets.token_urlsafe(16)
        session_key = self.encryption_engine.generate_quantum_key()
        self.session_keys[session_id] = (time.time(), session_key)
        return session_id
   
    def encrypt_data(self, session_id: str, data: str) -> bytes:
        """ØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙØªØ§Ø­ Ø§Ù„Ø¬Ù„Ø³Ø©"""
        if session_id not in self.session_keys:
            raise ValueError("Ù…Ø¹Ø±Ù Ø§Ù„Ø¬Ù„Ø³Ø© ØºÙŠØ± ØµØ§Ù„Ø­")
       
        _, session_key = self.session_keys[session_id]
        if isinstance(data, str):
            data = data.encode('utf-8')
        return self._quantum_encrypt(data, session_key)
   
    def decrypt_data(self, session_id: str, encrypted_data: bytes) -> str:
        """ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        if session_id not in self.session_keys:
            raise ValueError("Ù…Ø¹Ø±Ù Ø§Ù„Ø¬Ù„Ø³Ø© ØºÙŠØ± ØµØ§Ù„Ø­")
       
        _, session_key = self.session_keys[session_id]
        decrypted = self._quantum_decrypt(encrypted_data, session_key)
       
        try:
            return decrypted.decode('utf-8')
        except UnicodeDecodeError:
            return "ØªÙ… ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± Ø¨Ù†Ø¬Ø§Ø­ ÙˆÙ„ÙƒÙ† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ù†ØµÙŠ"
   
    def _quantum_encrypt(self, data: bytes, key: bytes) -> bytes:
        """ØªØ´ÙÙŠØ± ÙƒÙ…ÙˆÙ…ÙŠ Ù…ØªÙ‚Ø¯Ù…"""
        encrypted = bytearray()
        for i in range(len(data)):
            encrypted.append((data[i] + key[i % len(key)]) % 256)
        return bytes(encrypted)
   
    def _quantum_decrypt(self, encrypted: bytes, key: bytes) -> bytes:
        """ÙÙƒ ØªØ´ÙÙŠØ± ÙƒÙ…ÙˆÙ…ÙŠ"""
        decrypted = bytearray()
        for i in range(len(encrypted)):
            decrypted.append((encrypted[i] - key[i % len(key)]) % 256)
        return bytes(decrypted)

# =============== Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„ÙƒÙ…ÙˆÙ…ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ===============
class QuantumProbabilityEngine:
    """Ù†Ø¸Ø§Ù… Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª ÙƒÙ…ÙˆÙ…ÙŠØ© Ù…ØªÙ‚Ø¯Ù…"""
   
    class QuantumState:
        """ØªÙ…Ø«ÙŠÙ„ Ù„Ø­Ø§Ù„Ø© ÙƒÙ…ÙˆÙ…ÙŠØ© Ù…Ø¹Ù‚Ø¯Ø©"""
       
        def __init__(self, qubits: int):
            self.qubits = qubits
            self.state = np.zeros(2**qubits, dtype=complex)
            self.state[0] = 1.0  # Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
           
        def apply_gate(self, gate: np.ndarray, target: int, controls: List[int] = None):
            """ØªØ·Ø¨ÙŠÙ‚ Ø¨ÙˆØ§Ø¨Ø© ÙƒÙ…ÙˆÙ…ÙŠØ© Ù…Ø¹ Ø§Ù„ØªØ­ÙƒÙ…"""
            pass
       
        def measure(self) -> int:
            """Ù‚ÙŠØ§Ø³ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ÙƒÙ…ÙˆÙ…ÙŠØ©"""
            probabilities = np.abs(self.state)**2
            return random.choices(range(len(probabilities)), weights=probabilities)[0]
   
    def __init__(self, config: SystemConfig = None):
        self.config = config or SystemConfig()
        self.qubits = self.config.get("quantum.max_qubits", 8)
        self.entropy_level = self.config.get("quantum.entropy_level", 5)
        self.probability_cache = {}
        self.quantum_states = {}
        self.initialize_quantum_system()
   
    def initialize_quantum_system(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒÙ…ÙˆÙ…ÙŠ Ø¨Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""
        for i in range(1, self.entropy_level + 1):
            state_id = f"state_{i}"
            self.quantum_states[state_id] = self.QuantumState(self.qubits)
       
        self._apply_quantum_entanglement()
        self._initialize_probability_distributions()
   
    def _apply_quantum_entanglement(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªØ´Ø§Ø¨Ùƒ ÙƒÙ…ÙˆÙ…ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø­Ø§Ù„Ø§Øª"""
        for i in range(1, self.entropy_level):
            state_a = self.quantum_states[f"state_{i}"]
            state_b = self.quantum_states[f"state_{i+1}"]
   
    def _initialize_probability_distributions(self):
        """ØªÙ‡ÙŠØ¦Ø© ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©"""
        for i in range(1, 101):
            dist_id = f"dist_{i}"
            self.probability_cache[dist_id] = self._generate_probability_distribution()
   
    def _generate_probability_distribution(self) -> Dict[str, float]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØ²ÙŠØ¹ Ø§Ø­ØªÙ…Ø§Ù„ÙŠ ÙƒÙ…ÙˆÙ…ÙŠ Ù…Ø¹Ù‚Ø¯"""
        dist = {}
        total = 0.0
        for i in range(100):
            prob = random.random() ** self.entropy_level
            dist[f"event_{i}"] = prob
            total += prob
       
        for key in dist:
            dist[key] /= total
       
        return dist
   
    def calculate_complex_probability(self, event_space: List[str]) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª ÙÙŠ ÙØ¶Ø§Ø¡ Ø£Ø­Ø¯Ø§Ø« Ù…Ø¹Ù‚Ø¯"""
        event_hash = hashlib.sha256(','.join(event_space).encode()).hexdigest()
       
        if event_hash in self.probability_cache:
            return self.probability_cache[event_hash]
       
        dist = self._generate_probability_distribution_for_events(event_space)
        self.probability_cache[event_hash] = dist
        return dist
   
    def _generate_probability_distribution_for_events(self, events: List[str]) -> Dict[str, float]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØ²ÙŠØ¹ Ø§Ø­ØªÙ…Ø§Ù„ÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„ÙØ¦Ø© Ø£Ø­Ø¯Ø§Ø« Ù…Ø­Ø¯Ø¯Ø©"""
        quantum_result = self._simulate_quantum_events(len(events))
       
        probabilities = {}
        total = sum(quantum_result)
        for i, event in enumerate(events):
            probabilities[event] = quantum_result[i] / total
       
        self._apply_contextual_adjustments(probabilities)
       
        return probabilities
   
    def _simulate_quantum_events(self, num_events: int) -> List[float]:
        """Ù…Ø­Ø§ÙƒØ§Ø© Ø£Ø­Ø¯Ø§Ø« ÙƒÙ…ÙˆÙ…ÙŠØ© Ù…Ø¹Ù‚Ø¯Ø©"""
        state_id = random.choice(list(self.quantum_states.keys()))
        quantum_state = self.quantum_states[state_id]
       
        measurements = [quantum_state.measure() for _ in range(1000)]
       
        event_probs = [0.0] * num_events
        for measure in measurements:
            index = measure % num_events
            event_probs[index] += 1
       
        return event_probs
   
    def _apply_contextual_adjustments(self, probabilities: Dict[str, float]):
        """ØªØ·Ø¨ÙŠÙ‚ ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù…Ø¹ØªÙ…Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        entropy = self._calculate_distribution_entropy(probabilities)
        adjustment_factor = math.log(entropy + 1) * 0.1
       
        for key in probabilities:
            probabilities[key] = min(1.0, probabilities[key] * (1 + adjustment_factor))
       
        total = sum(probabilities.values())
        for key in probabilities:
            probabilities[key] /= total
   
    def _calculate_distribution_entropy(self, dist: Dict[str, float]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠ"""
        entropy = 0.0
        for p in dist.values():
            if p > 0:
                entropy -= p * math.log(p)
        return entropy

# =============== Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø®Ø§Ø±Ù‚Ø© ===============

class QuantumMemorySystem:
    """Ù†Ø¸Ø§Ù… Ø°Ø§ÙƒØ±Ø© ÙƒÙ…ÙˆÙ…ÙŠ Ù…ØªØ·ÙˆØ±"""
    
    def __init__(self):
        self.episodic_memory = {}  # Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ø¹Ø±Ø¶ÙŠØ©
        self.semantic_memory = {}  # Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…Ø©
        self.procedural_memory = {}  # Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª
        self.emotional_memory = {}  # Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        
    def store_experience(self, user_id, experience, emotional_weight=0.5):
        """ØªØ®Ø²ÙŠÙ† Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ù…Ø¹ ÙˆØ²Ù† Ø¹Ø§Ø·ÙÙŠ"""
        memory_id = f"exp_{hashlib.sha256(experience.encode()).hexdigest()[:16]}"
        
        self.episodic_memory[memory_id] = {
            'user_id': user_id,
            'experience': experience,
            'timestamp': time.time(),
            'emotional_weight': emotional_weight,
            'access_count': 0
        }
        
    def recall_context(self, user_id, current_context, top_k=5):
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©"""
        relevant_memories = []
        
        for memory_id, memory in self.episodic_memory.items():
            if memory['user_id'] == user_id:
                relevance = self._calculate_relevance(memory['experience'], current_context)
                if relevance > 0.3:  # Ø¹ØªØ¨Ø© Ø§Ù„ØµÙ„Ø©
                    relevant_memories.append((relevance, memory))
        
        relevant_memories.sort(reverse=True)
        return relevant_memories[:top_k]
    
    def _calculate_relevance(self, memory_text, current_context):
        """Ø­Ø³Ø§Ø¨ ØµÙ„Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        memory_words = set(memory_text.lower().split())
        context_words = set(current_context.lower().split())
        
        intersection = memory_words & context_words
        union = memory_words | context_words
        
        if len(union) == 0:
            return 0.0
            
        return len(intersection) / len(union)

class AdvancedReasoningEngine:
    """Ù…Ø­Ø±Ùƒ ØªÙÙƒÙŠØ± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª"""
    
    def __init__(self):
        self.reasoning_modes = {
            "deductive": self._deductive_reasoning,
            "inductive": self._inductive_reasoning,
            "abductive": self._abductive_reasoning,
            "analogical": self._analogical_reasoning,
            "counterfactual": self._counterfactual_reasoning
        }
    
    def solve_complex_problem(self, problem, context=""):
        """Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±Ù‚ ØªÙÙƒÙŠØ± Ù…ØªØ¹Ø¯Ø¯Ø©"""
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
        problem_type = self._classify_problem(problem)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
        solutions = []
        for mode_name, mode_func in self.reasoning_modes.items():
            try:
                solution = mode_func(problem, context)
                confidence = self._calculate_confidence(solution)
                solutions.append((confidence, solution, mode_name))
            except Exception as e:
                continue
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø­Ù„
        if solutions:
            solutions.sort(reverse=True)
            best_confidence, best_solution, best_mode = solutions[0]
            return {
                "solution": best_solution,
                "confidence": best_confidence,
                "method": best_mode,
                "alternative_approaches": solutions[1:3]
            }
        
        return {"solution": "Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø­Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠØ©", "confidence": 0.0}
    
    def _deductive_reasoning(self, problem, context):
        """ØªÙÙƒÙŠØ± Ø§Ø³ØªÙ†ØªØ§Ø¬ÙŠ (Ù…Ù† Ø§Ù„Ø¹Ø§Ù… Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø§Øµ)"""
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø®Ø§ØµØ©
        if "ÙƒÙ„" in problem and "Ø¨Ø¹Ø¶" in problem:
            return "Ù‡Ø°Ø§ Ø§Ø³ØªØ¯Ù„Ø§Ù„ ØºÙŠØ± ØµØ­ÙŠØ­. 'ÙƒÙ„ Ø£ Ù‡ÙŠ Ø¨' Ùˆ'Ø¨Ø¹Ø¶ Ø¨ Ù‡ÙŠ Ø¬' Ù„Ø§ ÙŠØ¹Ù†ÙŠ 'Ø¨Ø¹Ø¶ Ø£ Ù‡ÙŠ Ø¬'"
        
        return f"Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬: {problem} ÙŠØªØ·Ù„Ø¨ Ù…Ù‚Ø¯Ù…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"
    
    def _inductive_reasoning(self, problem, context):
        """ØªÙÙƒÙŠØ± Ø§Ø³ØªÙ‚Ø±Ø§Ø¦ÙŠ (Ù…Ù† Ø§Ù„Ø®Ø§Øµ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø§Ù…)"""
        return f"Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø­ÙˆÙ„ {problem}ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù†Ù…Ø· Ø¹Ø§Ù…"
    
    def _abductive_reasoning(self, problem, context):
        """ØªÙÙƒÙŠØ± ØªØ®Ù…ÙŠÙ†ÙŠ (Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙØ¶Ù„ ØªÙØ³ÙŠØ±)"""
        return f"Ø£ÙØ¶Ù„ ØªÙØ³ÙŠØ± Ù„Ù€ {problem} Ù‡Ùˆ..."
    
    def _analogical_reasoning(self, problem, context):
        """ØªÙÙƒÙŠØ± Ù‚ÙŠØ§Ø³ÙŠ (Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ´Ø§Ø¨Ù‡)"""
        return f"Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ØªØ´Ø¨Ù‡..."
    
    def _counterfactual_reasoning(self, problem, context):
        """ØªÙÙƒÙŠØ± Ø§ÙØªØ±Ø§Ø¶ÙŠ (Ù…Ø§Ø°Ø§ Ù„Ùˆ)"""
        return f"Ø¥Ø°Ø§ ØªØºÙŠØ±Øª Ø§Ù„Ø¸Ø±ÙˆÙØŒ ÙØ¥Ù†..."
    
    def _classify_problem(self, problem):
        """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©"""
        if any(word in problem for word in ["Ø­Ø³Ø§Ø¨", "Ø±ÙŠØ§Ø¶ÙŠØ§Øª", "Ù…Ø¹Ø§Ø¯Ù„Ø©"]):
            return "math"
        elif any(word in problem for word in ["Ø³Ø¨Ø¨", "Ù„Ù…Ø§Ø°Ø§", "ÙƒÙŠÙ"]):
            return "causal"
        elif any(word in problem for word in ["Ù…Ù‚Ø§Ø±Ù†Ø©", "Ø´Ø¨ÙŠÙ‡", "Ù…Ø«Ù„"]):
            return "comparative"
        else:
            return "general"
    
    def _calculate_confidence(self, solution):
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Ø§Ù„Ø­Ù„"""
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ø­Ù„ ÙˆØªØ¹Ù‚ÙŠØ¯Ù‡
        return min(0.95, len(solution) / 1000)

class DeepReinforcementLearning:
    """ØªØ¹Ù„Ù… ØªØ¹Ø²ÙŠØ²ÙŠ Ø¹Ù…ÙŠÙ‚ Ù„Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    
    def __init__(self):
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.learning_rate = 0.1
        self.discount_factor = 0.9
        self.exploration_rate = 0.3
        
    def choose_action(self, state, possible_actions):
        """Ø§Ø®ØªÙŠØ§Ø± action Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        if random.random() < self.exploration_rate:
            return random.choice(possible_actions)  # Ø§Ø³ØªÙƒØ´Ø§Ù
        
        # Ø§Ø³ØªØºÙ„Ø§Ù„ (Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ action Ù…Ø¹Ø±ÙˆÙ)
        q_values = [self.q_table[state][action] for action in possible_actions]
        max_q = max(q_values)
        
        # ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ø§Ø¯Ù„ØŒ Ø§Ø®ØªÙŠØ§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠ
        best_actions = [action for action in possible_actions 
                       if self.q_table[state][action] == max_q]
        return random.choice(best_actions)
    
    def update_q_value(self, state, action, reward, next_state):
        """ØªØ­Ø¯ÙŠØ« Q-value Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©"""
        best_next_q = max([self.q_table[next_state][a] for a in self.get_possible_actions(next_state)])
        current_q = self.q_table[state][action]
        
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * best_next_q - current_q
        )
        
        self.q_table[state][action] = new_q
    
    def get_possible_actions(self, state):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ù…ÙƒÙ†Ø© Ù„Ù„Ø­Ø§Ù„Ø©"""
        # Ù‡Ø°Ù‡ Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© ØªØ­ØªØ§Ø¬ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚ Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚
        return ["answer", "ask_clarification", "provide_example", "suggest_resource"]

class AdvancedEmotionalIntelligence:
    """Ø°ÙƒØ§Ø¡ Ø¹Ø§Ø·ÙÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„ÙÙ‡Ù… Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    
    def __init__(self):
        self.emotion_lexicon = self._load_emotion_lexicon()
        
    def analyze_emotional_state(self, text, voice_tone=None, typing_speed=None):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„Ø©"""
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ
        text_emotion = self._analyze_text_emotion(text)
        
        # ØªØ­Ù„ÙŠÙ„ Ù†Ù…Ø· Ø§Ù„ØªÙØ§Ø¹Ù„
        interaction_pattern = self._analyze_interaction_pattern(typing_speed)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        emotional_state = {
            'primary_emotion': text_emotion['dominant_emotion'],
            'emotional_intensity': text_emotion['intensity'],
            'valence': text_emotion['valence'],  # Ø¥ÙŠØ¬Ø§Ø¨ÙŠ/Ø³Ù„Ø¨ÙŠ
            'arousal': text_emotion['arousal'],  # Ù‡Ø§Ø¯Ø¦/Ù…ØªØ­Ù…Ø³
            'confidence': text_emotion['confidence']
        }
        
        return emotional_state
    
    def generate_empathetic_response(self, user_input, emotional_state):
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ÙˆØ¯ Ø¹Ø§Ø·ÙÙŠØ© Ù…ØªØ¹Ø§Ø·ÙØ©"""
        
        empathy_templates = {
            'anger': "Ø£ØªÙÙ‡Ù… Ø£Ù†Ùƒ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø¥Ø­Ø¨Ø§Ø·. Ø¯Ø¹Ù†Ø§ Ù†Ø­Ø§ÙˆÙ„ Ø­Ù„ Ù‡Ø°Ø§ Ù…Ø¹Ø§Ù‹.",
            'sadness': "Ø£Ø±Ù‰ Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± ÙŠØ²Ø¹Ø¬Ùƒ. Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„ØªØ­Ø¯Ø« Ø¹Ù†Ù‡ Ø£ÙƒØ«Ø±ØŸ",
            'joy': "Ø±Ø§Ø¦Ø¹! ÙŠØ¨Ø¯Ùˆ Ø£Ù†Ùƒ Ø³Ø¹ÙŠØ¯ Ø¨Ù‡Ø°Ø§. Ù‡Ø°Ø§ ÙŠØ¬Ø¹Ù„Ù†ÙŠ Ø³Ø¹ÙŠØ¯Ø§Ù‹ Ø£ÙŠØ¶Ø§Ù‹!",
            'fear': "Ø£ØªÙÙ‡Ù… Ù‚Ù„Ù‚Ùƒ. Ø¯Ø¹Ù†Ø§ Ù†Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø®ØªÙ„ÙØ©.",
            'surprise': "Ù…ÙØ§Ø¬Ø£Ø©! Ù‡Ø°Ø§ Ù…Ø«ÙŠØ± Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù…. Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø§Ù„Ù…Ø²ÙŠØ¯."
        }
        
        primary_emotion = emotional_state['primary_emotion']
        empathy_line = empathy_templates.get(primary_emotion, "Ø£ØªÙÙ‡Ù… Ù…Ø´Ø§Ø¹Ø±Ùƒ.")
        
        return f"{empathy_line} "
    
    def _load_emotion_lexicon(self):
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"""
        return {
            'Ø³Ø¹ÙŠØ¯': {'emotion': 'joy', 'intensity': 0.8, 'valence': 1.0},
            'ÙØ±Ø­': {'emotion': 'joy', 'intensity': 0.9, 'valence': 1.0},
            'Ø­Ø²ÙŠÙ†': {'emotion': 'sadness', 'intensity': 0.8, 'valence': -1.0},
            'ØºØ§Ø¶Ø¨': {'emotion': 'anger', 'intensity': 0.7, 'valence': -1.0},
            'Ø®Ø§Ø¦Ù': {'emotion': 'fear', 'intensity': 0.6, 'valence': -1.0},
            'Ù…ØªÙØ§Ø¬Ø¦': {'emotion': 'surprise', 'intensity': 0.5, 'valence': 0.0}
        }
    
    def _analyze_text_emotion(self, text):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ù† Ø§Ù„Ù†Øµ"""
        words = text.split()
        emotion_scores = defaultdict(float)
        
        for word in words:
            if word in self.emotion_lexicon:
                emotion_data = self.emotion_lexicon[word]
                emotion_scores[emotion_data['emotion']] += emotion_data['intensity']
        
        if emotion_scores:
            dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])
            return {
                'dominant_emotion': dominant_emotion[0],
                'intensity': dominant_emotion[1],
                'valence': self.emotion_lexicon.get(words[0], {}).get('valence', 0.0) if words else 0.0,
                'arousal': 0.5,  # Ù…Ø­Ø§ÙƒØ§Ø©
                'confidence': min(1.0, len(emotion_scores) / 10)
            }
        else:
            return {
                'dominant_emotion': 'neutral',
                'intensity': 0.0,
                'valence': 0.0,
                'arousal': 0.5,
                'confidence': 0.1
            }
    
    def _analyze_interaction_pattern(self, typing_speed):
        """ØªØ­Ù„ÙŠÙ„ Ù†Ù…Ø· Ø§Ù„ØªÙØ§Ø¹Ù„ (Ù…Ø­Ø§ÙƒØ§Ø©)"""
        return "normal"

class ExternalKnowledgeIntegration:
    """Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ù…ØµØ§Ø¯Ø± Ø®Ø§Ø±Ø¬ÙŠØ©"""
    
    def __init__(self):
        self.apis = {
            'wolfram_alpha': 'YOUR_WOLFRAM_APP_ID',
            'openweather': 'YOUR_WEATHER_API_KEY',
            'news_api': 'YOUR_NEWS_API_KEY'
        }
    
    def get_real_time_data(self, query):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ"""
        
        if self._is_mathematical(query):
            return self._query_wolfram_alpha(query)
        elif self._is_weather_related(query):
            return self._query_weather(query)
        elif self._is_news_related(query):
            return self._query_news(query)
        
        return None
    
    def _query_wolfram_alpha(self, query):
        """Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ù† Wolfram Alpha Ù„Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©"""
        try:
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… - ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ ØªØ­ØªØ§Ø¬ API key
            return f"Ù†ØªÙŠØ¬Ø© Ù…Ø­Ø³ÙˆØ¨Ø© Ù„Ù€ '{query}' (Ù…Ø­Ø§ÙƒØ§Ø© - ØªØ­ØªØ§Ø¬ Wolfram Alpha API)"
        except Exception as e:
            return None
    
    def _query_weather(self, query):
        """Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ø§Ù„Ø·Ù‚Ø³"""
        try:
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            return "Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ù‚Ø³: Ù…Ø¹ØªØ¯Ù„ 25Â°C (Ù…Ø­Ø§ÙƒØ§Ø©)"
        except Exception as e:
            return None
    
    def _query_news(self, query):
        """Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±"""
        try:
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            return "Ø£Ø­Ø¯Ø« Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù…ÙˆØ¶ÙˆØ¹Ùƒ (Ù…Ø­Ø§ÙƒØ§Ø©)"
        except Exception as e:
            return None
    
    def _is_mathematical(self, query):
        """Ø§Ù„ÙƒØ´Ù Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø±ÙŠØ§Ø¶ÙŠØ§Ù‹"""
        math_terms = ["Ø§Ø­Ø³Ø¨", "Ø­Ù„", "Ù…Ø¹Ø§Ø¯Ù„Ø©", "ØªÙƒØ§Ù…Ù„", "ØªÙØ§Ø¶Ù„", "calculate", "solve"]
        return any(term in query.lower() for term in math_terms)
    
    def _is_weather_related(self, query):
        """Ø§Ù„ÙƒØ´Ù Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ø§Ù„Ø·Ù‚Ø³"""
        weather_terms = ["Ø·Ù‚Ø³", "Ø¬Ùˆ", "Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©", "weather", "temperature"]
        return any(term in query.lower() for term in weather_terms)
    
    def _is_news_related(self, query):
        """Ø§Ù„ÙƒØ´Ù Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ø£Ø®Ø¨Ø§Ø±"""
        news_terms = ["Ø£Ø®Ø¨Ø§Ø±", "Ø­Ø¯Ø«", "Ø¬Ø¯ÙŠØ¯", "news", "update"]
        return any(term in query.lower() for term in news_terms)

class SelfEvaluationSystem:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø°Ø§ØªÙŠ ÙˆØ§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
    
    def evaluate_response_quality(self, user_input, ai_response, user_feedback=None):
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø±Ø¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
        
        metrics = {
            'relevance': self._calculate_relevance(user_input, ai_response),
            'accuracy': self._estimate_accuracy(ai_response),
            'completeness': self._check_completeness(user_input, ai_response),
            'empathy': self._measure_empathy(ai_response),
            'conciseness': self._measure_conciseness(ai_response)
        }
        
        overall_score = sum(metrics.values()) / len(metrics)
        
        return {
            'overall_score': overall_score,
            'detailed_metrics': metrics,
            'improvement_suggestions': self._generate_improvement_suggestions(metrics)
        }
    
    def _generate_improvement_suggestions(self, metrics):
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†"""
        suggestions = []
        
        if metrics['relevance'] < 0.7:
            suggestions.append("Ø§Ù„ØªØ±ÙƒÙŠØ² Ø£ÙƒØ«Ø± Ø¹Ù„Ù‰ ØµÙ„Ø© Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„")
        if metrics['empathy'] < 0.6:
            suggestions.append("Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¹Ø§Ø·Ù ÙÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯")
        if metrics['conciseness'] < 0.5:
            suggestions.append("ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥ÙŠØ¬Ø§Ø² ÙˆØªØ¬Ù†Ø¨ Ø§Ù„Ø¥Ø·Ø§Ù„Ø©")
            
        return suggestions
    
    def _calculate_relevance(self, user_input, ai_response):
        """Ø­Ø³Ø§Ø¨ ØµÙ„Ø© Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„"""
        input_words = set(user_input.lower().split())
        response_words = set(ai_response.lower().split())
        
        intersection = input_words & response_words
        union = input_words | response_words
        
        if len(union) == 0:
            return 0.0
            
        return len(intersection) / len(union)
    
    def _estimate_accuracy(self, response):
        """ØªÙ‚Ø¯ÙŠØ± Ø¯Ù‚Ø© Ø§Ù„Ø±Ø¯ (Ù…Ø­Ø§ÙƒØ§Ø©)"""
        # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… fact-checking APIs
        return 0.8  # Ù…Ø­Ø§ÙƒØ§Ø©
    
    def _check_completeness(self, user_input, ai_response):
        """ÙØ­Øµ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø±Ø¯"""
        question_types = {
            "Ù…Ø§": 0.8,
            "ÙƒÙŠÙ": 0.7,
            "Ù„Ù…Ø§Ø°Ø§": 0.9,
            "Ø£ÙŠÙ†": 0.6,
            "Ù…ØªÙ‰": 0.5
        }
        
        for q_type, threshold in question_types.items():
            if q_type in user_input:
                return threshold
        
        return 0.7
    
    def _measure_empathy(self, response):
        """Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØ¹Ø§Ø·Ù ÙÙŠ Ø§Ù„Ø±Ø¯"""
        empathy_terms = ["Ø£ØªÙÙ‡Ù…", "Ø£Ø±Ù‰", "Ø£Ø´Ø¹Ø±", "Ù…Ø¹Ùƒ", "Ø¯Ø¹Ù†Ø§", "Ù†Ø­Ø§ÙˆÙ„"]
        empathy_count = sum(1 for term in empathy_terms if term in response)
        
        return min(1.0, empathy_count / 3)
    
    def _measure_conciseness(self, response):
        """Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø²"""
        word_count = len(response.split())
        
        if word_count < 50:
            return 1.0
        elif word_count < 100:
            return 0.8
        elif word_count < 200:
            return 0.6
        else:
            return 0.4

# =============== Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ Ø§Ù„Ù…Ø­Ø³Ù† ===============
class CosmicSaadUltimate:
    """Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø®Ø§Ø±Ù‚ Ù…Ù† Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ"""
   
    def __init__(self, config_path: str = None):
        # ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù…
        self.config = SystemConfig(config_path)
       
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.probability_engine = QuantumProbabilityEngine(self.config)
        self.language_system = AdvancedLanguageSystem(self.config)
        self.learning_system = AdvancedLearningSystem(self.config)
        self.security_system = QuantumBiometricSecurity(self.config)
        self.conversation_memory = PersistentConversationMemory()
        self.response_guard = EnhancedResponseGuard()
        
        # Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø®Ø§Ø±Ù‚Ø©
        self.quantum_memory = QuantumMemorySystem()
        self.reasoning_engine = AdvancedReasoningEngine()
        self.reinforcement_learning = DeepReinforcementLearning()
        self.emotional_intelligence = AdvancedEmotionalIntelligence()
        self.external_knowledge = ExternalKnowledgeIntegration()
        self.self_evaluation = SelfEvaluationSystem()
        
        # Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…Ø¶Ø§ÙØ© Ø­Ø¯ÙŠØ«Ø§Ù‹
        self.universal_memory = UniversalMemorySystem()
        self.knowledge_base = ComprehensiveKnowledgeBase()
       
        # Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        self.active_sessions = {}
        self.system_status = "operational"
        self.startup_time = time.time()
       
        # ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
        self._log_system_event("system_start", "ØªÙ… Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø®Ø§Ø±Ù‚Ø©")
   
    def _log_system_event(self, event_type: str, message: str):
        """ØªØ³Ø¬ÙŠÙ„ Ø­Ø¯Ø« Ù†Ø¸Ø§Ù…ÙŠ"""
        timestamp = datetime.datetime.now().isoformat()
        event_data = {
            "timestamp": timestamp,
            "type": event_type,
            "message": message,
            "status": self.system_status
        }
        self.learning_system.learn_from_interaction(
            f"system_event:{event_type}",
            json.dumps(event_data)
        )
   
    def extract_and_store_personal_info(self, user_id: str, text: str):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ ÙˆØ­ÙØ¸Ù‡Ø§"""
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        text = normalize_arabic_text(text)
        
        name_patterns = [
            r"Ø§Ø³Ù…ÙŠ (Ù‡Ùˆ )?([\w\u0600-\u06FF]+)",
            r"Ø£Ù†Ø§ (Ø§Ø³Ù…ÙŠ|Ø£Ø¯Ø¹Ù‰) ([\w\u0600-\u06FF]+)",
            r"my name is ([\w]+)"
        ]
        
        for pattern in name_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                name = match.group(2)
                self.conversation_memory.add_user_memory(user_id, "name", name)
                break

    def advanced_generation(self, prompt, context, user_profile):
        """ØªÙˆÙ„ÙŠØ¯ Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenRouter"""
        
        # Ø¨Ù†Ø§Ø¡ Ø±Ø³Ø§Ø¦Ù„ OpenAI-compatible
        system_content = f"""Ø£Ù†Øª Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ - Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…Ø¨Ø¯Ø¹ ÙŠØªØ³Ù… Ø¨Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©.

[Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…] {user_profile.get('name', 'Ù…Ø³ØªØ®Ø¯Ù…')}
[Ø§Ù„Ø´Ø®ØµÙŠØ©] {user_profile.get('personality', 'Ø¹Ø§Ù…')}
[Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©] {context}
[Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ] {self.emotional_intelligence.analyze_emotional_state(prompt)}

Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:
- ÙƒÙ† Ù…ÙÙŠØ¯Ø§Ù‹ ÙˆØ¯Ù‚ÙŠÙ‚Ø§Ù‹
- ØªØ¹Ø§Ø·Ù Ù…Ø¹ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
- ÙƒÙ† Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ§Ù‹ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙ†Ø§Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚"""

        user_content = prompt
        
        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_content}
        ]
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenRouter API
        response = generate_via_openrouter(
            messages=messages,
            temperature=0.5,
            max_tokens=2000,
            model="meta-llama/llama-3.1-405b-instruct:free"
        )
        
        return response if response else "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯."

    def process_input(self, user_input: str, user_id: str = "default",
                      biometric_data: Dict = None) -> Dict:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆOpenRouter"""
        if biometric_data:
            if not self.security_system.authenticate_user(user_id, biometric_data):
                return {"error": "ÙØ´Ù„ Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø© Ø§Ù„Ø­ÙŠÙˆÙŠØ©"}
       
        # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©
        self.extract_and_store_personal_info(user_id, user_input)
        
        # 2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¹Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
        if "Ù…Ø§ Ù‡Ùˆ Ø§Ø³Ù…ÙŠ" in user_input or "Ù…Ø§ Ø§Ø³Ù…ÙŠ" in user_input:
            name = self.conversation_memory.get_user_memory(user_id, "name")
            if name:
                return {
                    "response": f"Ø§Ø³Ù…Ùƒ Ù‡Ùˆ {name}",
                    "session_id": "memory_access"
                }
       
        session_id = self.security_system.create_secure_session(user_id)
        self.active_sessions[session_id] = time.time()
        
        # 3. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø´Ø§Ù…Ù„
        memory_result = self.universal_memory.store_information(user_id, user_input)
        
        # 4. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
        lang = detect_lang(user_input)
        kb_results = self.knowledge_base.search_knowledge(user_input, lang)
        
        # 5. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©
        emotional_state = self.emotional_intelligence.analyze_emotional_state(user_input)
        
        # 6. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
        relevant_memories = self.quantum_memory.recall_context(user_id, user_input)
        
        # 7. Ù…Ø­Ø§ÙˆÙ„Ø© Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
        complex_solution = None
        if self._is_complex_problem(user_input):
            complex_solution = self.reasoning_engine.solve_complex_problem(user_input)
        
        # 8. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©
        external_data = self.external_knowledge.get_real_time_data(user_input)
        
        # 9. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenRouter API
        user_profile = {
            "name": self.conversation_memory.get_user_memory(user_id, "name") or "Ù…Ø³ØªØ®Ø¯Ù…",
            "personality": "Ø¹Ø§Ù…"
        }
        
        context = f"Ø°ÙƒØ±ÙŠØ§Øª Ø³Ø§Ø¨Ù‚Ø©: {relevant_memories[:2] if relevant_memories else 'Ù„Ø§ ØªÙˆØ¬Ø¯'}"
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¥Ø¬Ø§Ø¨Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª
        if kb_results and kb_results[0]['confidence'] > 0.7:
            response = random.choice(kb_results[0]['answers'])
        elif complex_solution and complex_solution.get("confidence", 0) > 0.7:
            response = complex_solution["solution"]
        elif external_data:
            response = f"Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©: {external_data}"
        else:
            response = self.advanced_generation(user_input, context, user_profile)
        
        # 10. Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¹Ø§Ø·Ù Ø§Ù„Ø¹Ø§Ø·ÙÙŠ
        empathetic_response = self.emotional_intelligence.generate_empathetic_response(user_input, emotional_state)
        final_response = empathetic_response + response
        
        # 11. Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø±Ø¯
        guarded_response = self.response_guard.guard(user_input, final_response)
        
        # 12. Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ØªÙØ§Ø¹Ù„
        self.learning_system.learn_from_interaction(user_input, guarded_response)
        self.quantum_memory.store_experience(user_id, user_input, emotional_state['emotional_intensity'])
        
        # 13. Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø°Ø§ØªÙŠ
        evaluation = self.self_evaluation.evaluate_response_quality(user_input, guarded_response)
        
        # 14. Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø´Ø§Ù…Ù„
        self.universal_memory.add_conversation(user_id, user_input, guarded_response, memory_result['category'])
        
        self._log_interaction(user_id, user_input, guarded_response)
       
        encrypted_response = self.security_system.encrypt_data(session_id, guarded_response)
       
        return {
            "session_id": session_id,
            "response": guarded_response,
            "encrypted_response": encrypted_response,
            "emotional_state": emotional_state,
            "evaluation": evaluation,
            "memory_stored": memory_result,
            "knowledge_used": len(kb_results) > 0,
            "timestamp": datetime.datetime.now().isoformat()
        }
    
    def _is_complex_problem(self, user_input):
        """Ø§Ù„ÙƒØ´Ù Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù…Ø¹Ù‚Ø¯Ø©"""
        complex_indicators = ["Ø­Ù„", "ØªØ­Ù„ÙŠÙ„", "Ù…Ù‚Ø§Ø±Ù†Ø©", "Ø³Ø¨Ø¨", "ÙƒÙŠÙ", "Ù„Ù…Ø§Ø°Ø§", "Ù…Ø´ÙƒÙ„Ø©", "issue", "problem", "solve"]
        return any(indicator in user_input for indicator in complex_indicators)
   
    def _log_interaction(self, user_id: str, input_text: str, output_text: str):
        """ØªØ³Ø¬ÙŠÙ„ ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        interaction_data = {
            "user_id": user_id,
            "input": input_text,
            "output": output_text,
            "timestamp": datetime.datetime.now().isoformat()
        }
        self.learning_system.learn_from_interaction(
            f"user_interaction:{user_id}",
            json.dumps(interaction_data)
        )
   
    def get_system_status(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
        uptime = time.time() - self.startup_time
        return {
            "status": self.system_status,
            "uptime": uptime,
            "components": {
                "probability_engine": "active",
                "language_system": "active",
                "learning_system": "active",
                "security_system": "active",
                "conversation_memory": "active",
                "quantum_memory": "active",
                "reasoning_engine": "active",
                "emotional_intelligence": "active",
                "external_knowledge": "active",
                "self_evaluation": "active",
                "universal_memory": "active",
                "knowledge_base": "active"
            },
            "statistics": {
                "interactions": len(self.learning_system.knowledge_graph.nodes) - 4,
                "memory_entries": self.conversation_memory.get_memory_count(),
                "unique_users": self.conversation_memory.get_user_count(),
                "quantum_memories": len(self.quantum_memory.episodic_memory),
                "universal_memories": self.universal_memory.get_user_profile("default")['stats']['total_memories'],
                "knowledge_entries": "1000+"
            }
        }
   
    def perform_self_diagnostic(self) -> Dict:
        """Ø¥Ø¬Ø±Ø§Ø¡ ØªØ´Ø®ÙŠØµ Ø°Ø§ØªÙŠ Ù„Ù„Ù†Ø¸Ø§Ù…"""
        diagnostic = {
            "quantum_probability": self._test_probability_engine(),
            "language_processing": self._test_language_system(),
            "learning_capabilities": self._test_learning_system(),
            "security_integrity": self._test_security_system(),
            "memory_system": self._test_memory_system(),
            "reasoning_engine": self._test_reasoning_engine(),
            "emotional_intelligence": self._test_emotional_intelligence(),
            "universal_memory": self._test_universal_memory(),
            "knowledge_base": self._test_knowledge_base()
        }
       
        all_ok = all(status == "ok" for status in diagnostic.values())
        self.system_status = "operational" if all_ok else "degraded"
       
        return diagnostic
   
    def _test_probability_engine(self) -> str:
        """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„ÙƒÙ…ÙˆÙ…ÙŠØ©"""
        try:
            events = ["event_a", "event_b", "event_c"]
            dist = self.probability_engine.calculate_complex_probability(events)
            if math.isclose(sum(dist.values()), 1.0, abs_tol=0.01):
                return "ok"
            return "warning: probability_sum_not_1"
        except Exception as e:
            return f"error: {str(e)}"
   
    def _test_language_system(self) -> str:
        """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ©"""
        try:
            response = self.language_system.process_input("Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…")
            if response and len(response) > 10:
                return "ok"
            return "warning: invalid_response"
        except Exception as e:
            return f"error: {str(e)}"
   
    def _test_learning_system(self) -> str:
        """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù…"""
        try:
            path = self.learning_system.get_knowledge_path("AI_principles", "learning_algorithms")
            if len(path) >= 2:
                return "ok"
            return "warning: knowledge_path_incomplete"
        except Exception as e:
            return f"error: {str(e)}"
   
    def _test_security_system(self) -> str:
        """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ù…Ø§Ù†"""
        try:
            session_id = self.security_system.create_secure_session("test_user")
            test_data = "Ø§Ø®ØªØ¨Ø§Ø± ØªØ´ÙÙŠØ±"
            encrypted = self.security_system.encrypt_data(session_id, test_data)
            decrypted = self.security_system.decrypt_data(session_id, encrypted)
            if decrypted == test_data:
                return "ok"
            return "warning: encryption_decryption_mismatch"
        except Exception as e:
            return f"error: {str(e)}"
            
    def _test_memory_system(self) -> str:
        """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        try:
            test_id = "test_user_123"
            test_key = "test_key"
            test_value = "test_value"
            
            self.conversation_memory.add_user_memory(test_id, test_key, test_value)
            retrieved = self.conversation_memory.get_user_memory(test_id, test_key)
            
            if retrieved == test_value:
                return "ok"
            return "warning: memory_retrieval_failed"
        except Exception as e:
            return f"error: {str(e)}"
    
    def _test_reasoning_engine(self) -> str:
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙÙƒÙŠØ±"""
        try:
            result = self.reasoning_engine.solve_complex_problem("Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ·")
            if result and "solution" in result:
                return "ok"
            return "warning: reasoning_failed"
        except Exception as e:
            return f"error: {str(e)}"
    
    def _test_emotional_intelligence(self) -> str:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ"""
        try:
            emotional_state = self.emotional_intelligence.analyze_emotional_state("Ø£Ù†Ø§ Ø³Ø¹ÙŠØ¯ Ø§Ù„ÙŠÙˆÙ…")
            if emotional_state and "primary_emotion" in emotional_state:
                return "ok"
            return "warning: emotion_analysis_failed"
        except Exception as e:
            return f"error: {str(e)}"
    
    def _test_universal_memory(self) -> str:
        """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø´Ø§Ù…Ù„"""
        try:
            test_id = "test_user_456"
            result = self.universal_memory.store_information(test_id, "Ø§Ø³Ù…ÙŠ Ø£Ø­Ù…Ø¯ ÙˆØ¹Ù…Ø±ÙŠ 25 Ø³Ù†Ø©")
            if result['stored_count'] > 0:
                return "ok"
            return "warning: memory_storage_failed"
        except Exception as e:
            return f"error: {str(e)}"
    
    def _test_knowledge_base(self) -> str:
        """Ø§Ø®ØªØ¨Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©"""
        try:
            results = self.knowledge_base.search_knowledge("Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ©", "ar")
            if results is not None:
                return "ok"
            return "warning: knowledge_search_failed"
        except Exception as e:
            return f"error: {str(e)}"

# =============== Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ===============
class AdvancedMemorySystem:
    """Ù†Ø¸Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ù…ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø°ÙƒÙŠ"""
    
    def __init__(self, db_path="advanced_memory.db"):
        self.conn = sqlite3.connect(db_path)
        self._init_memory_db()
        
    def _init_memory_db(self):
        cursor = self.conn.cursor()
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS user_memories (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id TEXT,
            memory_type TEXT,
            content TEXT,
            tags TEXT,
            importance INTEGER DEFAULT 1,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            last_accessed DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS memory_vectors (
            memory_id INTEGER,
            vector_data BLOB,
            FOREIGN KEY (memory_id) REFERENCES user_memories(id)
        )
        """)
        self.conn.commit()
    
    def extract_important_info(self, text: str, user_id: str) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ù…Ù† Ø§Ù„Ù†Øµ"""
        important_patterns = [
            (r'Ø§Ø³Ù…ÙŠ (Ù‡Ùˆ )?([\w\u0600-\u06FF\s]+)', "name", "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©"),
            (r'Ø£Ø¹ÙŠØ´ ÙÙŠ ([\w\u0600-\u06FF\s]+)', "location", "Ø§Ù„Ù…ÙƒØ§Ù†"),
            (r'Ø¹Ù…Ø±ÙŠ (\d+)', "age", "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©"),
            (r'Ø£Ø¹Ù…Ù„ ÙƒÙ€ ([\w\u0600-\u06FF\s]+)', "job", "Ø§Ù„Ù…Ù‡Ù†Ø©"),
            (r'Ø§Ù‡ØªÙ…Ø§Ù…Ø§ØªÙŠ (Ù‡ÙŠ )?([\w\u0600-\u06FF\s,]+)', "interests", "Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª"),
            (r'Ø£Ø­Ø¨ ([\w\u0600-\u06FF\s]+)', "likes", "Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª"),
            (r'Ù„Ø§ Ø£Ø­Ø¨ ([\w\u0600-\u06FF\s]+)', "dislikes", "Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª"),
        ]
        
        extracted_info = []
        for pattern, info_type, category in important_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                content = match[1] if len(match) > 1 else match[0]
                if len(content.strip()) > 2:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§
                    extracted_info.append({
                        'type': info_type,
                        'content': content.strip(),
                        'category': category,
                        'importance': 2 if info_type == 'name' else 1
                    })
        
        return extracted_info
    
    def store_memory(self, user_id: str, memory_type: str, content: str, 
                    tags: List[str] = None, importance: int = 1):
        """ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ø¹ Ø§Ù„ÙˆØ³ÙˆÙ…"""
        tags_str = ",".join(tags) if tags else ""
        
        cursor = self.conn.cursor()
        cursor.execute("""
        INSERT INTO user_memories (user_id, memory_type, content, tags, importance)
        VALUES (?, ?, ?, ?, ?)
        """, (user_id, memory_type, content, tags_str, importance))
        self.conn.commit()
    
    def get_relevant_memories(self, user_id: str, query: str, limit: int = 5) -> List[Dict]:
        """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©"""
        cursor = self.conn.cursor()
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„ÙˆØ³ÙˆÙ…
        cursor.execute("""
        SELECT memory_type, content, tags, importance, timestamp
        FROM user_memories 
        WHERE user_id = ? 
        AND (content LIKE ? OR tags LIKE ?)
        ORDER BY importance DESC, last_accessed DESC
        LIMIT ?
        """, (user_id, f"%{query}%", f"%{query}%", limit))
        
        memories = []
        for row in cursor.fetchall():
            memories.append({
                'type': row[0],
                'content': row[1],
                'tags': row[2].split(',') if row[2] else [],
                'importance': row[3],
                'timestamp': row[4]
            })
            
            # ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø¢Ø®Ø± ÙˆØµÙˆÙ„
            cursor.execute("""
            UPDATE user_memories SET last_accessed = CURRENT_TIMESTAMP 
            WHERE user_id = ? AND content = ?
            """, (user_id, row[1]))
        
        self.conn.commit()
        return memories

# =============== Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ===============
class PromptArchitecture:
    """Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ø³Ø¹Ø¯ AI"""
    
    SYSTEM_PROMPT = """
Ø£Ù†Øª Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ - Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…Ø¨Ø¯Ø¹ ÙŠØªØ³Ù… Ø¨Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©.

ğŸ¯ Ø´Ø®ØµÙŠØªÙŠ:
- Ù…Ø´Ø¬Ø¹ ÙˆØ¥ÙŠØ¬Ø§Ø¨ÙŠ Ø¯Ø§Ø¦Ù…Ø§Ù‹
- Ù…Ø¨Ø¯Ø¹ ÙÙŠ Ø§Ù„Ø­Ù„ÙˆÙ„
- Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù… ÙÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯
- Ù…Ø­ØªØ±Ù ÙˆÙˆØ§Ø¶Ø­
- Ù…Ø±Ø­ ÙˆÙ…Ø³Ø§Ø¹Ø¯ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙ†Ø§Ø³Ø¨ Ø§Ù„Ù…ÙˆÙ‚Ù
- Ø£Ø¹Ø·ÙŠ Ø¢Ø±Ø§Ø¡Ù‹ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©

ğŸ“ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø±Ø¯:
1. ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙ…Ø®ØªØµØ±Ø§Ù‹
2. Ù†Ø¸Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø©
3. ØªØ¬Ù†Ø¨ Ø§Ù„Ù‡Ù„ÙˆØ³Ø© Ø£Ùˆ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø¤ÙƒØ¯Ø©
4. Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ØªØ£ÙƒØ¯Ø§Ù‹ØŒ Ù‚Ù„ Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­
5. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„ØªØ®ØµÙŠØµ Ø§Ù„Ø±Ø¯ÙˆØ¯
6. Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø£ØµÙŠÙ„

ğŸ¨ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ÙˆØ§Ø¶Ø­Ø©
- Ù†Ø¸Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ù‚ÙˆØ§Ø¦Ù… Ù†Ù‚Ø·ÙŠØ©
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ø¹ØªØ¯Ù„
- ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
"""

    MEMORY_RULES = """
Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø©:
âœ… Ù…Ø§ ÙŠØ¬Ø¨ ØªØ®Ø²ÙŠÙ†Ù‡:
- Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© (Ø§Ù„Ø§Ø³Ù…ØŒ Ø§Ù„Ø¹Ù…Ø±ØŒ Ø§Ù„Ù…ÙƒØ§Ù†)
- Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª ÙˆØ§Ù„Ù‡ÙˆØ§ÙŠØ§Øª
- Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©
- Ø§Ù„Ø£Ù‡Ø¯Ø§Ù ÙˆØ§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹
- Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ù…Ù‡Ù…Ø©

âŒ Ù…Ø§ ÙŠØ¬Ø¨ ØªØ¬Ø§Ù‡Ù„Ù‡:
- Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø¹Ø§Ø¨Ø±Ø©
- Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
- Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©
- Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠ
"""

    def __init__(self):
        self.memory_system = AdvancedMemorySystem()
    
    def build_context_prompt(self, user_input: str, user_id: str = "default") -> str:
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø´Ø®ØµÙŠØ©"""
        
        # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
        relevant_memories = self.memory_system.get_relevant_memories(user_id, user_input)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        new_info = self.memory_system.extract_important_info(user_input, user_id)
        
        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        for info in new_info:
            self.memory_system.store_memory(
                user_id, 
                info['type'], 
                info['content'],
                tags=[info['category']],
                importance=info['importance']
            )
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©
        personal_info = []
        memory_conn = sqlite3.connect("conversation_memory.db")
        cursor = memory_conn.cursor()
        
        # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        cursor.execute("SELECT key, value FROM user_memory WHERE user_id = ?", (user_id,))
        for key, value in cursor.fetchall():
            if key in ["name", "age", "location", "job"]:
                personal_info.append(f"{key}: {value}")
        
        memory_conn.close()
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø´Ø§Ù…Ù„
        universal_memory = UniversalMemorySystem()
        conversation_context = universal_memory.get_conversation_context(user_id, limit=10)
        context_summary = universal_memory.generate_conversation_summary(user_id)
        
        # Ø¨Ù†Ø§Ø¡ Ù‚Ø³Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©
        memory_section = ""
        if relevant_memories:
            memory_section = "\nğŸ“ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©:\n"
            for memory in relevant_memories[:3]:  # Ø£ÙˆÙ„ 3 Ø°ÙƒØ±ÙŠØ§Øª ÙÙ‚Ø·
                memory_section += f"- [{memory['type']}] {memory['content'][:80]}...\n"
        
        # Ù‚Ø³Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©
        personal_section = ""
        if personal_info:
            personal_section = "\nğŸ‘¤ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©:\n" + "\n".join(personal_info[:5])  # Ø£ÙˆÙ„ 5 Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        
        # Ù‚Ø³Ù… Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        conversation_section = ""
        if conversation_context and len(conversation_context) > 0:
            conversation_section = "\nğŸ—£ï¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n"
            for i, conv in enumerate(conversation_context[-3:]):  # Ø¢Ø®Ø± 3 Ø±Ø³Ø§Ø¦Ù„
                user_msg = conv['user_input'][:60] + "..." if len(conv['user_input']) > 60 else conv['user_input']
                ai_msg = conv['ai_response'][:60] + "..." if len(conv['ai_response']) > 60 else conv['ai_response']
                conversation_section += f"{i+1}. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_msg}\n   Ø³Ø¹Ø¯: {ai_msg}\n"
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ø®Øµ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø·ÙˆÙŠÙ„Ø©
        summary_section = ""
        if len(conversation_context) > 5:
            summary_section = f"\nğŸ“‹ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:\n{context_summary[:200]}...\n"
        
        # Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ Ø­Ù‚Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆØ§Ù„Ø³ÙŠØ§Ù‚
        full_prompt = f"""
{self.SYSTEM_PROMPT}

{personal_section}

{memory_section}

{conversation_section}

{summary_section}

{self.MEMORY_RULES}

ğŸ¯ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©:
Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_input}

ÙÙƒØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ ÙˆØªØ£ÙƒØ¯ Ù…Ù†:
1. ÙÙ‡Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¯Ù‚Ø©
2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Ø°Ù„Ùƒ Ù…Ù†Ø§Ø³Ø¨Ø§Ù‹
3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
4. ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ
5. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©

Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© (Ø§Ù„Ø§Ø³Ù…ØŒ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§ØªØŒ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª) Ù„ØªØ®ØµÙŠØµ Ø§Ù„Ø±Ø¯ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ:
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø°ÙƒØ± Ø§Ù‡ØªÙ…Ø§Ù…Ø§ØªÙ‡ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„ÙŠÙ‡Ø§ ÙÙŠ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙ‡ ØªÙØ¶ÙŠÙ„Ø§Øª Ù…Ø¹Ø±ÙˆÙØ©ØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙÙŠ Ø§Ù„ØªÙˆØµÙŠØ§Øª
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ ÙˆÙ„ÙŠØ³ ÙÙŠ ÙƒÙ„ Ø¬Ù…Ù„Ø©

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
"""
        return full_prompt

# =============== Ù†Ø¸Ø§Ù… Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ø°Ø§ØªÙŠ ===============
class SelfCorrectionSystem:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ø°Ø§ØªÙŠ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¬ÙˆØ¯Ø©"""
    
    def __init__(self):
        self.correction_history = []
    
    def pre_response_check(self, reasoning: str, context: Dict) -> Dict:
        """ÙØ­Øµ Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
        checks = {
            'contradictions': self._check_contradictions(reasoning),
            'uncertainty': self._check_uncertainty(reasoning),
            'relevance': self._check_relevance(reasoning, context),
            'safety': self._check_safety(reasoning)
        }
        
        return {
            'passed': all(checks.values()),
            'details': checks,
            'warnings': self._generate_warnings(checks)
        }
    
    def post_response_evaluation(self, response: str, original_query: str) -> Dict:
        """ØªÙ‚ÙŠÙŠÙ… Ù…Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
        evaluation = {
            'relevance_score': self._calculate_relevance(response, original_query),
            'clarity_score': self._calculate_clarity(response),
            'accuracy_score': self._estimate_accuracy(response),
            'completeness_score': self._check_completeness(response, original_query)
        }
        
        overall_score = sum(evaluation.values()) / len(evaluation)
        
        return {
            'overall_score': overall_score,
            'detailed_scores': evaluation,
            'improvement_suggestions': self._generate_improvement_suggestions(evaluation)
        }
    
    def _check_contradictions(self, text: str) -> bool:
        """Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª"""
        contradiction_indicators = [
            "Ù…Ù† Ù†Ø§Ø­ÙŠØ©...ä½†æ˜¯ä»å¦ä¸€æ–¹é¢", "Ù„ÙƒÙ†... ÙˆÙ…Ø¹ Ø°Ù„Ùƒ", "Ø¨Ø§Ù„Ø±ØºÙ… Ù…Ù†... Ø¥Ù„Ø§ Ø£Ù†"
        ]
        return not any(indicator in text for indicator in contradiction_indicators)
    
    def _check_uncertainty(self, text: str) -> bool:
        """Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø¹Ø¯Ù… Ø§Ù„ÙŠÙ‚ÙŠÙ†"""
        uncertainty_phrases = [
            "Ø£Ø¹ØªÙ‚Ø¯ Ø±Ø¨Ù…Ø§", "Ù‚Ø¯ ÙŠÙƒÙˆÙ†", "Ù„ÙŠØ³ Ù…ØªØ£ÙƒØ¯", "Ø±Ø¨Ù…Ø§", "ÙŠØ­ØªÙ…Ù„"
        ]
        return uncertainty_phrases.count(text) < 2
    
    def _check_relevance(self, reasoning: str, context: Dict) -> bool:
        """ÙØ­Øµ ØµÙ„Ø© Ø§Ù„Ù…Ù†Ø·Ù‚ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚"""
        context_terms = set(context.get('query', '').lower().split())
        reasoning_terms = set(reasoning.lower().split())
        
        common_terms = context_terms & reasoning_terms
        return len(common_terms) >= 2
    
    def _check_safety(self, text: str) -> bool:
        """ÙØ­Øµ Ø§Ù„Ø³Ù„Ø§Ù…Ø©"""
        sensitive_terms = BAD_TERMS
        text_lower = text.lower()
        return not any(term in text_lower for term in sensitive_terms)
    
    def _calculate_relevance(self, response: str, query: str) -> float:
        """Ø­Ø³Ø§Ø¨ ØµÙ„Ø© Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„"""
        query_words = set(query.lower().split())
        response_words = set(response.lower().split())
        
        if not query_words:
            return 1.0
            
        intersection = query_words & response_words
        return len(intersection) / len(query_words)
    
    def _calculate_clarity(self, text: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ¶ÙˆØ­"""
        sentence_count = len(re.split(r'[.!ØŸ]', text))
        word_count = len(text.split())
        
        if sentence_count == 0:
            return 0.0
            
        avg_sentence_length = word_count / sentence_count
        return max(0.0, 1.0 - (abs(avg_sentence_length - 15) / 30))
    
    def _estimate_accuracy(self, text: str) -> float:
        """ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¯Ù‚Ø©"""
        confidence_indicators = ["Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯", "Ø¨Ù„Ø§ Ø´Ùƒ", "Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯", "Ù…Ø¤ÙƒØ¯"]
        uncertainty_indicators = ["Ø±Ø¨Ù…Ø§", "Ù‚Ø¯", "ÙŠØ­ØªÙ…Ù„", "Ø£Ø¸Ù†"]
        
        confidence_score = sum(1 for indicator in confidence_indicators if indicator in text)
        uncertainty_score = sum(1 for indicator in uncertainty_indicators if indicator in text)
        
        total_indicators = confidence_score + uncertainty_score
        if total_indicators == 0:
            return 0.7
            
        return confidence_score / total_indicators

    def _check_completeness(self, response: str, query: str) -> float:
        """ÙØ­Øµ Ø§Ù„Ø§ÙƒØªÙ…Ø§Ù„"""
        question_types = {
            "Ù…Ø§": 0.8, "ÙƒÙŠÙ": 0.7, "Ù„Ù…Ø§Ø°Ø§": 0.9, 
            "Ø£ÙŠÙ†": 0.6, "Ù…ØªÙ‰": 0.5, "Ù…Ù†": 0.8
        }
        
        for q_word, expected_score in question_types.items():
            if q_word in query:
                return expected_score
                
        return 0.7
    
    def _generate_warnings(self, checks: Dict) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØ­Ø°ÙŠØ±Ø§Øª"""
        warnings = []
        if not checks['contradictions']:
            warnings.append("Ø§Ø­ØªÙ…Ø§Ù„ ÙˆØ¬ÙˆØ¯ ØªÙ†Ø§Ù‚Ø¶ ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚")
        if not checks['uncertainty']:
            warnings.append("Ù…Ø³ØªÙˆÙ‰ Ø¹Ø§Ù„ Ù…Ù† Ø¹Ø¯Ù… Ø§Ù„ÙŠÙ‚ÙŠÙ†")
        if not checks['relevance']:
            warnings.append("Ø§Ù„Ù…Ù†Ø·Ù‚ Ù‚Ø¯ ÙŠÙƒÙˆÙ† ØºÙŠØ± Ø°ÙŠ ØµÙ„Ø©")
            
        return warnings
    
    def _generate_improvement_suggestions(self, scores: Dict) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†"""
        suggestions = []
        
        if scores['relevance_score'] < 0.7:
            suggestions.append("Ø§Ù„ØªØ±ÙƒÙŠØ² Ø£ÙƒØ«Ø± Ø¹Ù„Ù‰ ØµÙ„Ø© Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„")
        if scores['clarity_score'] < 0.6:
            suggestions.append("ØªØ­Ø³ÙŠÙ† ÙˆØ¶ÙˆØ­ ÙˆØ¨Ø³Ø§Ø·Ø© Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª")
        if scores['accuracy_score'] < 0.8:
            suggestions.append("Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¯Ù‚Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©")
            
        return suggestions

# =============== ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ===============
class CosmicSaadUltimateEnhanced(CosmicSaadUltimate):
    """Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø© Ù…Ù† Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ Ù…Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ø°Ø§ØªÙŠ"""
    
    def __init__(self, config_path: str = None):
        super().__init__(config_path)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        self.prompt_arch = PromptArchitecture()
        self.correction_system = SelfCorrectionSystem()
        
        # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        self.system_status = "enhanced_operational"
        
    def enhanced_process_input(self, user_input: str, user_id: str = "default") -> Dict:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù…Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„ØªØµØ­ÙŠØ­"""
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„
        full_prompt = self.prompt_arch.build_context_prompt(user_input, user_id)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
        conversation_context = self.universal_memory.get_conversation_context(user_id, limit=15)
        context_summary = self.universal_memory.generate_conversation_summary(user_id)
        
        # Ø¥Ø¶Ø§ÙØ© Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª
        if conversation_context:
            conversation_section = "\nğŸ—£ï¸ Ù…Ø­Ø§Ø¯Ø«Ø© Ø³Ø§Ø¨Ù‚Ø©:\n"
            for i, conv in enumerate(conversation_context[-5:]):  # Ø¢Ø®Ø± 5 Ø±Ø³Ø§Ø¦Ù„
                conversation_section += f"{i+1}. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {conv['user_input'][:80]}...\n"
                conversation_section += f"   Ø³Ø¹Ø¯: {conv['ai_response'][:80]}...\n"
            
            full_prompt = conversation_section + "\n" + full_prompt
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø·ÙˆÙŠÙ„Ø§Ù‹
        if len(conversation_context) > 10:
            full_prompt = f"Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:\n{context_summary}\n\n" + full_prompt
        
        # Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠ
        reasoning_prompt = f"{full_prompt}\n\nÙÙƒØ± Ø£ÙˆÙ„Ø§Ù‹ Ø«Ù… Ø£Ø¬Ø¨:"
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenRouter
        messages = [
            {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ. ÙÙƒØ± ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø«Ù… Ø£Ø¬Ø¨."},
            {"role": "user", "content": reasoning_prompt}
        ]
        
        reasoning_response = generate_via_openrouter(messages, temperature=0.3, max_tokens=200)
        
        # Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ø°Ø§ØªÙŠ Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        pre_check = self.correction_system.pre_response_check(
            reasoning_response, 
            {'query': user_input, 'user_id': user_id}
        )
        
        if not pre_check['passed']:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¯ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ù…Ø´Ø§ÙƒÙ„
            fallback_response = self._generate_fallback_response(user_input, pre_check['warnings'])
            final_response = fallback_response
        else:
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenRouter
            system_message = f"""Ø£Ù†Øª Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ - Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…Ø¨Ø¯Ø¹ ÙŠØªØ³Ù… Ø¨Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©.
            
{self.prompt_arch.SYSTEM_PROMPT}"""
            
            messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": full_prompt}
            ]
            
            final_response = generate_via_openrouter(messages, temperature=0.3, max_tokens=500)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø±Ø¯
        post_evaluation = self.correction_system.post_response_evaluation(final_response, user_input)
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„
        self._log_enhanced_interaction(
            user_id, user_input, final_response, 
            pre_check, post_evaluation
        )
        
        return {
            'response': final_response,
            'reasoning': reasoning_response,
            'pre_check': pre_check,
            'post_evaluation': post_evaluation,
            'relevant_memories': self.prompt_arch.memory_system.get_relevant_memories(user_id, user_input),
            'conversation_context': len(conversation_context),
            'timestamp': datetime.datetime.now().isoformat()
        }
    
    def _generate_fallback_response(self, user_input: str, warnings: List[str]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¢Ù…Ù†"""
        
        fallback_templates = [
            "Ø£Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙˆØ¶ÙŠØ­ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ÙƒØŸ",
            "Ø£ÙˆØ¯ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŒ Ù„ÙƒÙ†Ù†ÙŠ Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹.",
            "Ø­Ø§Ù„ÙŠØ§ØŒ Ù„Ø¯ÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø´ÙƒÙˆÙƒ Ø­ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©. Ø¯Ø¹Ù†Ø§ Ù†ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ØµØ¯Ø± Ù…ÙˆØ«ÙˆÙ‚.",
            "Ø³Ø¤Ø§Ù„Ùƒ Ù…Ø«ÙŠØ± Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù…! Ù„Ù„Ø£Ø³Ù Ø£Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø©."
        ]
        
        base_response = random.choice(fallback_templates)
        
        if warnings:
            warning_note = " Ù„Ø§Ø­Ø¸Øª Ø¨Ø¹Ø¶ Ø§Ù„ØµØ¹ÙˆØ¨Ø§Øª ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ."
            return base_response + warning_note
        
        return base_response
    
    def _log_enhanced_interaction(self, user_id: str, input_text: str, output_text: str,
                                pre_check: Dict, post_evaluation: Dict):
        """ØªØ³Ø¬ÙŠÙ„ ØªÙØ§Ø¹Ù„ Ù…Ø­Ø³Ù†"""
        
        interaction_data = {
            'user_id': user_id,
            'input': input_text,
            'output': output_text,
            'pre_check_results': pre_check,
            'post_evaluation': post_evaluation,
            'system_version': 'enhanced_1.0',
            'timestamp': datetime.datetime.now().isoformat()
        }
        
        self.learning_system.learn_from_interaction(
            f"enhanced_interaction:{user_id}",
            json.dumps(interaction_data, ensure_ascii=False)
        )

# =============== Ø£Ù…Ø«Ù„Ø© JSON Ù„Ù„Ø°Ø§ÙƒØ±Ø© ===============
MEMORY_EXAMPLES = {
    "user_profile": {
        "user_id": "user_123",
        "memories": [
            {
                "type": "name",
                "content": "Ø£Ø­Ù…Ø¯ Ù…Ø­Ù…Ø¯",
                "category": "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©",
                "importance": 2,
                "timestamp": "2024-01-15T10:30:00",
                "tags": ["Ù…Ø¹Ù„ÙˆÙ…Ø§Øª_Ø´Ø®ØµÙŠØ©", "Ø§Ø³Ù…"]
            },
            {
                "type": "location", 
                "content": "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©ØŒ Ù…ØµØ±",
                "category": "Ø§Ù„Ù…ÙƒØ§Ù†",
                "importance": 1,
                "timestamp": "2024-01-15T10:35:00",
                "tags": ["Ù…ÙˆÙ‚Ø¹", "Ø³ÙƒÙ†"]
            },
            {
                "type": "interests",
                "content": "Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©ØŒ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©ØŒ Ø§Ù„Ø³ÙØ±",
                "category": "Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª", 
                "importance": 1,
                "timestamp": "2024-01-15T10:40:00",
                "tags": ["Ù‡ÙˆØ§ÙŠØ§Øª", "Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª"]
            }
        ]
    }
}

# =============== Ø¯Ø§Ù„Ø© ÙˆØ§Ø¬Ù‡Ø© Ù…Ø¨Ø³Ø·Ø© Ù„Ù„Ù€ API ===============
def simple_openrouter_chat(user_input: str, system_prompt: str = None, 
                          temperature: float = 0.7, max_tokens: int = 512) -> str:
    """ÙˆØ§Ø¬Ù‡Ø© Ù…Ø¨Ø³Ø·Ø© Ù„Ù„Ø¯Ø±Ø¯Ø´Ø© Ù…Ø¹ OpenRouter"""
    if system_prompt is None:
        system_prompt = "Ø£Ù†Øª Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ - Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…Ø¨Ø¯Ø¹ ÙŠØªØ³Ù… Ø¨Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©."
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]
    
    return generate_via_openrouter(
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        model="meta-llama/llama-3.1-405b-instruct:free"
    )

# =============== Ø¯ÙˆØ±Ø© Inference Ø§Ù„ÙƒØ§Ù…Ù„Ø© ===============
def complete_inference_cycle(user_input: str, user_id: str = "default") -> Dict:
    """Ø¯ÙˆØ±Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒØ§Ù…Ù„Ø© Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©"""
    
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†
    saad_system = CosmicSaadUltimateEnhanced()
    
    # Ø§Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ø£ÙˆÙ„ÙŠ
    system_status = saad_system.get_system_status()
    
    # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
    result = saad_system.enhanced_process_input(user_input, user_id)
    
    # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    final_evaluation = {
        'system_status': system_status,
        'processing_result': result,
        'cycle_complete': True,
        'performance_metrics': {
            'response_time': 'optimized',
            'memory_usage': 'efficient', 
            'accuracy_estimate': result['post_evaluation']['overall_score']
        }
    }
    
    return final_evaluation

# =============== Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Flask ===============
@app.route('/api/chat/enhanced', methods=['POST'])
def enhanced_chat():
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø¯Ø±Ø¯Ø´Ø©"""
    data = request.get_json(force=True)
    user_input = data.get('message', '').strip()
    user_id = data.get('user_id', 'default')
    
    if not user_input:
        return jsonify({'Ø±Ø¯': 'Ù…Ù† ÙØ¶Ù„Ùƒ Ø£Ø¯Ø®Ù„ Ù†ØµØ§Ù‹.'})
    
    try:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯ÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
        result = complete_inference_cycle(user_input, user_id)
        
        return jsonify({
            'Ø±Ø¯': result['processing_result']['response'],
            'Ø§Ù„ØªÙ‚ÙŠÙŠÙ…': result['processing_result']['post_evaluation'],
            'Ø§Ù„Ø°Ø§ÙƒØ±Ø©_Ø§Ù„Ù…Ø³ØªØ¹Ù…Ù„Ø©': result['processing_result']['relevant_memories'],
            'Ø³ÙŠØ§Ù‚_Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©': result['processing_result']['conversation_context'],
            'Ø­Ø§Ù„Ø©_Ø§Ù„Ù†Ø¸Ø§Ù…': result['system_status']
        })
        
    except Exception as e:
        return jsonify({
            'Ø±Ø¯': f'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©: {str(e)}',
            'Ù†Øµ_Ø§Ø­ØªÙŠØ§Ø·ÙŠ': simple_openrouter_chat(user_input)
        })

# =============== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ===============
@app.route('/api/chat/advanced', methods=['POST'])
def advanced_chat():
    """ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª"""
    data = request.get_json(force=True)
    user_input = data.get('message', '').strip()
    user_id = data.get('user_id', 'default')
    
    # Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    temperature = data.get('temperature', 0.7)
    max_tokens = data.get('max_tokens', 512)
    
    if not user_input:
        return jsonify({'Ø±Ø¯': 'Ù…Ù† ÙØ¶Ù„Ùƒ Ø£Ø¯Ø®Ù„ Ù†ØµØ§Ù‹.'})
    
    try:
        start_time = time.time()
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø¨Ø³Ø·Ø©
        system_prompt = """Ø£Ù†Øª Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©.
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ø£Ø­Ø¯Ø« ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.
Ø£Ø¬Ø¨ Ø¨Ø¯Ù‚Ø© ÙˆØ¥Ø¨Ø¯Ø§Ø¹ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø£ØµØ§Ù„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."""
        
        response = simple_openrouter_chat(
            user_input, 
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        response_time = time.time() - start_time
        
        return jsonify({
            'Ø±Ø¯': response,
            'response_time': f"{response_time:.3f} Ø«Ø§Ù†ÙŠØ©",
            'model_used': 'meta-llama/llama-3.1-405b-instruct:free',
            'parameters': {
                'temperature': temperature,
                'max_tokens': max_tokens
            }
        })
        
    except Exception as e:
        return jsonify({
            'Ø±Ø¯': f'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: {str(e)}',
            'Ù†Øµ_Ø§Ø­ØªÙŠØ§Ø·ÙŠ': simple_openrouter_chat(user_input)
        })

def test_arabic_responses():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    test_cases = [
        "Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±",
        "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ©", 
        "Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠØ­Ø¯Ø« Ù„Ù„Ù…Ø§Ø¡ Ø¹Ù†Ø¯ 100 Ø¯Ø±Ø¬Ø©",
        "Ù…Ø±Ø­Ø¨Ø§"
    ]
    
    for question in test_cases:
        lang = detect_lang(question)
        factual = get_factual_answer(question, lang)
        print(f"Ø³Ø¤Ø§Ù„: '{question}' -> Ø¥Ø¬Ø§Ø¨Ø©: '{factual if factual else 'Ø³ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡Ø§'}'")

def run_local_smoke_tests():
    """Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ø­Ù„ÙŠØ© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥ØµÙ„Ø§Ø­ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
    print("=== Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ===")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ø§Ù„Ø© normalize_arabic_text
    test_cases = [
        ("Ø§Ù„Ù‚Ø§Ù‡Ø±Ù‡", "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©"),
        ("Ø§Ù„Ù‚Ø§Ø¨Ø±Ù‡", "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©"),
        ("Ø§Ù„Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠÙ‡", "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©"),
        ("Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠÙ‡", "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©"),
        ("Ø§Ù„Ø¬ÙŠØ²Ù‡", "Ø§Ù„Ø¬ÙŠØ²Ø©"),
        ("Ø§Ù„Ø§Ù†", "Ø§Ù„Ø¢Ù†"),
        ("Ù‡Ø§Ø°Ø§", "Ù‡Ø°Ø§"),
        ("Ù‡Ø°Ø©", "Ù‡Ø°Ù‡"),
        ("Ø§Ù„ÙŠ", "Ø¥Ù„Ù‰"),
        ("Ù…Ø¯Ø±Ø³Ù‡", "Ù…Ø¯Ø±Ø³Ø©"),
        ("Ø¬Ø§Ù…Ø¹Ù‡", "Ø¬Ø§Ù…Ø¹Ø©"),
    ]
    
    all_passed = True
    for input_text, expected in test_cases:
        result = normalize_arabic_text(input_text)
        status = "âœ“" if result == expected else "âœ—"
        print(f"{status} '{input_text}' -> '{result}' (Ù…ØªÙˆÙ‚Ø¹: '{expected}')")
        if result != expected:
            all_passed = False
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
    long_text = "Ø§Ù†Ø§ Ø³Ø§ÙƒÙ† ÙÙŠ Ø§Ù„Ù‚Ø§Ù‡Ø±Ù‡ ÙÙŠ Ù…Ù†Ø·Ù‚Ù‡ Ø§Ù„Ø¬ÙŠØ²Ù‡ Ù‚Ø±ÙŠØ¨ Ù…Ù† Ø§Ù„Ø¬Ø§Ù…Ø¹Ù‡"
    normalized_long = normalize_arabic_text(long_text)
    print(f"Ø§Ù„Ù†Øµ Ø§Ù„Ø·ÙˆÙŠÙ„: {long_text}")
    print(f"Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹: {normalized_long}")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø£Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ù„Ø§ ÙŠØªØ£Ø«Ø±
    english_text = "Hello from New York city"
    normalized_english = normalize_arabic_text(english_text)
    if english_text == normalized_english:
        print("âœ“ Ø§Ù„Ù†Øµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ù„Ù… ÙŠØªØ£Ø«Ø±")
    else:
        print("âœ— Ø§Ù„Ù†Øµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ ØªØ£Ø«Ø± Ø®Ø·Ø£Ù‹")
        all_passed = False
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    print("\n=== Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ===")
    test_arabic_responses()
    
    print(f"=== Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {'Ù†Ø¬Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª' if all_passed else 'ÙØ´Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª'} ===")
    return all_passed

# =============== ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… ===============
if __name__ == "__main__":
    # ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
    run_local_smoke_tests()
    
    print("\n" + "="*60)
    print("Ø³Ø¹Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙŠ - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø®Ø§Ø±Ù‚ (Ultimate Edition)")
    print("="*60)
    print("\nğŸ“¢ ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ù„Ù„Ø¹Ù…Ù„ Ø¹Ø¨Ø± OpenRouter API")
    print("ğŸ“‹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: meta-llama/llama-3.1-405b-instruct:free")
    print("âš ï¸  ØªØ£ÙƒØ¯ Ù…Ù† Ø¶Ø¨Ø· Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© OPENROUTER_API_KEY")
    
    saad_system = CosmicSaadUltimate()
    
    import os
    port = int(os.environ.get("PORT", 5000))  # Ù…Ù‡Ù… Ø¬Ø¯Ù‹Ø§ Ù„Ù€ Vercel
 

